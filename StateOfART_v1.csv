,title,authors,journal,issn,volume,issue,pages,year,publisher,url,abstract,notes,doi,keywords
0,6G: The next frontier: From holographic messaging to artificial intelligence using subterahertz and visible light communication,"EC Strinati, S Barbarossa, ...",IEEE Vehicular Technology Magazine,1556-6080,14,3,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8792135/?casa_token=Lse1HAY5fzYAAAAA:QAuvYVFgc_V3euDE3J95PHDCTRCH5px-BIQ2zd_CYX74cOUEEMHgKDRaJg1CrZEaWQiCgA_OP-jqUA,"With its ability to provide a single platform enabling a variety of services, such as enhanced mobile broadband communications, virtual reality, automated driving, and the Internet of Things, 5G represents a breakthrough in the design of communication networks. Nevertheless, considering the increasing requests for new services and predicting the development of new technologies within a decade, it is already possible to envision the need to move beyond 5G and design a new architecture incorporating innovative technologies to satisfy new needs at both the individual and societal levels.",,10.1109/MVT.2019.2921162,"5G mobile communication , Semantics , Three-dimensional displays , Cloud computing , Machine learning "
1,Future intelligent and secure vehicular network toward 6G: Machine-learning approaches,"F Tang, Y Kawamoto, N Kato, J Liu",Proceedings of the IEEE,1558-2256,108,2,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8926369/?casa_token=EtaE-KEbZH0AAAAA:mhc9SW55zQMx0H8KzCPSjiAqrQX2iJ7C7OFs9y4OPZCVNCcyS_VoBgdiKYRCPe1Qk3qeJFLDRGIaQA,"As a powerful tool, the vehicular network has been built to connect human communication and transportation around the world for many years to come. However, with the rapid growth of vehicles, the vehicular network becomes heterogeneous, dynamic, and large scaled, which makes it difficult to meet the strict requirements, such as ultralow latency, high reliability, high security, and massive connections of the next-generation (6G) network. Recently, machine learning (ML) has emerged as a powerful artificial intelligence (AI) technique to make both the vehicle and wireless communication highly efficient and adaptable. Naturally, employing ML into vehicular communication and network becomes a hot topic and is being widely studied in both academia and industry, paving the way for the future intelligentization in 6G vehicular networks. In this article, we provide a survey on various ML techniques applied to communication, networking, and security parts in vehicular networks and envision the ways of enabling AI toward a future 6G vehicular network, including the evolution of intelligent radio (IR), network intelligentization, and self-learning with proactive exploration.",,10.1109/JPROC.2019.2954595,"Vehicle dynamics , Resource management , Security , Array signal processing , Machine learning , OFDM , Vehicle-to-everything , Vehicular ad hoc networks "
2,Quantum machine learning for 6G communication networks: State-of-the-art and vision for the future,"SJ Nawaz, SK Sharma, S Wyne, MN Patwary, ...",IEEE Access,2169-3536,7,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8681450/,"The upcoming fifth generation (5G) of wireless networks is expected to lay a foundation of intelligent networks with the provision of some isolated artificial intelligence (AI) operations. However, fully intelligent network orchestration and management for providing innovative services will only be realized in Beyond 5G (B5G) networks. To this end, we envisage that the sixth generation (6G) of wireless networks will be driven by on-demand self-reconfiguration to ensure a many-fold increase in the network performance and service types. The increasingly stringent performance requirements of emerging networks may finally trigger the deployment of some interesting new technologies, such as large intelligent surfaces, electromagnetic-orbital angular momentum, visible light communications, and cell-free communications, to name a few. Our vision for 6G is a massively connected complex network capable of rapidly responding to the users' service calls through real-time learning of the network state as described by the network edge (e.g., base-station locations and cache contents), air interface (e.g., radio spectrum and propagation channel), and the user-side (e.g., battery-life and locations). The multi-state, multi-dimensional nature of the network state, requiring the real-time knowledge, can be viewed as a quantum uncertainty problem. In this regard, the emerging paradigms of machine learning (ML), quantum computing (QC), and quantum ML (QML) and their synergies with communication networks can be considered as core 6G enablers. Considering these potentials, starting with the 5G target services and enabling technologies, we provide a comprehensive review of the related state of the art in the domains of ML (including deep learning), QC, and QML and identify their potential benefits, issues, and use cases for their applications in the B5G networks. Subsequently, we propose a novel QC-assisted and QML-based framework for 6G communication networks while articulating its challenges and potential enabling technologies at the network infrastructure, network edge, air interface, and user end. Finally, some promising future research directions for the quantum- and QML-assisted B5G networks are identified and discussed.",,10.1109/ACCESS.2019.2909490,"5G mobile communication , Communication networks , Quantum computing , Machine learning , Wireless networks , Parallel processing , Quantum communication "
3,Channel state information prediction for 5G wireless communications: A deep learning approach,"C Luo, J Ji, Q Wang, X Chen, P Li",IEEE Transactions on Network Science and Engineering,2334-329X,7,1,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8395053/?casa_token=tkYk7_LyyecAAAAA:XoZm9C-R6Gbi5VcPYJPO4Hprv70_2JpqPml0pKq8IriJlBe6Fqt8LzIoPr0tS5dAzElr2BFvKjg8HQ,"Channel state information (CSI) estimation is one of the most fundamental problems in wireless communication systems. Various methods, so far, have been developed to conduct CSI estimation. However, they usually require high computational complexity, which makes them unsuitable for 5G wireless communications due to employing many new techniques (e.g., massive MIMO, OFDM, and millimeter-Wave (mmWave)). In this paper, we propose an efficient online CSI prediction scheme, called OCEAN, for predicting CSI from historical data in 5G wireless communication systems. Specifically, we first identify several important features affecting the CSI of a radio link and a data sample consists of the information of the features and the CSI. We then design a learning framework that is an integration of a CNN (convolutional neural network) and a long short term with memory (LSTM) network. We also further develop an offline-online two-step training mechanism, enabling the prediction results to be more stable when applying it to practical 5G wireless communication systems. To validate OCEAN's efficacy, we consider four typical case studies, and conduct extensive experiments in the four scenarios, i.e., two outdoor and two indoor scenarios. The experiment results show that OCEAN not only obtains the predicted CSI values very quickly but also achieves highly accurate CSI prediction with up to 2.650-3.457 percent average difference ratio (ADR) between the predicted and measured CSI.",,10.1109/TNSE.2018.2848960,"Wireless communication , 5G mobile communication , Estimation , Channel estimation , MIMO communication , Oceans , Fading channels "
4,"Machine learning for 5G/B5G mobile and wireless communications: Potential, limitations, and future directions","ME Morocho-Cayamcela, H Lee, W Lim",IEEE Access,2169-3536,7,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8844682/,"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",,10.1109/ACCESS.2019.2942390,"Wireless communication , Training , 5G mobile communication , Supervised learning , Task analysis , Reinforcement learning "
5,"6G Visions: Mobile ultra-broadband, super internet-of-things, and artificial intelligence","L Zhang, YC Liang, D Niyato",China Communications,1673-5447,16,8,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8820755/?casa_token=pqttGXbG8rcAAAAA:OxRJhbXk41rtIb-kYMbZSIv_-PFZaCYXkfeO8FOop2STq19ITXq_O70yJ5eztrB9VJE0JFCvwn9UHw,"With a ten-year horizon from concept to reality, it is time now to start thinking about what will the sixth-generation (6G) mobile communications be on the eve of the fifth-generation (5G) deployment. To pave the way for the development of 6G and beyond, we provide 6G visions in this paper. We first introduce the state-of-the-art technologies in 5G and indicate the necessity to study 6G. By taking the current and emerging development of wireless communications into consideration, we envision 6G to include three major aspects, namely, mobile ultra-broadband, super Internet-of-Things (IoT), and artificial intelligence (AI). Then, we review key technologies to realize each aspect. In particular, teraherz (THz) communications can be used to support mobile ultra-broadband, symbiotic radio and satellite-assisted communications can be used to achieve super IoT, and machine learning techniques are promising candidates for AI. For each technology, we provide the basic principle, key challenges, and state-of-the-art approaches and solutions.",,10.23919/JCC.2019.08.001,"6G mobile communication , 5G mobile communication , Absorption , Wireless communication , Artificial intelligence , Bandwidth , 3GPP "
6,"Deep learning for physical-layer 5G wireless techniques: Opportunities, challenges and solutions","H Huang, S Guo, G Gui, Z Yang, J Zhang, ...",IEEE Wireless Communications,1558-0687,27,1,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8786074/?casa_token=kxqgNlyhs24AAAAA:UyqUdrD5Rr5p2c1ywGBwObR9DNdQ3NZkLqWD8OnRlj6xwhpvDGcbEAcj5Am242b-ZTZc71Km2-MFVg,"The new demands for high-reliability and ultra-high capacity wireless communication have led to extensive research into 5G communications. However, current communication systems, which were designed on the basis of conventional communication theories, significantly restrict further performance improvements and lead to severe limitations. Recently, the emerging deep learning techniques have been recognized as a promising tool for handling the complicated communication systems, and their potential for optimizing wireless communications has been demonstrated. In this article, we first review the development of deep learning solutions for 5G communication, and then propose efficient schemes for deep learning-based 5G scenarios. Specifically, the key ideas for several important deep learning-based communication methods are presented along with the research opportunities and challenges. In particular, novel communication frameworks of NOMA, massive multiple-input multiple-output (MIMO), and millimeter wave (mmWave) are investigated, and their superior performances are demonstrated. We envision that the appealing deep learning- based wireless physical layer frameworks will bring a new direction in communication theories and that this work will move us forward along this road.",,10.1109/MWC.2019.1900027,"MIMO communication , Deep learning , 5G mobile communication , Channel estimation , Wireless communication , NOMA "
7,Artificial-intelligence-enabled intelligent 6G networks,"H Yang, A Alphones, Z Xiong, D Niyato, J Zhao, ...",2022 IEEE International Conference on Data Science and Information System (ICDSIS),,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9237460/?casa_token=KQN2dYbgKmYAAAAA:4fFRic3vEQwlMGgdajmKYPauvBQSbf7AEn-xvRds9_UpR7HqQuXd31lBK__Yt-UW6FgLqhcDprcCTw,"5G Generation connections, which have many novel features compared to Four-G connections, will be dispatched authoritatively very soon. Between 2027 and 2030, it is anticipated that the sixth-generation wireless communication system, utilising the entirety of artificial intelligence, will be implemented. In addition to 5G, there are a number of fundamental challenges that must be addressed, including increased scheme capability, higher data rates, and improved quality of service (QoS). This accessible manuscript discusses upcoming 6G wireless technology and its situation. Emerging technologies such as artificial intelligence and optical wireless technology are discussed. With 6G, mobile networks are anticipated to become one hundred times faster. As 6G expands beyond terrestrial networks and into space, it will enable new scenarios and services with terabytes of data traffic, enabling unprecedented human-machine interaction. 5G is intended to provide peak data rates of 20 Gigabits per second (Gbps) and average user experience rates of 120 Megabits per second (Mbps). It is anticipated that 6G speeds will be closer to 1,000 Gbps and 1 Gbps, respectively. 6G enables options such as holographic communication à la Star Trek and X reality (XR, which integrates AR, VR, and Mixed Reality). One of the goals of 6G cyberspace will be to deliver messages with a microsecond delay as opposed to a 1000-period delay. The 6G technology is enhanced by the combination of artificial intelligence and machine learning (AI), Using sub-mm waves, the 6G significantly influences the calculated communication capability for location determination. Using sub-mm Wave (e.g., wavelengths less than one millimetre) in conjunction with frequency selectivity to determine comparative electromagnetic incorporation charge will lead to significant advancements in wireless sensing technology. In terms of 5G, the calculation of mobile edge computing (MEC) is merely the tip of the iceberg. By the time 6G networks are established, it will be simpler to incorporate computation into collective communication and arithmetic. This generation continues to evolve in response to more distributed radio access networks (RAN) and the desire to utilise the terahertz (THz) range to further extend functionality, reduce latency, and improve spectrum sharing efficiency. It is expected that application 6G will find widespread use in the administration and production of emulsions. Clearly, 5G development communications are more uniform, and global spending has begun. Academic cooperation has started to incubate the next generation of wireless communication systems (namely 6G) in fields such as community security, health monitoring, and space excellent capabilities in order to further the development of wireless networks. Sixth G intended to provide the foundation for the stratification of communication needs in the 2030s.",,10.1109/ICDSIS55133.2022.9915945,"6G mobile communication , Wireless communication , Radio frequency , Wireless sensor networks , 5G mobile communication , Quality of service , Communication system security "
8,Ten challenges in advancing machine learning technologies toward 6G,"N Kato, B Mao, F Tang, Y Kawamoto, ...",IEEE Wireless Communications,1558-0687,27,3,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9061001/?casa_token=KEZti7RJkhcAAAAA:SEfyNunNDQc0j_iPyIGEDGBF2wan3U5rPL5YF0lMhYz4oeugFMtrzxSf7bKP9NmIeVXnIYSpsOvrng,"As the 5G standard is being completed, academia and industry have begun to consider a more developed cellular communication technique, 6G, which is expected to achieve high data rates up to 1 Tb/s and broad frequency bands of 100 GHz to 3 THz. Besides the significant upgrade of the key communication metrics, Artificial Intelligence (AI) has been envisioned by many researchers as the most important feature of 6G, since the state-of-the-art machine learning technique has been adopted as the top solution in many extremely complex scenarios. Network intelligentization will be the new trend to address the challenges of exponentially increasing number of connected heterogeneous devices. However, compared with the application of machine learning in other fields, such as computer games, current research on intelligent networking still has a long way to go to realize the automatically- configured cellular communication systems. Various problems in terms of communication system, machine learning architectures, and computation efficiency should be addressed for the full use of this technique in 6G. In this paper, we analyze machine learning techniques and introduce 10 most critical challenges in advancing the intelligent 6G system.",,10.1109/MWC.001.1900476,"Machine learning , 5G mobile communication , Machine learning algorithms , Measurement , Network security , Physical layer "
9,A self-adaptive deep learning-based system for anomaly detection in 5G networks,"LF Maimó, ÁLP Gómez, FJG Clemente, MG Pérez, ...",IEEE Access,2169-3536,6,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8283694/,"The upcoming fifth-generation (5G) mobile technology, which includes advanced communication features, is posing new challenges on cybersecurity defense systems. Although innovative approaches have evolved in the last few years, 5G will make existing intrusion detection and defense procedures become obsolete, in case they are not adapted accordingly. In this sense, this paper proposes a novel 5G-oriented cyberdefense architecture to identify cyberthreats in 5G mobile networks efficient and quickly enough. For this, our architecture uses deep learning techniques to analyze network traffic by extracting features from network flows. Moreover, our proposal allows adapting, automatically, the configuration of the cyberdefense architecture in order to manage traffic fluctuation, aiming both to optimize the computing resources needed in each particular moment and to fine tune the behavior and the performance of analysis and detection processes. Experiments using a well-known botnet data set depict how a neural network model reaches a sufficient classification accuracy in our anomaly detection system. Extended experiments using diverse deep learning solutions analyze and determine their suitability and performance for different network traffic loads. The experimental results show how our architecture can self-adapt the anomaly detection system based on the volume of network flows gathered from 5G subscribers' user equipments in real-time and optimizing the resource consumption.",,10.1109/ACCESS.2018.2803446,"Anomaly detection , 5G mobile communication , Machine learning , Botnet , Computer architecture , Feature extraction "
10,Deep learning-based traffic safety solution for a mixture of autonomous and manual vehicles in a 5G-enabled intelligent transportation system,"K Yu, L Lin, M Alazab, L Tan, ...",IEEE Transactions on Intelligent Transportation Systems,1558-0016,22,7,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9303409/?casa_token=G-h_oXhYXesAAAAA:2jPkdMweQzEUlUw6gV3iyU-VHTGjQRU_uk6eMnkViySiKbZCXsOqlKtg1xuGdfcCfRLAorONjH8UWw,"It is expected that a mixture of autonomous and manual vehicles will persist as a part of the intelligent transportation system (ITS) for many decades. Thus, addressing the safety issues arising from this mix of autonomous and manual vehicles before autonomous vehicles are entirely popularized is crucial. As the ITS system has increased in complexity, autonomous vehicles exhibit problems such as a low intention recognition rate and poor real-time performance when predicting the driving direction; these problems seriously affect the safety and comfort of mixed traffic systems. Therefore, the ability of autonomous vehicles to predict the driving direction in real time according to the surrounding traffic environment must be improved and researchers must work to create a more mature ITS. In this paper, we propose a deep learning-based traffic safety solution for a mixture of autonomous and manual vehicles in a 5G-enabled ITS. In this scheme, a driving trajectory dataset and a natural-driving dataset are employed as the network inputs to long-term memory networks in the 5G-enabled ITS: the probability matrix of each intention is calculated by the softmax function. Then, the final intention probability is obtained by fusing the mean rule in the decision layer. Experimental results show that the proposed scheme achieves intention recognition rates of 91.58% and 90.88% for left and right lane changes, respectively, effectively improving both accuracy and real-time intention recognition and improving the lane change problem in a mixed traffic environment.",,10.1109/TITS.2020.3042504,"Vehicles , Hidden Markov models , Safety , Manuals , 5G mobile communication , Real-time systems , Autonomous vehicles "
11,5G MIMO data for machine learning: Application to beam-selection using deep learning,"A Klautau, P Batista, N González-Prelcic, ...",2018 Information Theory and Applications Workshop (ITA),,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8503086/?casa_token=npOIjlauiYUAAAAA:JMYmoVRxM30JN_h59H7TeR9_rzYiyQ40abn6KbE0FMVD26KpcMRKyKJ3kExGezGMEkcsnaLZzVLOSQ,"The increasing complexity of configuring cellular networks suggests that machine learning (ML) can effectively improve 5G technologies. Deep learning has proven successful in ML tasks such as speech processing and computational vision, with a performance that scales with the amount of available data. The lack of large datasets inhibits the flourish of deep learning applications in wireless communications. This paper presents a methodology that combines a vehicle traffic simulator with a ray-tracing simulator, to generate channel realizations representing 5G scenarios with mobility of both transceivers and objects. The paper then describes a specific dataset for investigating beam-selection techniques on vehicle-to-infrastructure using millimeter waves. Experiments using deep learning in classification, regression and reinforcement learning problems illustrate the use of datasets generated with the proposed methodology.",,10.1109/ITA.2018.8503086,"5G mobile communication , MIMO communication , Ray tracing , Computational modeling , Receivers "
12,Artificial intelligence-enabled cellular networks: A critical path to beyond-5G and 6G,"R Shafin, L Liu, V Chandrasekhar, ...",IEEE Wireless Communications,1558-0687,27,2,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9040202/?casa_token=vzgn6PWStnoAAAAA:s9akQM_O-a5ddxKAsJbxUKuiKU8SrOBgc_xQANBAije4ElrfaLh-VAneV8D3UkkWk2s6YWOOqSKyYg,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.",,10.1109/MWC.001.1900323,"Cellular networks , Receivers , Complexity theory , MIMO communication , 5G mobile communication , Deep learning "
13,From 4G to 5G: Self-organized network management meets machine learning,"J Moysen, L Giupponi",Computer Communications,,,,,2018,Elsevier,https://www.sciencedirect.com/science/article/pii/S0140366418300380?casa_token=eLIUDmwIc_8AAAAA:4Kq1gZayOEcqQbMQQY_hhVj_Z61XK1juTqZSofmawf9QP4O5Ei6q9kgPojDqRkrprJoxgqyIrmU,"… We believe that Machine Learning (ML) can be effectively used to allow the network to learn … SON for 5G networks, and on how ML will be the driving tool for autonomous end-to-end 5G …",,,
14,DeepCog: Cognitive network management in sliced 5G networks with deep learning,"D Bega, M Gramaglia, M Fiore, ...",IEEE INFOCOM 2019 - IEEE Conference on Computer Communications,0743-166X,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8737488/?casa_token=8EesdBqHC58AAAAA:kIbITlsxZ5rsMU1xXxECI3D2XEFNacsTGhnbPjNZzabq2lc5JmKBRzNL21pM8Jvw1UPdoyCkTyKZGQ,"Network slicing is a new paradigm for future 5G networks where the network infrastructure is divided into slices devoted to different services and customized to their needs. With this paradigm, it is essential to allocate to each slice the needed resources, which requires the ability to forecast their respective demands. To this end, we present DeepCog, a novel data analytics tool for the cognitive management of resources in 5G systems. DeepCog forecasts the capacity needed to accommodate future traffic demands within individual network slices while accounting for the operator's desired balance between resource overprovisioning (i.e., allocating resources exceeding the demand) and service request violations (i.e., allocating less resources than required). To achieve its objective, DeepCog hinges on a deep learning architecture that is explicitly designed for capacity forecasting. Comparative evaluations with real-world measurement data prove that DeepCog's tight integration of machine learning into resource orchestration allows for substantial (50% or above) reduction of operating expenses with respect to resource allocation solutions based on state-of-the-art mobile traffic predictors. Moreover, we leverage DeepCog to carry out an extensive first analysis of the trade-off between capacity overdimensioning and unserviced demands in adaptive, sliced networks and in presence of real-world traffic.",,10.1109/INFOCOM.2019.8737488,"Resource management , Deep learning , Base stations , Neural networks , Maximum likelihood detection , Nonlinear filters , 5G mobile communication "
15,Improving traffic forecasting for 5G core network scalability: A machine learning approach,"I Alawe, A Ksentini, Y Hadjadj-Aoul, P Bertin",IEEE Network,1558-156X,32,6,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8553653/?casa_token=_IBrM-ofS4AAAAAA:brGpLHa3UigcrIP87eQaCStpGiS_dZ17FoW6E41QZniUZN-FA9bj5yx-zzwuNWKuapgTt_XUJdbGbQ,"5G is expected to provide network connectivity to not only classical devices (i.e., tablets, smartphones, etc.) but also to the IoT, which will drastically increase the traffic load carried over the network. 5G will mainly rely on NFV and SDN to build flexible and on-demand instances of functional networking entities via VNFs. Indeed, 3GPP is devising a new architecture for the core network, which replaces point-to-point interfaces used in 3G and 4G by producer/consumer-based communication among 5G core network functions, facilitating deployment over a virtual infrastructure. One big advantage of using VNFs is the possibility of dynamic scaling, depending on traffic load (i.e., instantiate new resources to VNFs when the traffic load increases and reduce the number of resources when the traffic load decreases). In this article, we propose a novel mechanism to scale 5G core network resources by anticipating traffic load changes through forecasting via ML techniques. The traffic load forecast is achieved by using and training a neural network on a real dataset of traffic arrival in a mobile network. Two techniques were used and compared: RNN, more specifically LSTM; and DNN. Simulation results show that the forecast-based scalability mechanism outperforms the threshold-based solutions, in terms of latency to react to traffic change, and delay to have new resources ready to be used by the VNF to react to traffic increase.",,10.1109/MNET.2018.1800104,"5G mobile communication , Scalability , 3GPP , Telecommunication traffic , Cloud computing , Networked control systems , Delays , Traffic control , Load management , Software defined networking , Smart devices , Dynamic scheduling "
16,A tutorial on ultrareliable and low-latency communications in 6G: Integrating domain knowledge into deep learning,"C She, C Sun, Z Gu, Y Li, C Yang, ...",Proceedings of the IEEE,1558-2256,109,3,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9369424/?casa_token=l1tIltFNmOMAAAAA:wXl6QQhO3yOInBX22kzHavAAiNFEdeP3RRFOuHDZxcDfBDVfKz0zeQmaEg59239cYYbgNwQkIk1HWQ,"As one of the key communication scenarios in the fifth-generation and also the sixth-generation (6G) mobile communication networks, ultrareliable and low-latency communications (URLLCs) will be central for the development of various emerging mission-critical applications. State-of-the-art mobile communication systems do not fulfill the end-to-end delay and overall reliability requirements of URLLCs. In particular, a holistic framework that takes into account latency, reliability, availability, scalability, and decision-making under uncertainty is lacking. Driven by recent breakthroughs in deep neural networks, deep learning algorithms have been considered as promising ways of developing enabling technologies for URLLCs in future 6G networks. This tutorial illustrates how domain knowledge (models, analytical tools, and optimization frameworks) of communications and networking can be integrated into different kinds of deep learning algorithms for URLLCs. We first provide some background of URLLCs and review promising network architectures and deep learning frameworks for 6G. To better illustrate how to improve learning algorithms with domain knowledge, we revisit model-based analytical tools and cross-layer optimization frameworks for URLLCs. Following this, we examine the potential of applying supervised/unsupervised deep learning and deep reinforcement learning in URLLCs and summarize related open problems. Finally, we provide simulation and experimental results to validate the effectiveness of different learning algorithms and discuss future directions.",,10.1109/JPROC.2021.3053601,"Deep learning , 6G mobile communication , Knowledge engineering , Ultra reliable low latency communication , Reliability , Network architecture , Optimization , Reinforcement learning , Low latency communication , Unsupervised learning "
17,Deep learning for hybrid 5G services in mobile edge computing systems: Learn from a digital twin,"R Dong, C She, W Hardjawana, Y Li, ...",IEEE Transactions on Wireless Communications,1558-2248,18,10,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8764584/?casa_token=Z7KMWLTojv8AAAAA:7PMsC8NAkl1-uq8QP3NaC6QQhdaDd7Bewz6YzA4rH9UmIgQR0xtoDnhrYL_sW3lcXerN2NJPpWBztw,"In this paper, we consider a mobile edge computing system with both ultra-reliable and low-latency communications services and delay tolerant services. We aim to minimize the normalized energy consumption, defined as the energy consumption per bit, by optimizing user association, resource allocation, and offloading probabilities subject to the quality-of-service requirements. The user association is managed by the mobility management entity (MME), while resource allocation and offloading probabilities are determined by each access point (AP). We propose a deep learning (DL) architecture, where a digital twin of the real network environment is used to train the DL algorithm off-line at a central server. From the pre-trained deep neural network (DNN), the MME can obtain user association scheme in a real-time manner. Considering that the real networks are not static, the digital twin monitors the variation of real networks and updates the DNN accordingly. For a given user association scheme, we propose an optimization algorithm to find the optimal resource allocation and offloading probabilities at each AP. The simulation results show that our method can achieve lower normalized energy consumption with less computation complexity compared with an existing method and approach to the performance of the global optimal solution.",,10.1109/TWC.2019.2927312,"Delays , Servers , Resource management , Energy consumption , Task analysis , Reliability , Machine learning algorithms "
18,A deep-learning-based radio resource assignment technique for 5G ultra dense networks,"Y Zhou, ZM Fadlullah, B Mao, N Kato",IEEE Network,1558-156X,32,6,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8553651/?casa_token=oUV9U2B2VUYAAAAA:BrB2jjybAK7wJWK1Vkge4-3_nFIhy8QcaXfHxS7o7SquoRglnPASPOTQNlDSdcx6YbbcN0lEXr1ELQ,"Recently, deep learning has emerged as a state-of-the-art machine learning technique with promising potential to drive significant breakthroughs in a wide range of research areas. The application of deep learning for network traffic control, however, remains immature due to the difficulty in uniquely characterizing the network traffic features as an appropriate input and output dataset to the learning structures. The network traffic features are anticipated to be even more dynamic and complex in the UDNs of the emerging 5G networks with high traffic demands coupled with beamforming and massive MIMO technologies. Therefore, it is critical for 5G network operators to carry out radio resource control in an efficient manner instead of adopting the simple conventional F/TDD. This is because the conventional uplink-downlink configuration change in the existing dynamic TDD method, typically used for resource assignment in beamforming and massive-MIMO-based UDNs, is prone to repeated congestion. In this article, we address this issue and discuss how to leverage the deep LSTM learning technique to make localized prediction of the traffic load at the UDN base station (i.e., the eNB). Based on localized prediction, our proposed algorithm executes the appropriate action policy a priori to avoid/alleviate the congestion in an intelligent fashion. Simulation results demonstrate that our proposal outperforms the conventional method in terms of packet loss rate, throughput, and MOS.",,10.1109/MNET.2018.1800085,"5G mobile communication , Array signal processing , MIMO communication , Telecommunication traffic , Dynamic scheduling , Traffic control , Networked traffic control , Predictive models , Deep learning "
19,Artificial intelligence enabled wireless networking for 5G and beyond: Recent advances and future challenges,"CX Wang, M Di Renzo, S Stanczak, ...",IEEE Wireless Communications,1558-0687,27,1,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9023918/?casa_token=glX9Kt4nhdEAAAAA:BndMPpaYYfPhJCr4hiEdR6IRKAJANfQL6BHGpT3LDBAiCjmXbLfxRYfirBE1L0ywj8lbqgErQeOC4A,"5G wireless communication networks are currently being deployed, and B5G networks are expected to be developed over the next decade. AI technologies and, in particular, ML have the potential to efficiently solve the unstructured and seemingly intractable problems by involving large amounts of data that need to be dealt with in B5G. This article studies how AI and ML can be leveraged for the design and operation of B5G networks. We first provide a comprehensive survey of recent advances and future challenges that result from bringing AI/ML technologies into B5G wireless networks. Our survey touches on different aspects of wireless network design and optimization, including channel measurements, modeling, and estimation, physical layer research, and network management and optimization. Then ML algorithms and applications to B5G networks are reviewed, followed by an overview of standard developments of applying AI/ML algorithms to B5G networks. We conclude this study with future challenges on applying AI/ML to B5G networks.",,10.1109/MWC.001.1900292,"Artificial intelligence , Channel estimation , Massive MIMO , 5G mobile communication , Loss measurement , Wireless networks "
20,"Machine learning for 6G wireless networks: Carrying forward enhanced bandwidth, massive access, and ultrareliable/low-latency service","J Du, C Jiang, J Wang, Y Ren, ...",IEEE Vehicular Technology Magazine,1556-6080,15,4,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9206115/?casa_token=0nKBNutZn9UAAAAA:0lAaou5TiytAPh6lBPJhFxCDEsrGam3pT9PDKb9uJP2jhxz8I2yrO3yT5eoUPyRLcmJZFatNRBM3Hg,"To satisfy the expected plethora of demanding services, the future generation of wireless networks (6G) has been mandated as a revolutionary paradigm to carry forward the capacities of enhanced broadband, massive access, and ultrareliable and lowlatency service in 5G wireless networks to a more powerful and intelligent level. Recently, the structure of 6G networks has tended to be extremely heterogeneous, densely deployed, and dynamic. Combined with tight quality of service (QoS), such complex architecture will result in the untenability of legacy network operation routines. In response, artificial intelligence (AI), especially machine learning (ML), is emerging as a fundamental solution to realize fully intelligent network orchestration and management. By learning from uncertain and dynamic environments, AI-/ML-enabled channel estimation and spectrum management will open up opportunities for bringing the excellent performance of ultrabroadband techniques, such as terahertz communications, into full play. Additionally, challenges brought by ultramassive access with respect to energy and security can be mitigated by applying AI-/ML-based approaches. Moreover, intelligent mobility management and resource allocation will guarantee the ultrareliability and low latency of services. Concerning these issues, this article introduces and surveys some state-of-the-art techniques based on AI/ML and their applications in 6G to support ultrabroadband, ultramassive access, and ultrareliable and lowlatency services.",,10.1109/MVT.2020.3019650,"6G mobile communication , 5G mobile communication , Artificial intelligence , Channel estimation , Radio spectrum management , Neural networks "
21,"Edge artificial intelligence for 6G: Vision, enabling technologies, and applications","KB Letaief, Y Shi, J Lu, J Lu",IEEE Journal on Selected Areas in Communications,1558-0008,40,1,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9606720/,"The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from “connected things” to “connected intelligence”. However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems.",,10.1109/JSAC.2021.3126076,"Artificial intelligence , 6G mobile communication , Task analysis , Sensors , Communication system security , Training , Standards "
22,6G white paper on machine learning in wireless communication networks,"S Ali, W Saad, N Rajatheva, K Chang, ...",arXiv preprint arXiv …,,,,,2020,arxiv.org,https://arxiv.org/abs/2004.13875,"… The rise of the deep learning paradigm has mainly been fueled by … in deep learning that are used for various tasks. In this section, we mention some of the most important deep learning …",,,
23,Explainable artificial intelligence for 6G: Improving trust between human and machine,W Guo,IEEE Communications Magazine,1558-1896,58,6,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9141213/?casa_token=wLm-2nSgiCIAAAAA:EG8WeQQVkamBmxtkIm8J6uZG0-IqSJVYhaZxknRqBDaRl4IIMJszzhMsgppjDMNTugQKY-Frsw-2iA,"As 5G mobile networks are bringing about global societal benefits, the design phase for 6G has started. Evolved 5G and 6G will need sophisticated AI to automate information delivery simultaneously for mass autonomy, human machine interfacing, and targeted healthcare. Trust will become increasingly critical for 6G as it manages a wide range of mission-critical services. As we migrate from traditional mathematical model-dependent optimization to data-dependent deep learning, the insight and trust we have in our optimization modules decrease. This loss of model explainability means we are vulnerable to malicious data, poor neural network design, and the loss of trust from stakeholders and the general public -- all with a range of legal implications. In this review, we outline the core methods of explainable artificial intelligence (XAI) in a wireless network setting, including public and legal motivations, definitions of explainability, performance vs. explainability trade-offs, and XAI algorithms. Our review is grounded in case studies for both wireless PHY and MAC layer optimization and provide the community with an important research area to embark upon.",,10.1109/MCOM.001.2000050,"Mathematical model , Computational modeling , Wireless communication , Optimization , Analytical models , Deep learning , 6G mobile communication "
24,Artificial intelligence in 5G technology: A survey,"MEM Cayamcela, W Lim",2018 International Conference on Information and Communication Technology Convergence (ICTC),2162-1233,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8539642/?casa_token=GQ3o3w2ltzQAAAAA:c7xIdju1MKdQwTgT6GHjkateUJNRTEqYjuFt2NRF4n_66NWLnri20Nsn-pBP8PFO4PvFb_4Zl8AaQw,"A fully operative and efficient 5G network cannot be complete without the inclusion of artificial intelligence (AI) routines. Existing 4G networks with all-IP (Internet Protocol) broadband connectivity are based on a reactive conception, leading to a poorly efficiency of the spectrum. AI and its subcategories like machine learning and deep learning have been evolving as a discipline, to the point that nowadays this mechanism allows fifth-generation (5G) wireless networks to be predictive and proactive, which is essential in making the 5G vision conceivable. This paper is motivated by the vision of intelligent base stations making decisions by themselves, mobile devices creating dynamically-adaptable clusters based on learned data rather than pre-established and fixed rules, that will take us to a improve in the efficiency, latency, and reliability of the current and real-time network applications in general. An exploration of the potential of AI-based solution approaches in the context of 5G mobile and wireless communications technology is presented, evaluating the different challenges and open issues for future research.",,10.1109/ICTC.2018.8539642,"Wireless communication , 5G mobile communication , Supervised learning , Artificial neural networks , Task analysis , Adaptation models "
25,Artificial intelligence defined 5G radio access networks,"M Yao, M Sohul, V Marojevic, ...",IEEE Communications Magazine,1558-1896,57,3,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8663985/?casa_token=mcTGjo4Bmm8AAAAA:tW9qrde2lOQivYb8mIZiJ_yUcovbqVxwSlR-T5OoP650pAD1A2jQ2MeTU77oFiq5a7BLK86b_hI4Pw,"Massive multiple-input multiple-output antenna systems, millimeter-wave communications, and ultra-dense networks have been widely perceived as the three key enablers that facilitate the development and deployment of 5G systems. This article discusses the intelligent agent that combines sensing, learning, and optimizing to facilitate these enablers. We present a flexible, rapidly deployable, and cross-layer artificial intelligence (AI)-based framework to enable the imminent and future demands on 5G and beyond. We present example AI-enabled 5G use cases that accommodate important 5G-specific capabilities and discuss the value of AI for enabling network evolution.",,10.1109/MCOM.2019.1800629,"5G mobile communication , Wireless communication , Resource management , Autonomous vehicles , Artificial intelligence , Wireless sensor networks "
26,An online context-aware machine learning algorithm for 5G mmWave vehicular communications,"GH Sim, S Klos, A Asadi, A Klein, ...",IEEE/ACM Transactions on Networking,1558-2566,26,6,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8472783/?casa_token=cVO1ndGERlUAAAAA:jzgLDGglaSSsttZNOl38YzZqdIuYKIb8YTCzamJS5-kkRbVRfpQ77jQuO2bSzuW5hdbxc8Azfexudg,"Millimeter-Wave (mmWave) bands have become the de-facto candidate for 5G vehicle-to-everything (V2X) since future vehicular systems demand Gbps links to acquire the necessary sensory information for (semi)-autonomous driving. Nevertheless, the directionality of mmWave communications and its susceptibility to blockage raise severe questions on the feasibility of mmWave vehicular communications. The dynamic nature of 5G vehicular scenarios and the complexity of directional mmWave communication calls for higher context-awareness and adaptability. To this aim, we propose an online learning algorithm addressing the problem of beam selection with environment-awareness in mmWave vehicular systems. In particular, we model this problem as a contextual multi-armed bandit problem. Next, we propose a lightweight context-aware online learning algorithm, namely fast machine learning (FML), with proven performance bound and guaranteed convergence. FML exploits coarse user location information and aggregates the received data to learn from and adapt to its environment. Furthermore, we demonstrate the feasibility of a real-world implementation of FML by proposing a standard-compliant protocol based on the existing architecture of cellular networks and the forthcoming features of 5G. We also perform an extensive evaluation using realistic traffic patterns derived from Google Maps. Our evaluation shows that FML enables mmWave base stations to achieve near-optimal performance on average within 33 mins of deployment by learning from the available context. Moreover, FML remains within ~ 5% of the optimal performance by swift adaptation to system changes (i.e., blockage, traffic).",,10.1109/TNET.2018.2869244,"Structural beams , 5G mobile communication , Base stations , Context modeling , Machine learning algorithms , Direction-of-arrival estimation , Array signal processing "
27,Deep learning-based mmWave beam selection for 5G NR/6G with sub-6 GHz channel information: Algorithms and prototype validation,"MS Sim, YG Lim, SH Park, L Dai, CB Chae",IEEE Access,2169-3536,8,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9034044/,"In fifth-generation (5G) communications, millimeter wave (mmWave) is one of the key technologies to increase the data rate. To overcome this technology’s poor propagation characteristics, it is necessary to employ a number of antennas and form narrow beams. It becomes crucial then, especially for initial access, to attain fine beam alignment between a next generation NodeB (gNB) and a user equipment (UE). The current 5G New Radio (NR) standard, however, adopts an exhaustive search-based beam sweeping, which causes time overhead of a half frame for initial beam establishment. In this paper, we propose a deep learning-based beam selection, which is compatible with the 5G NR standard. To select a mmWave beam, we exploit sub-6 GHz channel information. We introduce a deep neural network (DNN) structure and explain how we estimate a power delay profile (PDP) of a sub-6 GHz channel, which is used as an input of the DNN. We then validate its performance with real environment-based 3D ray-tracing simulations and over-the-air experiments with a mmWave prototype. Evaluation results confirm that, with support from the sub-6 GHz connection, the proposed beam selection reduces the beam sweeping overhead by up to 79.3 %.",,10.1109/ACCESS.2020.2980285,"5G mobile communication , Hidden Markov models , Channel models , Machine learning , Delays , Antenna arrays , Array signal processing "
28,DeepSlice: A deep learning approach towards an efficient and reliable network slicing in 5G networks,"A Thantharate, R Paropkari, V Walunj, ...","2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8993066/?casa_token=HzZkj1mYkO8AAAAA:WinKcWagZD85RpeTN0dUIA6L7jfnNX4cJsugcBGWiE0eitLMTEU4o8XjvMYwWnB_vrQMcbIisTpaUw,"Existing cellular communications and the upcoming 5G mobile network requires meeting high-reliability standards, very low latency, higher capacity, more security, and high-speed user connectivity. Mobile operators are looking for a programmable solution that will allow them to accommodate multiple independent tenants on the same physical infrastructure and 5G networks allow for end-to-end network resource allocation using the concept of Network Slicing (NS). Data-driven decision making will be vital in future communication networks due to the traffic explosion and Artificial Intelligence (AI) will accelerate the 5G network performance. In this paper, we have developed a `DeepSlice' model by implementing Deep Learning (DL) Neural Network to manage network load efficiency and network availability, utilizing in-network deep learning and prediction. We use available network Key Performance Indicators (KPIs) to train our model to analyze incoming traffic and predict the network slice for an unknown device type. Intelligent resource allocation allows us to use the available resources on existing network slices efficiently and offer load balancing. Our proposed DeepSlice model will be able to make smart decisions and select the most appropriate network slice, even in case of a network failure.",,10.1109/UEMCON47517.2019.8993066,
29,5G vehicular network resource management for improving radio access through machine learning,"SK Tayyaba, HA Khattak, A Almogren, MA Shah, ...",IEEE Access,2169-3536,8,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8951149/,"The current cellular technology and vehicular networks cannot satisfy the mighty strides of vehicular network demands. Resource management has become a complex and challenging objective to gain expected outcomes in a vehicular environment. The 5G cellular network promises to provide ultra-high-speed, reduced delay, and reliable communications. The development of new technologies such as the network function virtualization (NFV) and software defined networking (SDN) are critical enabling technologies leveraging 5G. The SDN-based 5G network can provide an excellent platform for autonomous vehicles because SDN offers open programmability and flexibility for new services incorporation. This separation of control and data planes enables centralized and efficient management of resources in a very optimized and secure manner by having a global overview of the whole network. The SDN also provides flexibility in communication administration and resource management, which are of critical importance when considering the ad-hoc nature of vehicular network infrastructures, in terms of safety, privacy, and security, in vehicular network environments. In addition, it promises the overall improved performance. In this paper, we propose a flow-based policy framework on the basis of two tiers virtualization for vehicular networks using SDNs. The vehicle to vehicle (V2V) communication is quite possible with wireless virtualization where different radio resources are allocated to V2V communications based on the flow classification, i.e., safety-related flow or non-safety flows, and the controller is responsible for managing the overall vehicular environment and V2X communications. The motivation behind this study is to implement a machine learning-enabled architecture to cater the sophisticated demands of modern vehicular Internet infrastructures. The inclination towards robust communications in 5G-enabled networks has made it somewhat tricky to manage network slicing efficiently. This paper also presents a proof of concept for leveraging machine learning-enabled resource classification and management through experimental evaluation of special-purpose testbed established in custom mininet setup. Furthermore, the results have been evaluated using Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN). While concluding the paper, it is shown that the LSTM has outperformed the rest of classification techniques with promising results.",,10.1109/ACCESS.2020.2964697,"Resource management , 5G mobile communication , Computer architecture , Wireless communication , Machine learning , Ad hoc networks , Communication system security "
30,When machine learning meets privacy in 6G: A survey,"Y Sun, J Liu, J Wang, Y Cao, ...",IEEE Communications Surveys & Tutorials,2373-745X,22,4,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9146540/?casa_token=unGqodHnHkQAAAAA:3YN2xBp0nrnzrw-7XmJVUxw4iaSD7k_lNaoIZhBIgOp4YTKxBswhg8THFDPzxqLyEr5xRl6lBCw13A,"The rapid-developing Artificial Intelligence (AI) technology, fast-growing network traffic, and emerging intelligent applications (e.g., autonomous driving, virtual reality, etc.) urgently require a new, faster, more reliable and flexible network form. At this time, researchers in both industry and academia have turned their attention to the sixth generation (6G) communication networks. In the 6G vision, various intelligent application scenarios that utilize Machine Learning (ML) technology (the most important branch of AI) will bring rich heterogeneous connections, as well as massive information storage and operations. When ML meets 6G, new opportunities will emerge along with numerous privacy challenges. On one hand, a secure ML structure, or the correct application of ML, can protect privacy in 6G. On the other hand, ML may be attacked or abused, resulting in privacy violation. It is worth noting that the alliance between 6G and ML may also be a double-edged sword in many cases, rather than absolutely infringe or protect privacy. Therefore, based on lots of existing meaningful works, this paper aims to provide a comprehensive survey of ML and privacy in 6G, with a view to further promoting the development of 6G and privacy protection technologies.",,10.1109/COMST.2020.3011561,"6G mobile communication , Data privacy , Big Data , MIMO communication , Tutorials , Machine learning "
31,Artificial intelligence to manage network traffic of 5G wireless networks,"Y Fu, S Wang, CX Wang, X Hong, ...",IEEE Network,1558-156X,32,6,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8553655/?casa_token=3nxyU2yW2uwAAAAA:mXQAahrpPgFlmCY8gP4Hbn1ObeIY1uV84IxclllvsdbOT5igqyQd9ENVWkl7IzyldXl5QECWwIS_jQ,"The deployment of 5G wireless communication systems is projected to begin in 2020. With new scenarios, new technologies, and new network architectures, the traffic management for 5G networks will present significant technical challenges. In recent years, AI technologies, especially ML technologies, have demonstrated significant success in many application domains, suggesting their potential to help solve the problem of 5G traffic management. In this article, we investigate the new characteristics of 5G wireless network traffic and discuss challenges they present for 5G traffic management. Potential solutions and research directions for the management of 5G traffic, including distributed and lightweight ML algorithms and a novel AI assistant content retrieval algorithm framework, are discussed.",,10.1109/MNET.2018.1800115,"5G mobile communication , Telecommunication traffic , Networked control systems , Traffic control , Supervised learning , Wireless networks , Network architecture , Artificial intelligence , Content management "
32,Deep learning based pilot allocation scheme (DL-PAS) for 5G massive MIMO system,"K Kim, J Lee, J Choi",IEEE Communications Letters,2373-7891,22,4,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8283585/?casa_token=DLz_AJ4HQSwAAAAA:iz_tE6ebFM5E8g5wF2AyebgAAw1V1_v2osPxUJQrkeCHvkyaZJd3WYhDKX-izmntQ5koeLqKE6dh3w,"This letter proposes a deep learning-based pilot assignment scheme (DL-PAS) for a massive multiple-input multiple-output (massive MIMO) system that utilizes a large number of antennas for multiple users. The proposed DL-PAS improves the performance in cellular networks with severe pilot contamination by learning the relationship between pilot assignment and the users' location pattern. In this letter, we design a novel supervised learning method, where input features and output labels are users' locations in all cells and pilot assignments, respectively. Specifically, pretrained optimal pilot assignments with given users' locations are provided through an exhaustive search method as the training data. Then, the proposed DL-PAS provides a near-optimal pilot assignment from the produced inferred function by analyzing the training data. We implement the proposed scheme using a commercial deep multilayer perceptron system. Simulation-based experiments show that the proposed scheme achieves almost 99.38% theoretical upper-bound performance with low complexity, requiring only 0.92-ms computational time.",,10.1109/LCOMM.2018.2803054,"Machine learning , MIMO communication , Base stations , Antennas , Contamination , Interference , Resource management "
33,Machine learning for intelligent authentication in 5G and beyond wireless networks,"H Fang, X Wang, S Tomasin",IEEE Wireless Communications,1558-0687,26,5,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8883130/?casa_token=KS6oVrno5m4AAAAA:SvNEPU9nr39_p77Nvl17dbU1U8TyuDzBEPU73Q7K2dwDtCd545NyZIn-UUUWOufyW55JUuj40u6QeQ,"The 5G and beyond wireless networks are critical to support diverse vertical applications by connecting heterogeneous devices and machines, which directly increase vulnerability for various spoofing attacks. Conventional cryptographic and physical layer authentication techniques are facing some challenges in complex dynamic wireless environments, including significant security overhead, low reliability, as well as difficulties in pre-designing a precise authentication model, providing continuous protection, and learning time-varying attributes. In this article, we envision new authentication approaches based on machine learning techniques by opportunistically leveraging physical layer attributes, and introduce intelligence to authentication for more efficient security provisioning. Machine learning paradigms for intelligent authentication design are presented, namely for parametric/non-parametric and supervised/ unsupervised/reinforcement learning algorithms. In a nutshell, the machine-learning-based intelligent authentication approaches utilize specific features in the multi-dimensional domain for achieving cost-effective, more reliable, model-free, continuous, and situation-aware device validation under unknown network conditions and unpredictable dynamics.",,10.1109/MWC.001.1900054,"Authentication , 5G mobile communication , Machine learning , Physical layer security , Wireless networks , Learning systems "
34,Deep-learning-empowered digital forensics for edge consumer electronics in 5G HetNets,"F Ding, G Zhu, M Alazab, X Li, ...",IEEE Consumer Electronics Magazine,2162-2256,11,2,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9309361/?casa_token=XLpt3N7xRl8AAAAA:MMoPjBE9lE-uPADVkbfvi31yM8nZSflL2ewbqXEHkQ_5GsfeQ2O2vIl9V2Rr-GF9ibO9HEDmMK-pQw,"The upcoming 5G heterogeneous networks (HetNets) have attracted much attention worldwide. Large amounts of high-velocity data can be transported by using the bandwidth spectrum of HetNets, yielding both great benefits and several concerning issues. In particular, great harm to our community could occur if the main visual information channels, such as images and videos, are maliciously attacked and uploaded to the Internet, where they can be spread quickly. Therefore, we propose a novel framework as a digital forensics tool to protect end users. It is built based on deep learning and can realize the detection of attacks via classification. Compared with the conventional methods and justified by our experiments, the data collection efficiency, robustness, and detection performance of the proposed model are all refined. In addition, assisted by 5G HetNets, our proposed framework makes it possible to provide high-quality real-time forensics services on edge consumer devices such as cell phone and laptops, which brings colossal practical value. Some discussions are also carried out to outline potential future threats.",,10.1109/MCE.2020.3047606,"Forensics , Digital forensics , 5G mobile communication , Detectors , Deep learning , Consumer electronics "
35,Intelligent wireless communications enabled by cognitive radio and machine learning,"X Zhou, M Sun, GY Li, BHF Juang",China Communications,1673-5447,15,12,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8594714/?casa_token=YJywNGoGTpMAAAAA:Nkk9MY8Eu6MADH4TWUAiXRgnd-xGUdcTvZlxi0AxTbQmDhQPncMoL4HxO4-5SRD4TlyGuEyyLSPAfA,"The ability to intelligently utilize resources to meet the need of growing diversity in services and user behavior marks the future of wireless communication systems. Intelligent wireless communications aims at enabling the system to perceive and assess the available resources, to autonomously learn to adapt to the perceived wireless environment, and to reconfigure its operating mode to maximize the utility of the available resources. The perception capability and reconfigurability are the essential features of cognitive radio while modern machine learning techniques project great potential in system adaptation. In this paper, we discuss the development of the cognitive radio technology and machine learning techniques and emphasize their roles in improving spectrum and energy utility of wireless communication systems. We describe the state-of-the-art of relevant techniques, covering spectrum sensing and access approaches and powerful machine learning algorithms that enable spectrum and energy-efficient communications in dynamic wireless environments. We also present practical applications of these techniques and identify further research challenges in cognitive radio and machine learning as applied to the existing and future wireless communication systems.",,10.12676/j.cc.2018.12.002,"Sensors , Cognitive radio , Wireless sensor networks , Machine learning , Wideband , Energy efficiency "
36,A machine learning approach to 5G infrastructure market optimization,"D Bega, M Gramaglia, A Banchs, ...",IEEE Transactions on Mobile Computing,2161-9875,19,3,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8632676/?casa_token=9LrGPWJRuMoAAAAA:4sdpbDhWmvmjspMUTe0h9qB32FPFJlTsveWTI6YWQsGdFZhgb7TF1EdHw0e9qOUqrBUhR47LlKQ1QQ,"It is now commonly agreed that future 5G Networks will build upon the network slicing concept. The ability to provide virtual, logically independent “slices” of the network will also have an impact on the models that will sustain the business ecosystem. Network slicing will open the door to new players: the infrastructure provider, which is the owner of the infrastructure, and the tenants, which may acquire a network slice from the infrastructure provider to deliver a specific service to their customers. In this new context, how to correctly handle resource allocation among tenants and how to maximize the monetization of the infrastructure become fundamental problems that need to be solved. In this paper, we address this issue by designing a network slice admission control algorithm that (i) autonomously learns the best acceptance policy while (ii) it ensures that the service guarantees provided to tenants are always satisfied. The contributions of this paper include: (i) an analytical model for the admissibility region of a network slicing-capable 5G Network, (ii) the analysis of the system (modeled as a Semi-Markov Decision Process) and the optimization of the infrastructure providers revenue, and (iii) the design of a machine learning algorithm that can be deployed in practical settings and achieves close to optimal performance.",,10.1109/TMC.2019.2896950,"5G mobile communication , Network slicing , Machine learning , Admission control , Biological system modeling , Machine learning algorithms , Optimization "
37,Deep learning-based DDoS-attack detection for cyber–physical system over 5G network,"B Hussain, Q Du, B Sun, Z Han",IEEE Transactions on Industrial Informatics,1941-0050,17,2,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9000893/?casa_token=E3_MdYDShwYAAAAA:5nvRczlxFmYaspiwxG9rgH8ZeZjw1uVxVCaPTgvFDvgjKkqPkgVeobGCYfC9HfpdYDDcsASPjVoCzA,"With the advent of 5G, cyber–physical systems (CPSs) employed in the vertical industries and critical infrastructures will depend on the cellular network more than ever; making their attack surface wider. Hence, guarding the network against cyberattacks is critical not only for its primary subscribers but to prevent it from being exploited as a proxy to attack CPSs. In this article, we propose a consolidated framework, by utilizing deep convolutional neural networks (CNNs) and real network data, to provide early detection for distributed denial-of-service (DDoS) attacks orchestrated by a botnet that controls malicious devices. These puppet devices individually perform silent call, signaling, SMS spamming, or a blend of these attacks targeting call, Internet, SMS, or a blend of these services, respectively, to cause a collective DDoS attack in a cell that can disrupt CPSs’ operations. Our results demonstrate that our framework can achieve higher than $91\%$ normal and underattack cell detection accuracy.",,10.1109/TII.2020.2974520,"Computer crime , 5G mobile communication , Computer architecture , Cellular networks , Microprocessors , Performance evaluation , Unsolicited electronic mail "
38,Machine learning techniques for 5G and beyond,"J Kaur, MA Khan, M Iftikhar, M Imran, QEU Haq",IEEE Access,2169-3536,9,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9321326/,"Wireless communication systems play a very crucial role in modern society for entertainment, business, commercial, health and safety applications. These systems keep evolving from one generation to next generation and currently we are seeing deployment of fifth generation (5G) wireless systems around the world. Academics and industries are already discussing beyond 5G wireless systems which will be sixth generation (6G) of the evolution. One of the main and key components of 6G systems will be the use of Artificial Intelligence (AI) and Machine Learning (ML) for such wireless networks. Every component and building block of a wireless system that we currently are familiar with from our knowledge of wireless technologies up to 5G, such as physical, network and application layers, will involve one or another AI/ML techniques. This overview paper, presents an up-to-date review of future wireless system concepts such as 6G and role of ML techniques in these future wireless systems. In particular, we present a conceptual model for 6G and show the use and role of ML techniques in each layer of the model. We review some classical and contemporary ML techniques such as supervised and un-supervised learning, Reinforcement Learning (RL), Deep Learning (DL) and Federated Learning (FL) in the context of wireless communication systems. We conclude the paper with some future applications and research challenges in the area of ML and AI for 6G networks.",,10.1109/ACCESS.2021.3051557,"6G mobile communication , 5G mobile communication , Artificial intelligence , Wireless networks , Resource management , Data models , Solid modeling "
39,"EdgeAI: A vision for distributed, edge-native artificial intelligence in future 6G networks","L Lovén, T Leppänen, E Peltonen, J Partala, ...",The 1st 6G wireless …,,,,,2019,jultika.oulu.fi,http://jultika.oulu.fi/files/nbnfi-fe2019050314180.pdf,"… the upcoming 5G mobile networks and future 6G technologies… artificial intelligence (AI) and in particular machine learning (… as predictive data analysis, machine learning, reasoning, and …",,,
40,Deep learning for ultra-reliable and low-latency communications in 6G networks,"C She, R Dong, Z Gu, Z Hou, Y Li, W Hardjawana, ...",IEEE Network,1558-156X,34,5,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9136591/?casa_token=VZmVeMfWmlwAAAAA:vMCBSfs4vk9wcHhst10-kA1g0-YrcHULsfDUlRjZjymQUfwOf-dr7Jqov1b7pmD36IzzGk8wrI-MjQ,"In future 6th generation networks, URLLC will lay the foundation for emerging mission-critical applications that have stringent requirements on end-to-end delay and reliability. Existing works on URLLC are mainly based on theoretical models and assumptions. The model-based solutions provide useful insights, but cannot be directly implemented in practice. In this article, we first summarize how to apply data-driven supervised deep learning and deep reinforcement learning in URLLC, and discuss some open problems of these methods. To address these open problems, we develop a multi-level architecture that enables device intelligence, edge intelligence, and cloud intelligence for URLLC. The basic idea is to merge theoretical models and realworld data in analyzing the latency and reliability and training deep neural networks (DNNs). Deep transfer learning is adopted in the architecture to fine-tune the pre-trained DNNs in non-stationary networks. Further considering that the computing capacity at each user and each mobile edge computing server is limited, federated learning is applied to improve the learning efficiency. Finally, we provide some experimental and simulation results and discuss some future directions.",,10.1109/MNET.011.1900630,"Ultra reliable low latency communication , Deep learning , Computer architecture , 6G mobile communication , Quality of service , Delays , Training "
41,Machine learning at the edge: A data-driven architecture with applications to 5G cellular networks,"M Polese, R Jana, V Kounev, K Zhang, ...",IEEE Transactions on Mobile Computing,2161-9875,20,12,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9107476/?casa_token=N9ZHhUB63uwAAAAA:ojsoWEQtIRgx-4VS8WniapyQKopSGMzJaYPZb57Q-ZMU28C5FXxb32oygs_2mqrx6NETqnzzqk2EXw,"The fifth generation of cellular networks (5G) will rely on edge cloud deployments to satisfy the ultra-low latency demand of future applications. In this paper, we argue that such deployments can also be used to enable advanced data-driven and Machine Learning (ML) applications in mobile networks. We propose an edge-controller-based architecture for cellular networks and evaluate its performance with real data from hundreds of base stations of a major U.S. operator. In this regard, we will provide insights on how to dynamically cluster and associate base stations and controllers, according to the global mobility patterns of the users. Then, we will describe how the controllers can be used to run ML algorithms to predict the number of users in each base station, and a use case in which these predictions are exploited by a higher-layer application to route vehicular traffic according to network Key Performance Indicators (KPIs). We show that the prediction accuracy improves when based on machine learning algorithms that rely on the controllers’ view and, consequently, on the spatial correlation introduced by the user mobility, with respect to when the prediction is based only on the local data of each single base station.",,10.1109/TMC.2020.2999852,"5G mobile communication , Cellular networks , Base stations , Machine learning algorithms , Machine learning , Computer architecture , Clustering algorithms "
42,"Artificial intelligence for 5G and beyond 5G: Implementations, algorithms, and optimizations","C Zhang, YL Ueng, C Studer, ...",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,2156-3365,10,2,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9108219/?casa_token=4EcSFyxQP34AAAAA:PbE84oAKc-bTUE_9uOEDQViWAfQWnph7VEAx6Qp6fvYSJ_xAGm0VO88Y3Gtd4I1NbwPrM9ObnwWUmQ,"This Special Issue of the IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS) is dedicated to demonstrating the latest research progress on artificial intelligence for 5G and beyond 5G (B5G) with respect to implementations, algorithms, and optimizations.",,10.1109/JETCAS.2020.2999944,"Special issues and sections , Wireless communication , Artificial intelligence , Hardware , Wireless sensor networks , 5G mobile communication "
43,FPGA realization of a reversible data hiding scheme for 5G MIMO-OFDM system by chaotic key generation-based paillier cryptography along with LDPC and its side …,"FH Shajin, P Rajesh","Journal of Circuits, Systems and Computers",,,,,2022,World Scientific,https://www.worldscientific.com/doi/abs/10.1142/S0218126622500931,"Multiple-Input and Multiple-Output (MIMO) technology is a significant and timely subject, which is highly motivated by the needs of 5G wireless communications. Data transmission …",,,
44,… scheme for 5G MIMO-OFDM system by chaotic key generation-based paillier cryptography along with LDPC and its side channel estimation using machine learning …,"FH Shajin, P Rajesh","Journal of Circuits, Systems and Computers",,,,,2022,World Scientific,https://www.worldscientific.com/doi/abs/10.1142/S0218126622500931,… A machine learning attack on text encrypted … the machine learning-based attack is good accuracy. The goal is to estimate the side channel using machine learning technique and FPGA …,,,
45,An optimal multitier resource allocation of cloud RAN in 5G using machine learning,"AK Bashir, R Arul, S Basheer, G Raja, ...",Transactions on …,,,,,2019,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.3627?casa_token=v3fhRzYH6l4AAAAA:QNgDYJDg--xTSLIMXU8pTONw9d_jW2HQtSq2MqrrXv9-3f5VjdguddyjpsmIKfyIZHdUjmnLQTGRbHNI,… technique forms a new design prototype in the 5G network … -CRAN in 5G by adopting the machine learning techniques to … to the domain of 5G and machine learning. The feasibility in the …,,,
46,A survey of 5G network systems: challenges and machine learning approaches,"H Fourati, R Maaloul, L Chaari",International Journal of Machine Learning …,,,,,2021,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s13042-020-01178-4&casa_token=LhWVrpkr9dsAAAAA:dITj5LY0Prd2BRvABZDlfQk-UWvzfDARiLtnudClJODxh3qnfDt2vir2L1F2ko167ICqUjX_QmkfS9Zc0Q,"… of the 5G wireless networks then we give a deep insight into a set of 5G challenges and research … We highlight a comprehensive view on 5G background, especially the 5G benefits and …",,,
47,Deep learning at the mobile edge: Opportunities for 5G networks,"M McClellan, C Cervelló-Pastor, S Sallent",Applied Sciences,,,,,2020,mdpi.com,https://www.mdpi.com/764806,"… of 5G … deep learning into mobile edge computing in 5G networks. In this paper, we explain the work undertaken and the challenges in applying the power of deep learning in 5G mobile …",,,
48,Consideration on automation of 5G network slicing with machine learning,"VP Kafle, Y Fukushima, P Martinez-Julia, ...",2018 ITU Kaleidoscope: Machine Learning for a 5G Future (ITU K),,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8597639/,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.",,10.23919/ITU-WT.2018.8597639,"Machine learning , Automation , 5G mobile communication , Network slicing , Monitoring , Security , Resource management "
49,Ultra-reliable MU-MIMO detector based on deep learning for 5G/B5G-enabled IoT,"K He, Z Wang, D Li, F Zhu, L Fan",Physical Communication,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S1874490720302585?casa_token=DGfKJYBBkJ8AAAAA:hzGCvg50jNJ5JMwtCy8sTWbyVYTrBpkaLwqZqjR9sRfRmlo0pZkPw3rLEcNnuxPpvN0z6tQ9yXg,"… of 5G and the successfully development of artificial intelligence (… For the industrial application, the machine learning methods … One application of 5G is the internet of things (IoT), where …",,,
50,Blockchain-based data security for artificial intelligence applications in 6G networks,"W Li, Z Su, R Li, K Zhang, Y Wang",IEEE Network,1558-156X,34,6,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9277898/?casa_token=ZfAaej_HgigAAAAA:nQw9rybBzg0lgc2igyMZ2FY9X76juf94TbAL3Y1IBIZ7ZtWOLujZUYlMrBC7vj9o1gKiAZlTzdvnpA,"The sixth generation (6G) networks are expected to provide a fully connected world with terrestrial wireless and satellite communications integration. The design concept of 6G networks is to leverage artificial intelligence (Ai) to promote the intelligent and agile development of network services. intelligent services inevitably involve the processing of large amounts of data, such as storage, computing, and analysis, such that the data may be vulnerable to tampering or contamination by attackers. in this article, we propose a blockchain-based data security scheme for Ai applications in 6G networks. Specifically, we first introduce the 6G architecture (i.e., a space-air-ground-underwater integrated network). Then we discuss two Ai-enabled applications, indoor positioning and autonomous vehicle, in the context of 6G. Through a case study of an indoor navigation system, we demonstrate the effectiveness of blockchain in data security. The integration of Ai and blockchain is developed to evaluate and optimize the quality of intelligent service. Finally, we discuss several open issues about data security in the upcoming 6G networks.",,10.1109/MNET.021.1900629,"6G mobile communication , Artificial intelligence , Autonomous vehicles , Servers , Satellite broadcasting , Blockchain , Wireless communication "
51,Deep learning based massive MIMO beamforming for 5G mobile network,"T Maksymyuk, J Gazda, O Yaremko, ...",2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8525802/,"The rapid increasing of the data volume in mobile networks forces operators to look into different options for capacity improvement. Thus, modern 5G networks became more complex in terms of deployment and management. Therefore, new approaches are needed to simplify network design and management by enabling self-organizing capabilities. In this paper, we propose a novel intelligent algorithm for performance optimization of the massive MIMO beamforming. The key novelty of the proposed algorithm is in the combination of three neural networks which cooperatively implement the deep adversarial reinforcement learning workflow. In the proposed system, one neural network is trained to generate realistic user mobility patterns, which are then used by second neural network to produce relevant antenna diagram. Meanwhile, third neural network estimates the efficiency of the generated antenna diagram returns corresponding reward to both networks. The advantage of the proposed approach is that it leans by itself and does not require large training datasets.",,10.1109/IDAACS-SWS.2018.8525802,"MIMO communication , Array signal processing , Antenna arrays , 5G mobile communication , Throughput , Spectral efficiency "
52,Optimal 5G network slicing using machine learning and deep learning concepts,"MH Abidi, H Alkhalefah, K Moiduddin, M Alazab, ...",Computer Standards & …,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S0920548921000131?casa_token=aJN0oGv0UrkAAAAA:OdkkvTEssV7SGalngJTqKGGaU7PdvV-tzQ4Zci1rPpp_m60tD7k1qatOjPb3fQIdqOK-XtnKw-o,"… First, we collected the 5G network slicing dataset, which involves the attributes associated with … that the proposed model could influence the provision of accurate 5G network slicing. …",,,
53,Machine learning‐based IDS for software‐defined 5G network,"J Li, Z Zhao, R Li",Iet Networks,,,,,2018,Wiley Online Library,https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-net.2017.0212,"… fifth generation (5G) … 5G. In this study, the authors propose an intelligent IDS taking the advances in software‐defined technology and artificial intelligence based on software‐defined 5G …",,,
54,An efficient deep learning model for intrusion classification and prediction in 5G and IoT networks,"S Rezvy, Y Luo, M Petridis, A Lasebae, ...",2019 53rd Annual Conference on Information Sciences and Systems (CISS),,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8693059/?casa_token=xBsu6xcm_qcAAAAA:uIgzY3yVb7bSd38SHiGDefFEyLTa6Ckq7pnYJINSt9RSpdSnJ3gUBTedQ9VhazQ3vu-NLH_f9AJjag,"A Network Intrusion Detection System is a critical component of every internet-connected system due to likely attacks from both external and internal sources. Such Security systems are used to detect network born attacks such as flooding, denial of service attacks, malware, and twin-evil intruders that are operating within the system. Neural networks have become an increasingly popular solution for network intrusion detection. Their capability of learning complex patterns and behaviors make them a suitable solution for differentiating between normal traffic and network attacks. In this paper, we have applied a deep autoencoded dense neural network algorithm for detecting intrusion or attacks in 5G and IoT network. We evaluated the algorithm with the benchmark Aegean Wi-Fi Intrusion dataset. Our results showed an excellent performance with an overall detection accuracy of 99.9% for Flooding, Impersonation and Injection type of attacks. We also presented a comparison with recent approaches used in literature which showed a substantial improvement in terms of accuracy and speed of detection with the proposed algorithm.",,10.1109/CISS.2019.8693059,"Deep learning , Neural networks , Intrusion detection , Feature extraction , Training , Internet of Things , Wireless networks "
55,Edge cloud server deployment with transmission power control through machine learning for 6G Internet of Things,"TK Rodrigues, K Suto, N Kato",IEEE Transactions on Emerging Topics in Computing,2376-4562,9,4,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8946743/?casa_token=5qjzxXtrASIAAAAA:dkD5QahevFBVfnxO4EoeWLZxTWl8wHZRG6Qo-Yl2OOApJ0xYiuKWPor4DToJiZuwueA-JspzfftOBA,"Cloud computing is an important technology for bringing a big pool of elastic resources to client devices. Their main drawback has long been the long distance between users and servers, but this has been remedied by Edge Cloud Computing, where the cloud servers are located in the network edge. Edge Cloud Computing is regarded as essential for future networks and consequently, there is plenty of research on how to optimize its operation. However, the vast majority of them ignore the decision of where the edge servers should be deployed, despite how severely this can affect the performance of the system. Furthermore, future networks must also deal with massive amounts of clients and servers, such as the ones characteristic of the Internet of Things and 6G Networks. This demands solutions that are scalable. Given these two points, we propose a Machine Learning-based server deployment policy in 6G Internet of Things environments. Our solution is proven to approach optimality while being feasible. Furthermore, we also prove that our proposal leads to lower latency and higher resource efficiency than conventional Edge Cloud Computing server deployment solutions.",,10.1109/TETC.2019.2963091,"Cloud computing , Servers , 6G mobile communication , Base stations , Edge computing , Computational modeling , Machine learning "
56,Artificial intelligence‐enabled sensing technologies in the 5G/internet of things era: from virtual reality/augmented reality to the digital twin,"Z Zhang, F Wen, Z Sun, X Guo, T He, ...",Advanced Intelligent …,,,,,2022,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202100228,"… In addition, artificial intelligence (AI) is also emerging and evolving by recent breakthroughs in … to apply artificial intelligence to the design of intelligent sensor systems for the 5G and IoT …",,,
57,Distributed artificial intelligence-as-a-service (DAIaaS) for smarter IoE and 6G environments,"N Janbi, I Katib, A Albeshri, R Mehmood",Sensors,,,,,2020,mdpi.com,https://www.mdpi.com/854906,"Artificial intelligence (AI) has taken us by storm, helping us to make decisions in everything we do, even in finding our “true love” and the “significant other”. While 5G promises us high-…",,,
58,Enhanced deployment strategy for the 5G drone-BS using artificial intelligence,"F Al-Turjman, JP Lemayian, S Alturjman, ...",IEEE Access,2169-3536,7,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8732936/,"The use of drones to perform various task has recently gained a lot of attention. Drones have been used by traders to deliver goods to customers, scientists, and researchers to observe and search for endangered species, and by the military during critical operations. The flexibility of drones in remote controlling makes them ideal candidates to perform critical tasks with minimum time and cost. In this paper, we use drones to setup base stations that provide 5G cellular coverage over a given area in danger. The aim of this paper is to determine the optimum number of drones and their optimum location, such that each point in the selected area is covered with the least cost while considering communication relevant parameters such as data rate, latency, and throughput. The problem is mathematically modeled by forming linear optimization equations. For fast optimized solutions, genetic algorithm (GA) and simulated annealing (SA) algorithms are provisionally employed to solve the problem, and the results are accordingly compared. Using these two meta-heuristic methods, quick and relatively inexpensive feedback can be provided to designers and service providers in 5G next generation networks.",,10.1109/ACCESS.2019.2921729,"Drones , 5G mobile communication , Genetic algorithms , Base stations , Simulated annealing , Quality of service "
59,Optimizing computation offloading in satellite-UAV-served 6G IoT: A deep learning approach,"B Mao, F Tang, Y Kawamoto, N Kato",IEEE Network,1558-156X,35,4,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9520341/?casa_token=kgLwCL1cHfAAAAAA:zzepD7e0-L4Iu4NN3f_yLnCYHnY7VzJTYM-sqh_OlZHTONFuWqDRkzwwj_CFcjrgwdJk7n18KySSwQ,"Satellite networks can provide Internet of Things (IoT) devices in remote areas with seamless coverage and downlink multicast transmissions. However, the large transmission latency, serious path loss, as well as the energy and resource constraints of IoT terminals challenge the stringent service requirements for throughput and latency in the 6G era. To address these problems, technologies including space-air-ground integrated networks (SAGINs), machine learning, edge computing, and energy harvesting are highly expected in 6G IoT. In this article, we consider the unmanned aerial vehicles (UAVs) and satellites to offer wireless-powered IoT devices edge computing and cloud computing services, respectively. To accelerate the communications, Terahertz frequency bands are utilized for communications between UAVs and IoT devices. Since the tasks generated by terrestrial IoT devices can be conducted locally, offloaded to the UAV-based edge servers or remote cloud servers through satellites, we focus on the computation offloading problem and consider deep learning techniques to optimize the task success rate considering the energy dynamics and channel conditions. A deep-learning-based offloading policy optimization strategy is given where the long short-term memory model is considered to address the dynamics of energy harvesting performance. Through the theoretical explanation and performance analysis, we discover the importance of emerging technologies including SAGIN, energy harvesting, and artificial intelligence techniques for 6G IoT.",,10.1109/MNET.011.2100097,"6G mobile communication , Deep learning , Cloud computing , Satellites , Internet of Things , Energy harvesting , Servers "
60,Fusion of blockchain and artificial intelligence for secure drone networking underlying 5G communications,"R Gupta, A Kumari, S Tanwar",Transactions on Emerging …,,,,,2021,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.4176?casa_token=I8Ntg5qXmAwAAAAA:FonOkHRvALYjMNxLMddnLUMJYPrROd_36SFakDnpiMHSnJY_l__Z6xsHjkt1jqX9YvuWgQyed7Y1CH7E,"… architecture underlying 5G communication network and artificial intelligence (AI) techniques. … on‐the‐fly decisions competencies through 5G and AI technologies. Then, we incorporate a …",,,
61,Machine learning for 5G and beyond: From model-based to data-driven mobile wireless networks,"T Wang, S Wang, ZH Zhou",China Communications,1673-5447,16,1,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8633313/?casa_token=62hRa3OV-rgAAAAA:jtbJjUwHWvbPQIrvCxETm3v7_PWe71Ostf4AeLsUIN8yaJDvubiW-0bIgYHaTSLgTYfa91wC89TPIg,"During the past few decades, mobile wireless communications have experienced four generations of technological revolution, namely from 1G to 4G, and the deployment of the latest 5G networks is expected to take place in 2019. One fundamental question is how we can push forward the development of mobile wireless communications while it has become an extremely complex and sophisticated system. We believe that the answer lies in the huge volumes of data produced by the network itself, and machine learning may become a key to exploit such information. In this paper, we elaborate why the conventional model-based paradigm, which has been widely proved useful in pre-5G networks, can be less efficient or even less practical in the future 5G and beyond mobile networks. Then, we explain how the data-driven paradigm, using state-of-the-art machine learning techniques, can become a promising solution. At last, we provide a typical use case of the data-driven paradigm, i.e., proactive load balancing, in which online learning is utilized to adjust cell configurations in advance to avoid burst congestion caused by rapid traffic changes.",,10.12676/j.cc.2019.01.015,"5G mobile communication , Data models , Wireless networks , Machine learning , Computational modeling , Broadband antennas "
62,"Survey on machine learning for intelligent end-to-end communication toward 6G: From network access, routing to traffic control and streaming adaption","F Tang, B Mao, Y Kawamoto, ...",IEEE Communications Surveys & Tutorials,2373-745X,23,3,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9403380/?casa_token=9lWmsKMjvRYAAAAA:gVHbuc_943J-FHjucV04tyflCAoFpASuMSrWVkofTn52WANKffby9FlDx6PCGfaJZEH1sHGWxoXmQg,"The end-to-end quality of service (QoS) and quality of experience (QoE) guarantee is quite important for network optimization. The current 5G and conceived 6G network in the future with ultra high density, bandwidth, mobility and large scale brings urgent requirement of high efficient end-to-end optimization methods. The conventional network optimization methods without learning and intelligent decision ability are hard to handle the high complexity and dynamic scenarios of 6G. Recently, machine learning based QoS and QoE aware network optimization algorithms emerge as a hot research area and attract much attention, which is widely acknowledged as the potential solution for end-to-end optimization in 6G. However, there are still many critical issues of employing machine learning in networks, especially in 6G. In this paper, we give a comprehensive survey on the recent machine learning based network optimization methods to guarantee the end-to-end QoS and QoE. To easy to follow, we introduce the investigated works following the end-to-end transmission flow from network access, routing to network congestion control and adaptive steaming control. Then we discuss some open issues and potential future research directions.",,10.1109/COMST.2021.3073009,"Quality of service , Heuristic algorithms , 6G mobile communication , Quality of experience , Routing , Machine learning algorithms , Reinforcement learning "
63,A heuristic offloading method for deep learning edge services in 5G networks,"X Xu, D Li, Z Dai, S Li, X Chen",IEEE Access,2169-3536,7,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8721102/,"With the continuous development of the Internet of Things (IoT) and communications technology, especially under the epoch of 5G, mobile tasks with big scales of data have a strong demand in deep learning such as virtual speech recognition and video classification. Considering the limited computing resource and battery consumption of mobile devices (MDs), these tasks are often offloaded to the remote infrastructure, like cloud platforms, which leads to the unavoidable offloading transmission delay. Edge computing (EC) is a novel computing paradigm, capable of offloading the computation tasks to the edge of networks, which reduces the transmission delay between the MDs and cloud. Therefore, combining deep learning and EC is expected to be a solution for these tasks. However, how to decide the offloading destination [cloud or deep learning-enabled edge computing nodes (ECNs)] for computation offloading is still a challenge. In this paper, a heuristic offloading method, named HOM, is proposed to minimize the total transmission delay. To be more specific, an offloading framework for deep learning edge services is built upon centralized unit (CU)-distributed unit (DU) architecture. Then, we acquire the appropriate offloading strategy by the origin-destination ECN distance estimation and heuristic searching of the destination virtual machines for accommodating the offloaded computation tasks. Finally, the effectiveness of the scheme is verified by detailed experimental evaluations.",,10.1109/ACCESS.2019.2918585,"Task analysis , Deep learning , 5G mobile communication , Delays , Cloud computing , Internet of Things , Edge computing "
64,A survey of online data-driven proactive 5G network optimisation using machine learning,"B Ma, W Guo, J Zhang",IEEE Access,2169-3536,8,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9003183/,"In the fifth-generation (5G) mobile networks, proactive network optimisation plays an important role in meeting the exponential traffic growth, more stringent service requirements, and to reduce capital and operational expenditure. Proactive network optimisation is widely acknowledged as one of the most promising ways to transform the 5G network based on big data analysis and cloud-fog-edge computing, but there are many challenges. Proactive algorithms will require accurate forecasting of highly contextualised traffic demand and quantifying the uncertainty to drive decision making with performance guarantees. Context in Cyber-Physical-Social Systems (CPSS) is often challenging to uncover, unfolds over time, and even more difficult to quantify and integrate into decision making. The first part of the review focuses on mining and inferring CPSS context from heterogeneous data sources, such as online user-generated-content. It will examine the state-of-the-art methods currently employed to infer location, social behaviour, and traffic demand through a cloud-edge computing framework; combining them to form the input to proactive algorithms. The second part of the review focuses on exploiting and integrating the demand knowledge for a range of proactive optimisation techniques, including the key aspects of load balancing, mobile edge caching, and interference management. In both parts, appropriate state-of-the-art machine learning techniques (including probabilistic uncertainty cascades in proactive optimisation), complexity-performance trade-offs, and demonstrative examples are presented to inspire readers. This survey couples the potential of online big data analytics, cloud-edge computing, statistical machine learning, and proactive network optimisation in a common cross-layer wireless framework. The wider impact of this survey includes better cross-fertilising the academic fields of data analytics, mobile edge computing, AI, CPSS, and wireless communications, as well as informing the industry of the promising potentials in this area.",,10.1109/ACCESS.2020.2975004,"Optimization , 5G mobile communication , Cloud computing , Machine learning , Forecasting , Big Data , 3GPP "
65,Artificial intelligence for elastic management and orchestration of 5G networks,"DM Gutierrez-Estevez, M Gramaglia, ...",IEEE Wireless Communications,1558-0687,26,5,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8809570/?casa_token=f2lSjMfkUAUAAAAA:ev1yaVXcCACCr0rSTkosWPBET3yJpz1fM0FO68R22E271piL3Jrvj3TXEonbLkzD8bPpCz_g4ccS6w,"The emergence of 5G enables a broad set of diversified and heterogeneous services with complex and potentially conflicting demands. For networks to be able to satisfy those needs, a flexible, adaptable, and programmable architecture based on network slicing is being proposed. A softwarization and cloudification of the communications networks is required, where network functions (NFs) are being transformed from programs running on dedicated hardware platforms to programs running over a shared pool of computational and communication resources. This architectural framework allows the introduction of resource elasticity as a key means to make an efficient use of the computational resources of 5G systems, but adds challenges related to resource sharing and efficiency. In this article, we propose Artificial Intelligence (AI) as a built-in architectural feature that allows the exploitation of the resource elasticity of a 5G network. Building on the work of the recently formed Experiential Network Intelligence (ENI) industry specification group of the European Telecommunications Standards Institute (ETSI) to embed an AI engine in the network, we describe a novel taxonomy for learning mechanisms that target exploiting the elasticity of the network as well as three different resource elastic use cases leveraging AI. This work describes the basis of a use case recently approved at ETSI ENI.",,10.1109/MWC.2019.1800498,"Elasticity , Artificial intelligence , 5G mobile communication , Computer architecture , Resource management , Cloud computing , Network slicing "
66,Enhanced machine learning techniques for early HARQ feedback prediction in 5G,"N Strodthoff, B Göktepe, T Schierl, ...",IEEE Journal on Selected Areas in Communications,1558-0008,37,11,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8792202/,"We investigate Early Hybrid Automatic Repeat reQuest (E-HARQ) feedback schemes enhanced by machine learning techniques as a path towards ultra-reliable and low-latency communication (URLLC). To this end, we propose machine learning methods to predict the outcome of the decoding process ahead of the end of the transmission. We discuss different input features and classification algorithms ranging from traditional methods to newly developed supervised autoencoders. These methods are evaluated based on their prospects of complying with the URLLC requirements of effective block error rates below 10-5 at small latency overheads. We provide realistic performance estimates in a system model incorporating scheduling effects to demonstrate the feasibility of E-HARQ across different signal-to-noise ratios, subcode lengths, channel conditions and system loads, and show the benefit over regular HARQ and existing E-HARQ schemes without machine learning.",,10.1109/JSAC.2019.2934001,"Machine learning , Decoding , Probabilistic logic , Machine learning algorithms , Error analysis , Delays , Receivers "
67,Toward smart and cooperative edge caching for 5G networks: A deep learning based approach,"H Pang, J Liu, X Fan, L Sun",2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS),1548-615X,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8624176/?casa_token=4DDJ4gsbFW0AAAAA:jNugKIQy3XEKCzfbpwuatg_IFC9VagXUorLnWfXPgRNWr9-Cvn97uzUnIjz8gaD3kjtHvd_RWMUpFg,"The emerging 5G mobile networking promises ultrahigh network bandwidth and ultra-low communication latency (<;1ms), benefiting a wide range of applications, including live video streaming, online gaming, virtual and augmented reality, and Vehicle-to-X, to name but a few. The backbone Internet, however, does not keep up, particularly in latency (>100ms), due to its store-and-forward design and the physical barrier from signal propagation speed, not to mention congestion that frequently happens. Caching is known to be effective to bridge the speed gap, which has become a critical component in the 5G deployment as well. Besides storage, 5G base stations (BSs) will also be powered with strong computing modules, offering mobile edge computing (MEC) capability. This paper explores the potentials of edge computing towards improving the cache performance, and we envision a learning-based framework that facilitates smart caching beyond simple frequency- and time-based replace strategies and cooperation among base stations. Within this framework, we develop DeepCache, a deep-learning-based solution to understand the request patterns in individual base stations and accordingly make intelligent cache decisions. Using mobile video, one of the most popular applications with high traffic demand, as a case, we further develop a cooperation strategy for nearby base stations to collectively serve user requests. Experimental results on real-world dataset show that using the collaborative DeepCache algorithm, the overall transmission delay is reduced by 14%~22%, with a backhaul data traffic saving of 15%~23%.",,10.1109/IWQoS.2018.8624176,"5G mobile communication , Servers , Streaming media , Training , Deep learning , Base stations , Bandwidth "
68,Dynamic management of a deep learning-based anomaly detection system for 5G networks,"L Fernández Maimó, A Huertas Celdrán, ...",Journal of Ambient …,,,,,2019,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s12652-018-0813-4&casa_token=5nYM-4mKQ7QAAAAA:r9nngOJxLkUF54yugnf24VRFC3JfdglP_c7FRMJhVnUwrzxvHXmWenBwBSPl3w7w06NYn2xeHVRxCULREg,"… in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile … Our proposal uses deep learning techniques to analyze network flows and to detect network …",,,
69,Intelligent network data analytics function in 5G cellular networks using machine learning,"S Sevgican, M Turan, K Gökarslan, ...",Journal of Communications and Networks,1976-5541,22,3,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9143579/,"5G cellular networks come with many new features compared to the legacy cellular networks, such as network data analytics function (NWDAF), which enables the network operators to either implement their own machine learning (ML) based data analytics methodologies or integrate third-party solutions to their networks. In this paper, the structure and the protocols of NWDAF that are defined in the 3rd Generation Partnership Project (3GPP) standard documents are first described. Then, cell-based synthetic data set for 5G networks based on the fields defined by the 3GPP specifications is generated. Further, some anomalies are added to this data set (e.g., suddenly increasing traffic in a particular cell), and then these anomalies within each cell, subscriber category, and user equipment are classified. Afterward, three ML models, namely, linear regression, long-short term memory, and recursive neural networks are implemented to study behaviour information estimation (e.g., anomalies in the network traffic) and network load prediction capabilities of NWDAF. For the prediction of network load, three different models are used to minimize the mean absolute error, which is calculated by subtracting the actual generated data from the model prediction value. For the classification of anomalies, two ML models are used to increase the area under the receiver operating characteristics curve, namely, logistic regression and extreme gradient boosting. According to the simulation results, neural network algorithms outperform linear regression in network load prediction, whereas the tree-based gradient boosting algorithm outperforms logistic regression in anomaly detection. These estimations are expected to increase the performance of the 5G network through NWDAF.",,10.1109/JCN.2020.000019,"5G mobile communication , Cellular networks , 3GPP , Data analysis , Machine learning , Load modeling , Data models "
70,Deep learning for security problems in 5G heterogeneous networks,"Z Lv, AK Singh, J Li",IEEE Network,1558-156X,35,2,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9318424/?casa_token=ac-x0p4SZU8AAAAA:ufCGhO5QZUUxjXG6_-w19uA4zDtHAz3TPB-W3IgiqP94Lcx37cXju2fN6PUQNFpoDdbMKDDdUICJow,"With increasingly complex network structure, requirements for heterogeneous 5G are also growing. The aim of this study is to meet the network security performance under the existing high-capacity and highly reliable transmission. In this context, deep learning technology is adopted to solve the security problem of the 5G heterogeneous network. First, the security problems existing in 5G heterogeneous networks are presented, mainly from two aspects of the physical layer security problems and application prospects of deep learning in communication technology. Then the combination of deep learning and 5G heterogeneous networks is analyzed. The combination of deep learning technology, modulation information recognition, and beam formation is introduced. The application of deep learning in communications technology is analyzed, and the modulation information recognition and beamforming based on deep learning are introduced. Finally, the challenges of solving security problems in 5G heterogeneous networks by deep learning are explored. The results show that the deep learning model can solve the modulation recognition problem well, and the modulation mode of the convolutional neural network can well identify the modulation signals involved in the experiment. Therefore, deep learning has a good advantage in solving modulation recognition. In addition, compared to the traditional algorithm, the unsupervised beamforming algorithm based on deep learning proposed in this research can effectively reduce the computational complexity under different numbers of transmitting antennas, which verifies the superiority of the unsupervised beamforming algorithm based on deep learning proposed in this research. Therefore, the present work provides a good idea for solving the security problem of 5G heterogeneous networks.",,10.1109/MNET.011.2000229,"Security , Heterogeneous networks , 5G mobile communication , Deep learning , Modulation , Communication networks , Communication systems "
71,Secure5G: A deep learning framework towards a secure network slicing in 5G and beyond,"A Thantharate, R Paropkari, V Walunj, ...",2020 10th Annual Computing and Communication Workshop and Conference (CCWC),,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9031158/?casa_token=k6Vl5YGxfTkAAAAA:ND4DyK_Rz4KAq33PTwOmgDN9trwrbnOq6j1DYBb5sRE_KU1vE8i_OpqexoxuNaT0_omd-Hykq6wafg,"Network Slicing will play a vital role in enabling a multitude of 5G applications, use cases, and services. Network slicing functions will provide an end-to-end isolation between slices with an ability to customize each slice based on the service demands (bandwidth, coverage, security, latency, reliability, etc.). Maintaining isolation of resources, traffic flow, and network functions between the slices is critical in protecting the network infrastructure system from Distributed Denial of Service (DDoS) attack. The 5G network demands and new feature sets to support ever-growing and complex business requirements have made existing approaches to network security inadequate. In this paper, we have developed a Neural Network based `Secure5G' Network Slicing model to proactively detect and eliminate threats based on incoming connections before they infest the 5G core network. `Secure5G' is a resilient model that quarantines the threats ensuring end-to-end security from device(s) to the core network, and to any of the external networks. Our designed model will enable the network operators to sell network slicing as-a-service to serve diverse services efficiently over a single infrastructure with high security and reliability.",,10.1109/CCWC47524.2020.9031158,"5G mobile communication , Network slicing , Computer crime , Computer architecture , 3GPP , Servers "
72,"Towards artificial intelligence enabled 6G: State of the art, challenges, and opportunities","S Zhang, D Zhu",Computer Networks,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S138912862031207X?casa_token=mpRKqDYl9dIAAAAA:SsgNJTNguppSPZ_6SqI266bCBLDX-XkDeV0tzPkc_tsFn0cSDGZNA6Gsn_8tEkcxmXAazy16LSM,"… of AI-enabled 6G system, the driving forces of introducing AI into 6G and the state of the art in machine learning. Then applying machine learning techniques to major 6G network issues …",,,
73,Redefining wireless communication for 6G: Signal processing meets deep learning with deep unfolding,"A Jagannath, J Jagannath, ...",IEEE Transactions on Artificial Intelligence,2691-4581,2,6,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9524496/?casa_token=rHG7Rr94KC8AAAAA:7KzN1bHBjou-EoHOcrho8YQ9up44feCOymiIp1bs13_nd4TTAomkMUipyl_V0lYCT6R1x2VN0wNIbA,"The year 2019 witnessed the rollout of the 5G standard, which promises to offer significant data rate improvement over 4G. While 5G is still in its infancy, there has been an increased shift in the research community for communication technologies beyond 5G. The recent emergence of machine learning approaches for enhancing wireless communications and empowering them with much-desired intelligence holds immense potential for redefining wireless communication for 6G. The evolving communication systems will be bottlenecked in terms of latency, throughput, and reliability by the underlying signal processing at the physical layer. In this position letter, we motivate the need to redesign iterative signal processing algorithms by leveraging deep unfolding techniques to fulfill the physical layer requirements for 6G networks. To this end, we begin by presenting the service requirements and the key challenges posed by the envisioned 6G communication architecture. We outline the deficiencies of the traditional algorithmic principles and data-hungry deep learning (DL) approaches in the context of 6G networks. Specifically, deep unfolded signal processing is presented by sketching the interplay between domain knowledge and DL. The deep unfolded approaches reviewed in this letter are positioned explicitly in the context of the requirements imposed by the next generation of cellular networks. Finally, this letter motivates open research challenges to truly realize hardware-efficient edge intelligence for future 6G networks. Impact Statement—In this letter, we discuss why the infusion of domain knowledge into machine learning frameworks holds the key to future embedded intelligent communication systems. Applying traditional signal processing and deep learning approaches independently entails significant computational and memory constraints. This becomes challenging in the context of future communication networks, such as 6G with significant communication demands where dense deployments of embedded Internet of Things (IoT) devices are envisioned. Hence, we put forth deep unfolded approaches as the potential enabling technology for 6G artificial intelligence (AI) radio to mitigate the computational and memory demands as well as to fulfill the future 6G latency, reliability, and throughput requirements. To this end, we present a general deep unfolding methodology that can be applied to iterative signal processing algorithms. Thereafter, we survey some initial steps taken in this direction and more importantly discuss the potential it has in overcoming challenges in the context of 6G requirements. This letter concludes by providing future research directions in this promising realm.",,10.1109/TAI.2021.3108129,"6G mobile communication , Physical layer , 5G mobile communication , Artificial intelligence , Wireless communication , Deep learning , Signal processing algorithms , Wireless networks "
74,Decoding optimization for 5G LDPC codes by machine learning,"X Wu, M Jiang, C Zhao",IEEE Access,2169-3536,6,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8458121/,"In this paper, we propose a generalized minimum-sum decoding algorithm using a linear approximation (LAMS) for protograph-based low-density parity-check (PB-LDPC) codes with quasi-cyclic (QC) structures. The linear approximation introduces some factors in each decoding iteration, which linearly adjust the check node updating and channel output. These factors are optimized iteratively using machine learning, where the optimization can be efficiently solved by a small and shallow neural network with training data produced by the LAMS decoder. The neural network is built according to the parity check matrix of a PB-LDPC code with a QC structure which can greatly reduce the size of the neural network. Since, we optimize the factors once per decoding iteration, the optimization is not limited by the number of the iterations. Then, we give the optimized results of the factors in the LAMS decoder and perform decoding simulations for PB-LDPC codes in fifth generation mobile networks (5G). In the simulations, the LAMS algorithm shows noticeable improvement over the normalized and the offset minimum-sum algorithms and even better performance than the belief propagation algorithm in some high signal-to-noise ratio regions.",,10.1109/ACCESS.2018.2869374,"Iterative decoding , Decoding , Approximation algorithms , Neural networks , Optimization , Machine learning "
75,Pirate: A blockchain-based secure framework of distributed machine learning in 5g networks,"S Zhou, H Huang, W Chen, P Zhou, Z Zheng, ...",IEEE Network,1558-156X,34,6,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9210138/?casa_token=enuDHdNUZI4AAAAA:wvbI8955JrzEWATYFvbQ6Za5Ad0uigIIMGvKBdLYyK43ncVbMA-1xSqfgnhrQH5kbNnBvlsPLbTfOQ,"In fifth-generation (5G) networks and beyond, communication latency and network bandwidth will be no longer be bottlenecks to mobile users. Thus, almost every mobile device can participate in distributed learning. That is, the availability issue of distributed learning can be eliminated. However, model safety will become a challenge. This is because the distributed learning system is prone to suffering from byzantine attacks during the stages of updating model parameters and aggregating gradients among multiple learning participants. Therefore, to provide the byzantine-resilience for distributed learning in the 5G era, this article proposes a secure computing framework based on the sharding technique of blockchain, namely PiRATE. To prove the feasibility of the proposed PiRATE, we implemented a prototype. A case study shows how the proposed PiRATE contributes to distributed learning. Finally, we also envision some open issues and challenges based on the proposed byzantine- resilient learning framework.",,10.1109/MNET.001.1900658,"Protocols , Machine learning , Computational modeling , Training , Peer-to-peer computing , 5G mobile communication "
76,Deep learning-based resource allocation for 5G broadband TV service,"P Yu, F Zhou, X Zhang, X Qiu, ...",IEEE Transactions on Broadcasting,1557-9611,66,4,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8989816/?casa_token=WpyiX0wWsyYAAAAA:XReOTm5HzzGDrST9m_L4upGAn-6mUOOwnApZABZM5ld4UyTfdRTvx3dIJBz38cy8uMFFoZnUX1S-9A,"The vision of next-generation TV is to support media services to achieve sharing of cross-domain experience, and the eMBB scenario of the 5G network is one of its important driving forces. Considering the bandwidth and resource requirements of different services, such as unicast and multicast services of multimedia TV broadcasting, rationally allocating resources while providing high-quality services and realizing green energy savings of base stations is one of the challenges. This paper is aimed at the resource allocation for TV multimedia service in the 5G wireless cloud network (C-RAN) scenario, which can support unicast services for cellular users and multicast services for broadcast services simultaneously, and it proposes the corresponding slice resources allocation architecture based on the concept of a self-organizing network. The management architecture first builds the functions and processes of the corresponding autonomous resource management. Based on the multidimensional data, an effective deep learning model named LSTM (long short-term memory) is used to construct the dynamic traffic model of the multicast service in space-time, which provides a basis for further network resource allocation. Based on the prediction results and the condition of satisfying the changing requirements of users, the corresponding optimization model is constructed with the goal of minimizing the energy usage of the RRHs (remote radio heads) and taking the QoS constraints of the users into account. A deep reinforcement learning (DRL) framework combined with a convex optimization method are then used to complete the users' bandwidth and power resource allocation. The experimental results show that the proposed method can not only predict the multicast service requirement accurately but also effectively improve the energy efficiency of the network under targeted QoS requirements along with time variations.",,10.1109/TBC.2020.2968730,"5G mobile communication , TV , Resource management , Predictive models , Digital multimedia broadcasting , Unicast , Wireless communication "
77,Deep learning for radio resource allocation with diverse quality-of-service requirements in 5G,"R Dong, C She, W Hardjawana, Y Li, ...",IEEE Transactions on Wireless Communications,1558-2248,20,4,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9286851/?casa_token=ALZx4paQP2YAAAAA:Rfhv7idi804_3Lr0IV5eFIbcZNRAFzNpQbb4BV2L6OwmA1-VCqYsVA1JupTK4R0-zIIB6yJXqXFqkg,"To accommodate diverse Quality-of-Service (QoS) requirements in 5th generation cellular networks, base stations need real-time optimization of radio resources in time-varying network conditions. This brings high computing overheads and long processing delays. In this work, we develop a deep learning framework to approximate the optimal resource allocation policy that minimizes the total power consumption of a base station by optimizing bandwidth and transmit power allocation. We find that a fully-connected neural network (NN) cannot fully guarantee the QoS requirements due to the approximation errors and quantization errors of the numbers of subcarriers. To tackle this problem, we propose a cascaded structure of NNs, where the first NN approximates the optimal bandwidth allocation, and the second NN outputs the transmit power required to satisfy the QoS requirement with given bandwidth allocation. Considering that the distribution of wireless channels and the types of services in the wireless networks are non-stationary, we apply deep transfer learning to update NNs in non-stationary wireless networks. Simulation results validate that the cascaded NNs outperform the fully connected NN in terms of QoS guarantee. In addition, deep transfer learning can reduce the number of training samples required to train the NNs remarkably.",,10.1109/TWC.2020.3041319,"Artificial neural networks , Quality of service , Resource management , Optimization , Ultra reliable low latency communication , Wireless networks , Delays "
78,Machine learning threatens 5G security,"J Suomalainen, A Juhola, S Shahabuddin, ...",IEEE Access,2169-3536,8,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9229146/,"Machine learning (ML) is expected to solve many challenges in the fifth generation (5G) of mobile networks. However, ML will also open the network to several serious cybersecurity vulnerabilities. Most of the learning in ML happens through data gathered from the environment. Un-scrutinized data will have serious consequences on machines absorbing the data to produce actionable intelligence for the network. Scrutinizing the data, on the other hand, opens privacy challenges. Unfortunately, most of the ML systems are borrowed from other disciplines that provide excellent results in small closed environments. The resulting deployment of such ML systems in 5G can inadvertently open the network to serious security challenges such as unfair use of resources, denial of service, as well as leakage of private and confidential information. Therefore, in this article we dig into the weaknesses of the most prominent ML systems that are currently vigorously researched for deployment in 5G. We further classify and survey solutions for avoiding such pitfalls of ML in 5G systems.",,10.1109/ACCESS.2020.3031966,"5G mobile communication , Data models , Machine learning , Wireless networks , Computer security , Data privacy "
79,Blockchain and artificial intelligence for dynamic resource sharing in 6G and beyond,"S Hu, YC Liang, Z Xiong, ...",IEEE Wireless Communications,1558-0687,28,4,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9382024/?casa_token=0MgZu2yT3icAAAAA:QMDPMssAmy0dptGu2IWZ1gBQXHkAsLxWlQYdVI5ROUlEtTuv4O_GIylaGpux7jkeCsbUKMKIrtPsow,"Wireless resources, such as spectrum, computation, infrastructures and so on, are critical in 6G and beyond. Dynamic Resource Sharing (DRS), which improves the resource utilization compared to the static and fixed resource allocation, thus needs to be further exploited. Blockchain and AI are two promising techniques for DRS in 6G and beyond. In this article, a blockchain and AI-empow-ered DRS architecture is proposed, where block-chain is used to achieve the functionalities in DRS with improved distribution, security and automation, and AI is implemented to improve the performance of pattern recognition and decision-making in DRS. Finally, a case study where dynamic spectrum sharing is implemented within the proposed architecture, and deep reinforcement learning is used and shown to optimize the profit ratio of the users.",,10.1109/MWC.001.2000409,"Blockchains , Artificial intelligence , Resource management , 6G mobile communication , Dynamic scheduling , Smart contracts , Pattern recognition "
80,"Artificial-intelligence-enabled air interface for 6G: Solutions, challenges, and standardization impacts","S Han, T Xie, I Chih-Lin, L Chai, Z Liu, ...",IEEE Communications Magazine,1558-1896,58,10,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9247527/?casa_token=Wh0QMQELzkcAAAAA:SFJ3fxI5SxmKNS3LFMbnQkAj9HUKQAPHjhngf9d-PfX1zalxU-rFEtk40WnsMAnmRoqIR4i_aBwPRg,"As 3GPP has completed Release 16 specifications and worldwide 5G commercialization is speeding up, global interest in 6G is starting to grow. An interesting and important question is: will the rapid progress in artificial intelligence (AI) eventually alleviate the tremendous efforts required for future standardization of 6G and beyond? In this article, the potential impacts of AI on the air interface design and standardization are investigated. The AI-enabled network architecture is first discussed. The higher layer, physical layer, and cross-layer design empowered by AI capability are further presented. Based on these designs, the future 6G and beyond are expected to enter into an AI era. For potential new use cases and more challenging requirements, the network is capable of automatic updating the air interface protocols, which may substantially reduce the standardization efforts and costs of wireless communication networks.",,10.1109/MCOM.001.2000218,"Artificial intelligence , 6G mobile communication , Protocols , 5G mobile communication , Communication channels , Wireless communication "
81,Distributed artificial intelligence solution for D2D communication in 5G networks,"I Ioannou, V Vassiliou, C Christophorou, ...",IEEE Systems Journal,2373-7816,14,3,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9080591/?casa_token=F_aJY5rMCtUAAAAA:cI3KXvHRzjUIbtgoAqjPhrAjDKmsH4L6rWlWgBVopVwWTIiDP6_Ee617zW0_G5NexBUHrnclSNYBCw,"Device-to-device (D2D) communication, a core technology component of the evolving fifth-generation (5G) architecture, promises improvements in energy efficiency, spectral efficiency, overall system capacity, and higher data rates. These improvements in network performance spearheaded a vast amount of research in D2D, which identified significant challenges that need to be addressed before realizing their full potential in 5G networks, and beyond. Toward this end, this article proposes the use of a distributed intelligent approach to control the generation of D2D networks. More precisely, the proposed approach uses Belief Desire Intention (BDI) intelligent agents with extended capabilities (BDIx) to manage each D2D node independently and autonomously, without the help of the base station. To illustrate the above, this article proposes the DAIS algorithm for the decision of transmission mode in D2D, which maximizes the data rate and minimizes the power consumption in the network, while taking into consideration the computational load. Simulations show the applicability of BDI agents in solving D2D challenges.",,10.1109/JSYST.2020.2979044,"Device-to-device communication , Interference , 5G mobile communication , Relays , Intelligent agents , Cellular networks , Artificial intelligence "
82,Machine learning prediction approach to enhance congestion control in 5G IoT environment,"IA Najm, AK Hamoud, J Lloret, I Bosch",Electronics,,,,,2019,mdpi.com,https://www.mdpi.com/471384,… three machine learning techniques in congestion control in 5G … machine learning prediction techniques on hardware and test it in a real test-bed environment. Machine learning and 5G …,,,
83,When 5G meets deep learning: a systematic review,"GL Santos, PT Endo, D Sadok, J Kelner",Algorithms,,,,,2020,mdpi.com,https://www.mdpi.com/807044,"… spanning the used different deep learning models applied to 5G networks. We also cover other problems present in 5G networks, that demand the use of different deep learning models. …",,,
84,Machine learning-based network sub-slicing framework in a sustainable 5g environment,"SK Singh, MM Salim, J Cha, Y Pan, JH Park",Sustainability,,,,,2020,mdpi.com,https://www.mdpi.com/787976,"… sub-slicing in a 5G network environment based on Machine Learning, where resources are … Machine learning-based algorithms are used to learn the features of the application as well …",,,
85,Deep learning architectures for accurate millimeter wave positioning in 5G,"J Gante, G Falcao, L Sousa",Neural Processing Letters,,,,,2020,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11063-019-10073-1&casa_token=zx9_Pr5HnUMAAAAA:tv181gofo0ShfU2BqwkO0AY2Ue3b0PO-y2-ahLMFr6gco0zm7OZdyD7IgsgTUi_cLqdwBoqsaDwafxzgbg,"… 5G is expected to bring new wireless communication capabilities, yet at a cost of additional challenges. One of 5G’… This paper discusses a new system that, making use of deep learning …",,,
86,Deep learning and blockchain-empowered security framework for intelligent 5G-enabled IoT,"S Rathore, JH Park, H Chang",IEEE Access,2169-3536,9,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9420742/,"Recently, many IoT applications, such as smart transportation, healthcare, and virtual and augmented reality experiences, have emerged with fifth-generation (5G) technology to enhance the Quality of Service (QoS) and user experience. The revolution of 5G-enabled IoT supports distinct attributes, including lower latency, higher system capacity, high data rate, and energy saving. However, such revolution also delivers considerable increment in data generation that further leads to a major requirement of intelligent and effective data analytic operation across the network. Furthermore, data growth gives rise to data security and privacy concerns, such as breach and loss of sensitive data. The conventional data analytic and security methods do not meet the requirement of 5G-enabled IoT including its unique characteristic of low latency and high throughput. In this paper, we propose a Deep Learning (DL) and blockchain-empowered security framework for intelligent 5G-enabled IoT that leverages DL competency for intelligent data analysis operation and blockchain for data security. The framework's hierarchical architecture wherein DL and blockchain operations emerge across the four layers of cloud, fog, edge, and user is presented. The framework is simulated and analyzed, employing various standard measures of latency, accuracy, and security to demonstrate its validity in practical applications.",,10.1109/ACCESS.2021.3077069,"Security , Internet of Things , Data analysis , Blockchain , Reliability , 5G mobile communication , Quality of service "
87,"Blockchain and artificial intelligence for 5G‐enabled Internet of Things: Challenges, opportunities, and solutions","A Dhar Dwivedi, R Singh, K Kaushik, ...",Transactions on …,,,,,2021,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.4329?casa_token=KSIV-NvJeqwAAAAA:Eumv6c4zfIEjjhpweE40DveIuQnrAdUAlUq1xaqoNUE1naELzb-J3AoXepQ76UoFoyG_jnRJsDhx28Yg,… is a major issue where the 5G-enabled environment plays an … Merging artificial intelligence to 5G wireless systems solves … of providing functionality for a 5G-enabled IoT based network. …,,,
88,On the application of machine learning to the design of UAV-based 5G radio access networks,"V Kouhdaragh, F Verde, G Gelli, J Abouei",Electronics,,,,,2020,mdpi.com,https://www.mdpi.com/699178,"A groundbreaking design of radio access networks (RANs) is needed to fulfill 5G traffic requirements. To this aim, a cost-effective and flexible strategy consists of complementing …",,,
89,Patient-centric HetNets powered by machine learning and big data analytics for 6G networks,"MS Hadi, AQ Lawey, TEH El-Gorashi, ...",IEEE Access,2169-3536,8,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9086596/,"Having a cognitive and self-optimizing network that proactively adapts not only to channel conditions, but also according to its users' needs can be one of the highest forthcoming priorities of future 6G Heterogeneous Networks (HetNets). In this paper, we introduce an interdisciplinary approach linking the concepts of e-healthcare, priority, big data analytics (BDA) and radio resource optimization in a multi-tier 5G network. We employ three machine learning (ML) algorithms, namely, naïve Bayesian (NB) classifier, logistic regression (LR), and decision tree (DT), working as an ensemble system to analyze historical medical records of stroke out-patients (OPs) and readings from body-attached internet-of-things (IoT) sensors to predict the likelihood of an imminent stroke. We convert the stroke likelihood into a risk factor functioning as a priority in a mixed integer linear programming (MILP) optimization model. Hence, the task is to optimally allocate physical resource blocks (PRBs) to HetNet users while prioritizing OPs by granting them high gain PRBs according to the severity of their medical state. Thus, empowering the OPs to send their critical data to their healthcare provider with minimized delay. To that end, two optimization approaches are proposed, a weighted sum rate maximization (WSRMax) approach and a proportional fairness (PF) approach. The proposed approaches increased the OPs' average signal to interference plus noise (SINR) by 57% and 95%, respectively. The WSRMax approach increased the system's total SINR to a level higher than that of the PF approach, nevertheless, the PF approach yielded higher SINRs for the OPs, better fairness and a lower margin of error.",,10.1109/ACCESS.2020.2992555,"Optimization , Machine learning , Big Data , Stroke (medical condition) , Resource management , Support vector machines , Interference "
90,Towards an FPGA-Accelerated programmable data path for edge-to-core communications in 5G networks,"R Ricart-Sanchez, P Malagon, P Salva-Garcia, ...",Journal of Network and …,,,,,2018,Elsevier,https://www.sciencedirect.com/science/article/pii/S1084804518302923?casa_token=a15Ct0SsEVIAAAAA:VWYWJyXmA3gDh8hWjOH7jMAgraLChPJluwWwkj23Q2im3Y2qBw0utG0zZg_LPsN8Pl9ENlX5fZQ,… of a FPGA NIC with nested encapsulation support for hardware acceleration suitable for 5G … architecture to achieve an FPGA-accelerated multi-tenant 5G network data path for the Edge…,,,
91,A survey on deep learning for ultra-reliable and low-latency communications challenges on 6G wireless systems,"A Salh, L Audah, NSM Shah, A Alhammadi, ...",IEEE Access,2169-3536,9,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9389782/,"The sixth generation (6G) wireless communication network presents itself as a promising technique that can be utilized to provide a fully data-driven network evaluating and optimizing the end-to-end behavior and big volumes of a real-time network within a data rate of Tb/s. In addition, 6G adopts an average of 1000+ massive number of connections per person in one decade (2030 virtually instantaneously). The data-driven network is a novel service paradigm that offers a new application for the future of 6G wireless communication and network architecture. It enables ultra-reliable and low latency communication (URLLC) enhancing information transmission up to around 1 Tb/s data rate while achieving a 0.1 millisecond transmission latency. The main limitation of this technique is the computational power available for distributing with big data and greatly designed artificial neural networks. The work carried out in this paper aims to highlight improvements to the multi-level architecture by enabling artificial intelligence (AI) in URLLC providing a new technique in designing wireless networks. This is done through the application of learning, predicting, and decision-making to manage the stream of individuals trained by big data. The secondary aim of this research paper is to improve a multi-level architecture. This enables user level for device intelligence, cell level for edge intelligence, and cloud intelligence for URLLC. The improvement mainly depends on using the training process in unsupervised learning by developing data-driven resource management. In addition, improving a multi-level architecture for URLLC through deep learning (DL) would facilitate the creation of a data-driven AI system, 6G networks for intelligent devices, and technologies based on an effective learning capability. These investigational problems are essential in addressing the requirements in the creation of future smart networks. Moreover, this work provides further ideas on several research gaps between DL and 6G that are up-to-date unknown.",,10.1109/ACCESS.2021.3069707,"6G mobile communication , Ultra reliable low latency communication , Artificial intelligence , Computer architecture , Wireless networks , Bandwidth , Reliability "
92,"The role of artificial intelligence driven 5G networks in COVID-19 outbreak: Opportunities, challenges, and future outlook","AI Abubakar, KG Omeke, M Ozturk, ...",Frontiers in …,,,,,2020,frontiersin.org,https://www.frontiersin.org/articles/10.3389/frcmn.2020.575065/full,"… mobile networks (5G) is at its … 5G empowered by artificial intelligence in tackling these problems. In addition, we also provide a brief insight on the use of artificial intelligence driven 5G …",,,
93,5G standards for the Industry 4.0 enabled communication systems using artificial intelligence: perspective of smart healthcare system,"B Alhayani, AS Kwekha-Rashid, HB Mahajan, ...",Applied …,,,,,2022,Springer,https://link.springer.com/article/10.1007/s13204-021-02152-4,"… and artificial intelligence. Computer-based intelligence will emphatically affect a few key regions of 5G … Hence, for IoT frameworks, simulated intelligence will improve the 5G from start to …",,,
94,Machine learning based anomaly detection for 5g networks,"J Lam, R Abbas",arXiv preprint arXiv:2003.03474,,,,,2020,arxiv.org,https://arxiv.org/abs/2003.03474,"… 5G and IoT security use cases. The goal of this project is to provide some insight into the effectiveness of machine learning … greater flexibility, scalabililty and portability in a 5G network. …",,,
95,A survey on artificial intelligence techniques in cognitive radio networks,"R Ganesh Babu, V Amudha",Emerging Technologies in Data Mining and …,,,,,2019,Springer,https://link.springer.com/chapter/10.1007/978-981-13-1951-8_10,"… Cognitive radio (CR) is the solution for the current spectral underutilized problems, Context … will become Cognitive Radio by imparting intelligence to SDR using Artificial Intelligence …",,,
96,GPF: A GPU-based Design to Achieve~ 100 μs Scheduling for 5G NR,"Y Huang, S Li, YT Hou, W Lou",Proceedings of the 24th Annual …,,,,,2018,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3241539.3241552,"… In this paper, we present the design of GPF – a GPU-based proportional fair (PF) scheduler … fit into a GPU. By implementing GPF on an off-the-shelf Nvidia Quadro P6000 GPU, we show …",,,
97,Role of machine learning and deep learning in securing 5G-driven industrial IoT applications,"P Sharma, S Jain, S Gupta, V Chamola",Ad Hoc Networks,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S1570870521001906?casa_token=w0K8Ori1dSkAAAAA:wjVw9JHMtb7hCMTzesFLZOaYA2bbre3uF8Ny1UCrg89gNC85J9ZjYFYQSxDxcHeVUHi_A-NE_eA,… with emergent technologies like 5G and blockchain affects … the use of machine learning (ML) and deep learning (DL) … for I-IoT deployed over 5G and blockchain. The survey provides a …,,,
98,Dethroning GPS: Low-power accurate 5G positioning systems using machine learning,"J Gante, L Sousa, G Falcao",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,2156-3365,10,2,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9080126/?casa_token=8VGVqAmUfI0AAAAA:mnaMfn6yW1PbApsYy3PJpD2ygRZDxmjaJckHWjxz7euJLbj_Yz3qfBVQUqkTI1sAoCjKskUGlZAPCw,"Over the last years positioning systems have become increasingly pervasive, covering most of the planet's surface. Although they are accurate enough for a large number of uses, their precision, power consumption, and hardware requirements establish the limits for their adoption in mobile devices. In this paper, the energy consumption of a proposed deep learning-based millimeter wave positioning method is assessed, being subsequently compared to the state-of-the-art on accurate outdoor positioning systems. Requiring as low as 0.4 mJ per position fix, when compared to the most recent assisted-GPS implementations the proposed method has energy efficiency gains of 47× and 85× for continuous and sporadic position fixes (respectively), while also having slightly lower estimation errors. Therefore, the proposed method significantly reduces the energy required for precise positioning in the presence of millimeter wave networks, enabling the design of more efficient and accurate positioning-enabled mobile devices.",,10.1109/JETCAS.2020.2991024,"Mobile handsets , Global navigation satellite system , Satellites , Circuits and systems , Energy consumption , Global Positioning System , Millimeter wave technology "
99,Fpga for 5g: Re-configurable hardware for next generation communication,"V Chamola, S Patra, N Kumar, ...",IEEE Wireless Communications,1558-0687,27,3,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9076114/?casa_token=kx87xaXGJuUAAAAA:7gvnUDWl6Vsngm9Qe-qQoqX30VJopqAu-zTtcYvrV8CcGqGIsWwDoymgv-KtnW-m-yak06G9SWSrig,"Next generation communication relies on standardized protocols, heterogeneous architectures and advanced technologies that are envisioned to bring ubiquitous and seamless connectivity. This evolution of communication will not only improve the performance of the existing networks, but will also enable various applications in other fields while integrating different heterogeneous systems. This massive scaling of mobile communication requires higher bandwidth to operate. 5G promises a robust solution by offering ultra-low latency and high bandwidth for data transmission. To provide individuals and companies with a real-time, social, and all connected experience, an end-to-end coordinated architecture which is agile and intelligent has to be designed at each stage. As FPGA has the potential to be resource/power efficient, it can be used for building up constituents of 5G infrastructure. It can accelerate network performance without making a large investment in new hardware. Dynamic reconfigurability and in-field programming features of FPGAs compared to fixed function ASICs help in developing better wireless systems. This article presents various application areas of FPGAs for the upcoming 5G network planning.",,10.1109/MWC.001.1900359,"Field programmable gate arrays , 5G mobile communication , Virtualization , Bandwidth , Hardware , Cloud computing , Macrocell networks "
100,Deep learning at the physical layer: System challenges and applications to 5G and beyond,"F Restuccia, T Melodia",IEEE Communications Magazine,1558-1896,58,10,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9247524/?casa_token=ZEqe5ynCC0UAAAAA:c2_LVp5kDLS1AtgqowAmwP-M3o_DeaRhl1NvJbYq33oAqAXfLUeQPKQhfkyZDHq0FqB3Ab62z21KpQ,"The unprecedented requirements of IoT have made fine-grained optimization of spectrum resources an urgent necessity. Thus, designing techniques able to extract knowledge from the spectrum in real time and select the optimal spectrum access strategy accordingly has become more important than ever. Moreover, 5G networks will require complex management schemes to deal with problems such as adaptive beam management and rate selection. Although deep learning (DL) has been successful in modeling complex phenomena, commercially available wireless devices are still very far from actually adopting learning-based techniques to optimize their spectrum usage. In this article, we first discuss the need for real-time DL at the physical layer, and then summarize the current state of the art and existing limitations. We conclude the article by discussing an agenda of research challenges and how DL can be applied to address crucial problems in 5G and beyond networks.",,10.1109/MCOM.001.2000243,"Real-time systems , Wireless communication , Hardware , Neural networks , Feature extraction , Modulation , Deep learning "
101,"Towards energy efficient 5G networks using machine learning: Taxonomy, research challenges, and future research directions","A Mughees, M Tahir, MA Sheikh, A Ahad",IEEE Access,2169-3536,8,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9218920/,"As the world pushes toward the use of greener technology and minimizes energy waste, energy efficiency in the wireless network has become more critical than ever. The next-generation networks, such as 5G, are being designed to improve energy efficiency and thus constitute a critical aspect of research and network design. The 5G network is expected to deliver a wide range of services that includes enhanced mobile broadband, massive machine-type communication and ultra-reliability, and low latency. To realize such a diverse set of requirement, 5G network has evolved as a multi-layer network that uses various technological advances to offer an extensive range of wireless services. Several technologies, such as software-defined networking, network function virtualization, edge computing, cloud computing, and small cells, are being integrated into the 5G networks to fulfill the need for diverse requirements. Such a complex network design is going to result in increased power consumption; therefore, energy efficiency becomes of utmost importance. To assist in the task of achieving energy efficiency in the network machine learning technique could play a significant role and hence gained significant interest from the research community. In this paper, we review the state-of-art application of machine learning techniques in the 5G network to enable energy efficiency at the access, edge, and core network. Based on the review, we present a taxonomy of machine learning applications in 5G networks for improving energy efficiency. We discuss several issues that can be solved using machine learning regarding energy efficiency in 5G networks. Finally, we discuss various challenges that need to be addressed to realize the full potential of machine learning to improve energy efficiency in the 5G networks. The survey presents a broad range of ideas related to machine learning in 5G that addresses the issue of energy efficiency in virtualization, resource optimization, power allocation, and incorporating enabling technologies of 5G can enhance energy efficiency.",,10.1109/ACCESS.2020.3029903,"5G mobile communication , Machine learning , Energy consumption , Massive MIMO , Virtualization , Hardware , Resource management "
102,"Artificial intelligence for 5g wireless systems: Opportunities, challenges, and future research direction","Y Arjoune, S Faruque",2020 10th Annual Computing and Communication Workshop and Conference (CCWC),,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9031117/?casa_token=jOhDSc5_hrsAAAAA:JKr4EZDTEYffmmXk48DtnIu4I_5DCRI5WzG6-gX2GCHvgIrVvMgFZls9MzmZKiOryvKjoYpQAwikLg,"The advent of the wireless communications systems augurs new cutting-edge technologies, including self-driving vehicles, unmanned aerial systems, autonomous robots, the Internet-of-Things, and virtual reality. These technologies require high data rates, ultra-low latency, and high reliability, all of which are promised by the fifth generation of wireless communication systems (5G). Many research groups state that 5G cannot meet its demands without artificial intelligence (A.I.) integration as 5G wireless networks are expected to generate unprecedented traffic giving wireless research designers access to big data that can help in predicting the demands and adjust cell designs to meet the users' requirements. Subsequently, many researchers applied A.I. in many aspects of 5G wireless communication design including radio resource allocation, network management, cyber-security. In this paper, we provide an in-depth review of A.I. for 5G wireless communication systems. in this respect, the aim of this paper is to survey A.I. in 5G wireless communication and networking by discussing many case studies, discuss the challenges, and shed new light on future research directions for leveraging A.I. in 5G wireless communications.",,10.1109/CCWC47524.2020.9031117,"5G mobile communication , Machine learning , Wireless communication , Modulation , Decoding , MIMO communication , Signal to noise ratio "
103,Prediction-based conditional handover for 5G mm-Wave networks: A deep-learning approach,"C Lee, H Cho, S Song, JM Chung",IEEE Vehicular Technology Magazine,1556-6080,15,1,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8959359/?casa_token=8UFtjTBvHcgAAAAA:7wKzC0FD0tac9OE3XmGjmHjwK7oKm4EhCaw73cwQod24OhqIctA-8R4hZ31boA8x0N5NO_nbtj7sbw,"Conditional handover (CHO) is one of several promising mobility enhancements in 5G networks. By making preparation decisions earlier than in LTE HO, CHO can provide an improved HO success rate. This article, analyzes the strengths and weaknesses of CHO by comparing CHO to 5G baseline HO. Since millimeter-wave communications are vulnerable to blockages, sudden changes in signal reception power can mislead CHO into making undesired early preparations in 5G networks. To enhance the robustness of CHO, current studies propose using an increased number of preparations, resulting in considerable signaling overhead. This article offers a novel prediction-based CHO (PCHO) scheme that uses deep-learning technology to overcome the weaknesses of CHO and make more intelligent preparation decisions. Based on the changes in the signal patterns of the base stations, PCHO uses former blockage information to predict the best next base station to which to conduct HO. Performance evaluation demonstrates that PCHO can improve the early preparation success rate while reducing signaling overhead compared to current CHO schemes.",,10.1109/MVT.2019.2959065,"5G mobile communication , Handover , Long Term Evolution , Reliability , Base stations , 3GPP "
104,Deep learning-aided 5G channel estimation,"A Le Ha, T Van Chien, TH Nguyen, ...","2022 4th Global Power, Energy and Communication Conference (GPECOM)",,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9377351/?casa_token=50w_5CPQJbcAAAAA:xe7xIlL0cGtdR7kB5haAZAdPl4bJdUgKmzNRhPxP_Uh3S1J2OaSBT0qhC9n99uIRaC65tUD5-YNhHg,"The defining feature of the Fifth Generation (5G) mobile communication systems is going to be Multiple Input Multiple Output (MIMO) transmission scheme, which utilizes the multipath diversities to achieve beamforming and increase spectral efficiency. However, these MIMO algorithms rely on accurate channel parameters. To improve the accuracy of the channel coefficients, the study presents a Deep Learning (DL) based approach that uses the 5G Demodulation Reference Signals (DMRS) as training sequence and Deep Neural Networks (DNN) as training and prediction network in a MIMO scenario. The DNN is trained with training data obtained by applying Least Squares (LS) method to the received pilot signals and by comparing it to Clustered Delay Line (CDL) channel model. The DNN is then used to predict real-time channel coefficients. The results show that the model improves channel estimation performance by reducing the effects of noise, thus improving the Normalized Mean Square Error (NMSE) versus Signal-to-Noise Ratio (SNR) metric of the MIMO system.",,10.1109/GPECOM55404.2022.9815811,"Training , Deep learning , 5G mobile communication , Spectral efficiency , Neural networks , Channel estimation , Training data "
105,Adversarial machine learning for 5G communications security,"YE Sagduyu, T Erpek, Y Shi",… Theory and Machine Learning for …,,,,,2021,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119723950.ch14,… machine learning to the 5G … of machine learning applications in 5G with two motivating examples and show how adversarial machine learning provides a new attack surface in 5G …,,,
106,"Artificial Intelligence and Machine Learning in 5G Network Security: Opportunities, advantages, and future research trends","N Haider, MZ Baig, M Imran",arXiv preprint arXiv:2007.04490,,,,,2020,arxiv.org,https://arxiv.org/abs/2007.04490,"… As 5G networks’ primary selling point has been higher data … 5G network security, their implications and possible research directions. Also, an overview of key data collection points in 5G …",,,
107,Deep-learning-based intelligent intervehicle distance control for 6G-enabled cooperative autonomous driving,"X Chen, S Leng, J He, L Zhou",IEEE Internet of Things Journal,2372-2541,8,20,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9310315/?casa_token=Q1bZvfxZCJMAAAAA:Q_W2dfaoq76ykCjZC9lM3X5z_GvWE6Ey4Be-MwsH2FkjAlZ2iNElW7pkkcfjDGeF0CKYkaUe9wZJPw,"Research on the sixth-generation cellular networks (6G) is gaining huge momentum to achieve ubiquitous wireless connectivity. Connected autonomous vehicles (CAVs) is a critical vertical application for 6G, holding great potentials of improving road safety, road and energy efficiency. However, the stringent service requirements of CAV applications on reliability, latency, and high speed communications will present big challenges to 6G networks. New channel access algorithms and intelligent control schemes for connected vehicles are needed for 6G-supported CAV. In this article, we investigated 6G-supported cooperative driving, which is an advanced driving mode through information sharing and driving coordination. First, we quantify the delay upper bounds of 6G vehicle-to-vehicle (V2V) communications with hybrid communication and channel access technologies. A deep learning neural network is developed and trained for the fast computation of the delay bounds in real-time operations. Then, an intelligent strategy is designed to control the intervehicle distance for cooperative autonomous driving. Furthermore, we propose a Markov chain-based algorithm to predict the parameters of the system states, and also a safe distance mapping method to enable smooth vehicular speed changes. The proposed algorithms are implemented in the AirSim autonomous driving platform. Simulation results show that the proposed algorithms are effective and robust with safe and stable cooperative autonomous driving, which greatly improve the road safety, capacity, and efficiency.",,10.1109/JIOT.2020.3048050,"6G mobile communication , Delays , Autonomous vehicles , Upper bound , Safety , Real-time systems , Prediction algorithms "
108,Throughput prediction using machine learning in lte and 5g networks,"D Minovski, N Ogren, C Ahlund, ...",IEEE Transactions on Mobile Computing,2161-9875,22,3,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9495144/?casa_token=3AglNBAlXvMAAAAA:vZlglUi2jGvlVKEuOxsYlbfqaBMeqY-OHnM9Vdbrmd_cN5TJvRP-uRvLOFsWoUTQ-DdC84p0gIyucw,"The emergence of novel cellular network technologies, within 5G, are envisioned as key enablers of a new set of use-cases, including industrial automation, intelligent transportation, and tactile internet. The critical nature of the traffic requirements ranges from ultra-reliable communications, massive connectivity, and enhanced mobile broadband. Thus, the growing research on cellular network monitoring and prediction aims for ensuring a satisfied user-base and fulfillment of service level agreements. The scope of this study is to develop an approach for predicting the cellular link throughput of end-users, with a goal to benchmark the performance of network slices. First, we report and analyze a measurement study involving real-life cases, such as driving in urban, sub-urban, and rural areas, as well as tests in large crowded areas. Second, we develop machine learning models using lower-layer metrics, describing the radio environment, to predict the available throughput. The models are initially validated on the LTE network and then applied to a non-standalone 5G network. Finally, we suggest scaling the proposed model into the future standalone 5G network. We have achieved 93 and 84 percent $R^2$R2 accuracy, with 0.06 and 0.17 mean squared error, in predicting the end-user's throughput in LTE and non-standalone 5G network, respectively.",,10.1109/TMC.2021.3099397,"Throughput , Long Term Evolution , 5G mobile communication , Quality of service , Measurement , Predictive models , Interference "
109,"From 5G to 6G technology: meets energy, internet-of-things and machine learning: a survey","MN Mahdi, AR Ahmad, QS Qassim, H Natiq, ...",Applied Sciences,,,,,2021,mdpi.com,https://www.mdpi.com/2076-3417/11/17/8117,"… and aspects of 5G and 6G focusing on the projected 5G and 6G system architecture, … of ML usage in incorporating energy and IoT in 5G and 6G, applications, and use cases. The …",,,
110,A survey of machine learning applications to handover management in 5G and beyond,"MS Mollel, AI Abubakar, M Ozturk, SF Kaijage, ...",IEEE Access,2169-3536,9,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9381854/,"Handover (HO) is one of the key aspects of next-generation (NG) cellular communication networks that need to be properly managed since it poses multiple threats to quality-of-service (QoS) such as the reduction in the average throughput as well as service interruptions. With the introduction of new enablers for fifth-generation (5G) networks, such as millimetre wave (mm-wave) communications, network densification, Internet of things (IoT), etc., HO management is provisioned to be more challenging as the number of base stations (BSs) per unit area, and the number of connections has been dramatically rising. Considering the stringent requirements that have been newly released in the standards of 5G networks, the level of the challenge is multiplied. To this end, intelligent HO management schemes have been proposed and tested in the literature, paving the way for tackling these challenges more efficiently and effectively. In this survey, we aim at revealing the current status of cellular networks and discussing mobility and HO management in 5G alongside the general characteristics of 5G networks. We provide an extensive tutorial on HO management in 5G networks accompanied by a discussion on machine learning (ML) applications to HO management. A novel taxonomy in terms of the source of data to be utilized in training ML algorithms is produced, where two broad categories are considered; namely, visual data and network data. The state-of-the-art on ML-aided HO management in cellular networks under each category is extensively reviewed with the most recent studies, and the challenges, as well as future research directions, are detailed.",,10.1109/ACCESS.2021.3067503,"5G mobile communication , Visualization , Cellular networks , 6G mobile communication , Bandwidth , Taxonomy , Quality of service "
111,Customized slicing for 6G: Enforcing artificial intelligence on resource management,"W Guan, H Zhang, VCM Leung",IEEE Network,1558-156X,35,5,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9382026/?casa_token=aaiNFaMnFgUAAAAA:OlZf7dHK2t9zz-1RKEgTquBHdxq1J6Dx128lz7FpePttyUi9pgBY9VIo1JaB9aujFBHhmTo0LCyIag,"Next generation wireless networks are expected to support diverse vertical industries and offer countless emerging use cases. To satisfy stringent requirements of diversified services, network slicing is developed, which enables service-oriented resource allocation by tailoring the infrastructure network into multiple logical networks. However, there are still some challenges in cross-domain multi-dimensional resource management for end-to-end (E2E) slices under the dynamic and uncertain environment. Trading off the revenue and cost of resource allocation while guaranteeing service quality is significant to tenants. Therefore, this article introduces a hierarchical resource management framework, utilizing deep reinforcement learning in admission control of resource requests from different tenants and resource adjustment within admitted slices for each tenant. In particular, we first discuss the challenges in customized resource management of 6G. Second, the motivation and background are presented to explain why artificial intelligence (AI) is applied in resource customization of multi-tenant slicing. Third, E2E resource management is decomposed into two problems, multi-dimensional resource allocation decision based on slice-level feedback, and real-time slice adaption aimed at avoiding service quality degradation. Simulation results demonstrate the effectiveness of AI-based customized slicing. Finally, several significant challenges that need to be addressed in practical implementation are investigated.",,10.1109/MNET.011.2000644,"Resource management , 6G mobile communication , Network slicing , Dynamic scheduling , Decision making , Real-time systems "
112,UAVs joint optimization problems and machine learning to improve the 5G and Beyond communication,"Z Ullah, F Al-Turjman, U Moatasim, L Mostarda, ...",Computer Networks,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S1389128620311518?casa_token=M03r0XT3-qgAAAAA:vbFzkM_Bs9IS9IQeLrFFrHaVEuxwHb1GbcIZroXjC438IsaJjEN8cmGrHUmmiqmIEYAkzOBKr00,"… an effective candidate for many applications in 5G and Beyond communications. The UAVs-… various techniques and technologies like artificial intelligence (AI), machine learning (ML), …",,,
113,Machine learning-based 5G-and-beyond channel estimation for MIMO-OFDM communication systems,"HA Le, T Van Chien, TH Nguyen, H Choo, VD Nguyen",Sensors,,,,,2021,mdpi.com,https://www.mdpi.com/1191314,"… , deep learning has demonstrated significant improvements in enhancing the communication reliability and reducing the computational complexity of 5G-… assistance of deep learning in …",,,
114,Machine learning in beyond 5G/6G networks—State-of-the-art and future trends,"VP Rekkas, S Sotiroudis, P Sarigiannidis, S Wan, ...",Electronics,,,,,2021,mdpi.com,https://www.mdpi.com/1357474,"Artificial Intelligence (AI) and especially Machine Learning (ML) can play a very important role in realizing and optimizing 6G network applications. In this paper, we present a brief …",,,
115,Nine challenges in artificial intelligence and wireless communications for 6G,"W Tong, GY Li",IEEE Wireless Communications,1558-0687,29,4,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9770094/?casa_token=r4uCtBTNBWYAAAAA:xTAnVV37fvEPfY-sXOtp9Fp-0Clk607c9mjj6sgTP2JgR846F5pg2S2lvHGXS_aISZLm64_Fo5i_Gw,"In recent years, artificial intelligence (AI) techniques, especially machine learning (ML), have been successfully applied in various areas, leading to a widespread belief that AI will collectively play an important role in future wireless communications. To accomplish the aspiration, we present nine challenges to be addressed by the interdisciplinary areas of AI/ML and wireless communications, with particular focus on the sixth generation (6G) wireless networks. Specifically, this article classifies the nine challenges into computation in AI, distributed neural networks and learning, and semantic communications.",,10.1109/MWC.006.2100543,"Artificial intelligence , Biological neural networks , Neural networks , 6G mobile communication , Deep learning , Wireless networks , Data models "
116,Leveraging machine-learning for D2D communications in 5G/beyond 5G networks,"S Hashima, BM ElHalawany, K Hatano, K Wu, ...",Electronics,,,,,2021,mdpi.com,https://www.mdpi.com/960106,"Device-to-device (D2D) communication is a promising paradigm for the fifth generation (5G) and beyond 5G (B5G) networks. Although D2D communication provides several benefits, …",,,
117,The role of deep learning in NOMA for 5G and beyond communications,"MK Hasan, M Shahjalal, MM Islam, ...",2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9065219/?casa_token=H9X7LuiDOjUAAAAA:CTM8rqJGufQuflE97rqkksK2aqyLpjo-H_6jkt2JY59OtZFn6XbFsl6mL-gxZIQpF19bSzOmfqj7zg,"In the coming future, it is obvious that the wireless networks will be congested with massive amounts of data traffic with the increasing number of users. Current multiple access techniques will certainly not have the capability to efficiently serve in the massively congested scenarios. In recent times, nonorthogonal multiple access (NOMA) has been recognized as an immensely potential technique for 5G and beyond communications that can increase spectral efficiency to a greater extent serving a vast number of users. However, several circumscriptions are observed in NOMA such as the requirement of a perfect channel state information in the transmitter and high computational complexity in the receiver. The use of deep learning (DL) techniques is a great resolution to deal with the challenges. This paper discusses the applications of the DL methods in NOMA for 5G and beyond communications. Firstly, the deep neural networks that are employed in NOMA are listed up and discussed. After that, their functions are studied specifically focusing on how to improve the NOMA performance. Finally, the possible future challenges and research issues are identified at the end of the paper.",,10.1109/ICAIIC48513.2020.9065219,"NOMA , Resource management , 5G mobile communication , Neural networks , Training , Machine learning , Silicon carbide "
118,Implementing deep learning techniques in 5G IoT networks for 3D indoor positioning: DELTA (DeEp Learning-Based Co-operaTive Architecture),"B El Boudani, L Kanaris, A Kokkinis, M Kyriacou, ...",Sensors,,,,,2020,mdpi.com,https://www.mdpi.com/838928,"… In this work, an experimental 5G testbed has been designed integrating C-RAN and IoT … Localization) in a 5G IoT environment. To achieve this, we propose the DEep Learning-based co-…",,,
119,"A review of deep learning in 5G research: Channel coding, massive MIMO, multiple access, resource allocation, and network security","A Ly, YD Yao",IEEE Open Journal of the Communications Society,2644-125X,2,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9353849/,"The current development of 5G technology is flourishing with widespread deployment across the world at a rapid pace. However, there is still a demand concerning 5G research for service and performance improvement. Research tasks include but are not limited to quality-of-service (QoS), energy efficiency, massive connectivity, reliable communications, and security. Due to the advancement of deep learning, numerous such research has utilized this technique. This article provides a comprehensive review of 5G communications research using deep learning. Specifically, we address the issues of low-density parity-check (LDPC) coding, massive multiple-input multiple-output (MIMO), non-orthogonal multiple access (NOMA), resource allocation, and security.",,10.1109/OJCOMS.2021.3058353,"Deep learning , NOMA , 5G mobile communication , Wireless networks , Massive MIMO , Quality of service , Parity check codes "
120,5G signal identification using deep learning,"MH Alhazmi, M Alymani, H Alhazmi, ...",2020 29th Wireless and Optical Communications Conference (WOCC),2379-1268,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9114912/?casa_token=ldg168Gy8UkAAAAA:0ETpdvkpKxHVKW3JdmvkVL9W1oqwX4GW57ki6eKLhCmH-OzeecB_BifZYekbrs9AH6qRdWEWwQTwLA,"Spectrum awareness, including identifying different types of signals, is very important in a cellular system environment. In this paper, a neural network is utilized to identify 5G signals among different cellular communications signals, including Long-Term Evolution (LTE) and Universal Mobile Telecommunication Service (UMTS). We explore the use of deep learning in wireless communications systems. We consider the effects of training dataset size, features extracted, and channel fading in our study. Experiment results demonstrate the effectiveness of deep learning neural networks in identifying cellular system signals, including UMTS, LTE, and 5G.",,10.1109/WOCC48579.2020.9114912,"Deep learning , Wireless communication , Fading channels , Training , 5G mobile communication , Neural networks , 3G mobile communication "
121,A tutorial on ultra-reliable and low-latency communications in 6G: Integrating domain knowledge into deep learning,"C She, C Sun, Z Gu, Y Li, C Yang, HV Poor, ...",arXiv preprint arXiv …,,,,,2020,arxiv.org,https://arxiv.org/abs/2009.06010,"… 2) In Section III, we review promising network architectures and deep learning frameworks for URLLC and summarize the design principles of deep learning frameworks in 6G networks. …",,,
122,RF fingerprinting and deep learning assisted UE positioning in 5G,"MM Butt, A Rao, D Yoon",2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring),1090-3038,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9128640/?casa_token=zoibivI94qUAAAAA:ETWyFNxz5vNNUeW532_pQSuxBFrmD_aonj2IJMUbZ75aAT9_0CA2EEqLP2lPfQYmx9nFtqS84jO3mw,"In this work, we investigate user equipment (UE) positioning assisted by deep learning (DL) in 5G and beyond networks. As compared to state of the art positioning algorithms used in today’s networks, radio signal fingerprinting and machine learning (ML) assisted positioning requires smaller additional feedback overhead; and the positioning estimates are made directly inside the radio access network (RAN), thereby assisting in radio resource management. The conventional positioning algorithms will be used as back-up for the environments with high variability in conditions; but ML-assisted positioning serves as more efficient and simpler technique to provide better or similar positioning accuracy. In this regard, we study ML-assisted positioning methods and evaluate their performance using system level simulations for an outdoor scenario in Lincoln park Chicago. The study is based on the use of raytracing tools, a 3GPP 5G NR compliant system level simulator and DL framework to estimate positioning accuracy of the UE. The use of raytracing tool and system level simulator helps avoid expensive drive test measurements in practical scenarios. Our proposed mechanism is a first step towards more proactive mobility management in future networks. We evaluate and compare performance of various DL models and show mean positioning error in the range of 1-1.5m for the best DL configuration with appropriate system feature-modeling.",,10.1109/VTC2020-Spring48590.2020.9128640,"Training , 3GPP , Position measurement , Artificial neural networks , Deep learning , 5G mobile communication , Data models "
123,Assessment of deep learning methodology for self-organizing 5g networks,"MZ Asghar, M Abbas, K Zeeshan, P Kotilainen, ...",Applied Sciences,,,,,2019,mdpi.com,https://www.mdpi.com/502838,"… of the deep learning method is assessed for application in 5G self … A few decades ago, machine learning (ML) was a less … Common machine learning techniques including supervised …",,,
124,Deep learning at the edge for channel estimation in beyond-5G massive MIMO,"M Belgiovine, K Sankhe, C Bocanegra, ...",IEEE Wireless Communications,1558-0687,28,2,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9430899/?casa_token=Irl83BTAXbIAAAAA:zp6AZ4fy3JOjFhfPtulQaqQrxcmH01Iuvt09mikAF4V5exuaswVMW9Fl6ZFpBLrCvykGi1mYUkOaYQ,"Massive multiple-input multiple-output (mMIMO) is a critical component in upcoming 5G wireless deployment as an enabler for high data rate communications. mMIMO is effective when each corresponding antenna pair of the respective transmitter-receiver arrays experiences an independent channel. While increasing the number of antenna elements increases the achievable data rate, at the same time computing the channel state information (CSI) becomes prohibitively expensive. In this article, we propose to use deep learning via a multi-layer perceptron architecture that exceeds the performance of traditional CSI processing methods like least square (LS) and linear minimum mean square error (LMMSE) estimation, thus leading to a beyond fifth generation (B5G) networking paradigm wherein machine learning fully drives networking optimization. By computing the CSI of all pairwise channels simultaneously via our deep learning approach, our method scales with large antenna arrays as opposed to traditional estimation methods. The key insight here is to design the learning architecture such that it is implementable on massively parallel architectures, such as GPU or FPGA. We validate our approach by simulating a 32-element array base station and a user equipment with a 4-element array operating on millimeter-wave frequency band. Results reveal an improvement up to five and two orders of magnitude in BER with respect to fastest LS estimation and optimal LMMSE, respectively, substantially improving the end-to-end system performance and providing higher spatial diversity for lower SNR regions, achieving up to 4 dB gain in received power signal compared to performance obtained through LMMSE estimation.",,10.1109/MWC.001.2000322,"MIMO , Edge computing , 5G mobile communication , Channel estimation , Deep learning "
125,Trustworthy deep learning in 6G-enabled mass autonomy: From concept to quality-of-trust key performance indicators,"C Li, W Guo, SC Sun, S Al-Rubaye, ...",IEEE Vehicular Technology Magazine,1556-6080,15,4,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9210233/?casa_token=P96T2vkvp-kAAAAA:6huCThSIApwOmM9wE6DYxaBWe63_0yHE8Uc5pzE-POxacXpb7kAWI8-jDCSjYlDm1ywcpNBc3MVUDQ,"Mass autonomy promises to revolutionize a wide range of engineering, service, and mobility industries. Coordinating complex communication among hyperdense autonomous agents requires new artificial intelligence (AI)-enabled orchestration of wireless communication services beyond 5G and 6G mobile networks. In particular, safety and mission-critical tasks will legally require both transparent AI decision processes and quantifiable quality-of-trust (QoT) metrics for a range of human end users (consumer, engineer, and legal). We outline the concept of trustworthy autonomy for 6G, including essential elements such as how explainable AI (XAI) can generate the qualitative and quantitative modalities of trust. We also provide XAI test protocols for integration with radio resource management and associated key performance indicators (KPIs) for trust. The research directions proposed will enable researchers to start testing existing AI optimization algorithms and develop new ones with the view that trust and transparency should be built in from the design through the testing phase.",,10.1109/MVT.2020.3017181,"Artificial intelligence , Cognition , Bit rate , Analytical models , Quality of service , Solid modeling , 5G mobile communication , 6G mobile communication , Decision making , Trust management "
126,Automation of 5G network slice control functions with machine learning,"VP Kafle, P Martinez-Julia, ...",IEEE Communications Standards Magazine,2471-2833,3,3,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8928169/?casa_token=9o45FQ3A5yQAAAAA:jzz2X3h9P9UV_GQsDQlMmx7oo8kBhtRmtraDenHsZiYoTAXZUuSN7B81QULhDNuT51UaeIy_DgKsuw,"5G communication networks will be complex due to the emergence of an unprecedented huge number of new types of connected devices and services. Moreover, the on-demand creation of virtual network slices, each suitable for a different application, is posing challenges to the efficient management of network resources, while optimally satisfying the quality of service requirements in time-varying workloads and network conditions. This article, which is tutorial in nature, introduces 5G network slices (from the point of view of the non-wireless part of the network) and elaborates the necessity of automation of network functions related to the design, construction, deployment, operation, control, and management of network slices. It revisits machine learning techniques applicable to the automation of network functions. It then presents a machine-learning-based framework for the operation and control of network slices by continuously monitoring workload, performance, and resource utilization, and dynamically adjusting the resources allocated to network slices. Preliminary results of workload prediction accuracy obtained from the analysis of real-life data collected from a web server are also reported.",,10.1109/MCOMSTD.001.1900010,"Machine learning , 5G mobile communication , Automation , Monitoring , Resource management , Quality of service , Network security , Communication networks "
127,Fast initial access with deep learning for beam prediction in 5G mmWave networks,"TS Cousik, VK Shah, JH Reed, T Erpek, ...",MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM),2155-7578,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9653011/?casa_token=eISwKGqLfrcAAAAA:SNp6ug2nOycH8BlU1HxPnC7cT7D9W3zl-CSWjjJJSq7-jC6neo_RcrCesRr1UBxRFGg5weUSysfJNA,"We present DeepIA, a deep learning solution for a fast, reliable and secure initial access (IA) in directional networks such as the mmWave networks in 5G systems. By utilizing only a subset of beams during the IA process, DeepIA removes the need for an exhaustive beam search thereby reducing the beam sweep time in IA. A deep neural network (DNN) is trained to learn the complex mapping from the received signal strengths (RSSs) collected with a reduced number of beams to the optimal spatial beam of the receiver (among a larger set of beams). In test time, DeepIA measures the RSSs only from a small number of beams and runs the DNN to predict the best beam for IA. We show that DeepIA reduces the IA time by sweeping fewer beams and significantly outperforms the conventional IA&#x0027;s beam prediction accuracy in both line of sight (LoS) and non-line of sight (NLoS) mmWave channel conditions.",,10.1109/MILCOM52596.2021.9653011,"Deep learning , Military communication , 5G mobile communication , Simulation , Conferences , Neural networks , Receivers "
128,Usage of network simulators in machine-learning-assisted 5G/6G networks,"F Wilhelmi, M Carrascosa, C Cano, ...",IEEE Wireless Communications,1558-0687,28,1,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9363049/?casa_token=fRT8wF6vVsUAAAAA:WVwo4N6rOJIonpp9nm9XMfRGD1-yw6xVRy4RFxsfYkFTkTJlu8Y60NLR-I9xpA-bIpnRBOBbDpqBBw,"Without any doubt, Machine Learning (ML) will be an important driver of future communications due to its foreseen performance when applied to complex problems. However, the application of ML to networking systems raises concerns among network operators and other stakeholders, especially regarding trustworthiness and reliability. In this article, we devise the role of network simulators for bridging the gap between ML and communications systems. In particular, we present an architectural integration of simulators in ML-aware networks for training, testing, and validating ML models before being applied to the operative network. Moreover, we provide insights into the main challenges resulting from this integration, and then give hints discussing how they can be overcome. Finally, we illustrate the integration of network simulators into ML-assisted communications through a proof-of-concept testbed implementation of a residential WiFi network.",,10.1109/MWC.001.2000206,"Training data , Communication systems , Machine learning , Stakeholders , Reliability , Wireless fidelity , 5G mobile communication , 6G mobile communication "
129,Towards 6G IoT: tracing mobile sensor nodes with deep learning clustering in UAV networks,"Y Spyridis, T Lagkas, P Sarigiannidis, V Argyriou, ...",Sensors,,,,,2021,mdpi.com,https://www.mdpi.com/1141088,"… services in the context of the upcoming 6G networks. This paper considered the objective of … A deep learning model performed clustering in the UAV network at regular intervals, based …",,,
130,Deep learning modalities for biometric alteration detection in 5G networks-based secure smart cities,"A Sedik, M Hammad, AA Abd El-Latif, ...",IEEE Access,2169-3536,9,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9450808/,"Smart cities and their applications have become attractive research fields birthing numerous technologies. Fifth generation (5G) networks are important components of smart cities, where intelligent access control is deployed for identity authentication, online banking, and cyber security. To assure secure transactions and to protect user's identities against cybersecurity threats, strong authentication techniques should be used. The prevalence of biometrics, such as fingerprints, in authentication and identification makes the need to safeguard them important across different areas of smart applications. Our study presents a system to detect alterations to biometric modalities to discriminate pristine, adulterated, and fake biometrics in 5G-based smart cities. Specifically, we use deep learning models based on convolutional neural networks (CNN) and a hybrid model that combines CNN with convolutional long-short term memory (ConvLSTM) to compute a three-tier probability that a biometric has been tempered. Simulation-based experiments indicate that the alteration detection accuracy matches those recorded in advanced methods with superior performance in terms of detecting central rotation alteration to fingerprints. This makes the proposed system a veritable solution for different biometric authentication applications in secure smart cities.",,10.1109/ACCESS.2021.3088341,"Biometrics (access control) , Feature extraction , Authentication , Iris recognition , Fingerprint recognition , Smart cities , Veins "
131,Testbed for 5G connected artificial intelligence on virtualized networks,"CV Nahum, LDNM Pinto, VB Tavares, P Batista, ...",IEEE Access,2169-3536,8,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9290141/,"The fifth-generation (5G) cellular networks incorporate a large variety of technologies in order to address very distinct use cases. Assessing these technologies and investigating future alternatives is complicated when one relies only on simulators. 5G testbeds are an important alternative to simulators and many have been recently described, emphasizing aspects such as cloud functionalities, management and orchestration. This work presents a 5G mobile network testbed with a virtualized and orchestrated structure using containers, which focuses on integration to artificial intelligence (AI) applications. The presented testbed uses open-source technologies to deploy and orchestrate the virtual network functions (VNFs) to flexibly create various mobile network scenarios, with distinct fronthaul and backhaul topologies. Distinctive features of the testbed are its relatively low cost and the support to using AI for optimizing the network performance. The paper explains how to deploy the testbed structure and reproduce the presented results with the provided code. AI-based radio access network (RAN) slicing and VNF placement are used as examples of the testbed capabilities.",,10.1109/ACCESS.2020.3043876,"5G mobile communication , Network topology , Containers , Topology , Artificial intelligence , Open source software , Radio access networks "
132,5G positioning-a machine learning approach,"M Malmström, I Skog, SM Razavi, ...","2019 16th Workshop on Positioning, Navigation and Communications (WPNC)",2164-9758,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8970186/,"In urban environments, cellular network-based positioning of user equipment (ue) is a challenging task, especially in frequently occurring non-line-of-sight (nlos) conditions. This paper investigates the use of two machine learning methods - neural networks and random forests - to estimate the position of ue in nlos using best received reference signal beam power measurements. We evaluated the suggested positioning methods using data collected from a fifth-generation cellular network (5g) testbed provided by Ericsson. A statistical test to detect nlos conditions with a probability of detection that is close to 90% is suggested. We show that knowledge of the antenna are crucial for accurate position estimation. In addition, our results show that even with a limited set of training data and one 5g transmission point, it is possible to position ue within 10 meters with 80% accuracy.",,10.1109/WPNC47567.2019.8970186,
133,Integrating artificial intelligence Internet of Things and 5G for next-generation smartgrid: A survey of trends challenges and prospect,"E Esenogho, K Djouani, AM Kurien",IEEE Access,2169-3536,10,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9672084/,"Smartgrid is a paradigm that was introduced into the conventional electricity network to enhance the way generation, transmission, and distribution networks interrelate. It involves the use of Information and Communication Technology (ICT) and other solution in fault and intrusion detection, mere monitoring of energy generation, transmission, and distribution. However, on one hand, the actual and earlier smartgrid, do not integrate more advanced features such as automatic decision making, security, scalability, self-healing and awareness, real-time monitoring, cross-layer compatibility, etc. On the other hand, the emergence of the digitalization of the communication infrastructure to support the economic sector which among them are energy generation and distribution grid with Artificial Intelligence (AI) and large-scale Machine to Machine (M2M) communication. With the future Massive Internet of Things (MIoT) as one of the pillars of 5G/6G network factory, it is the enabler to support the next generation smart grid by providing the needed platform that integrates, in addition to the communication infrastructure, the AI and IoT support, providing a multitenant system. This paper aim at presenting a comprehensive review of next smart grid research trends and technological background, discuss a futuristic next-generation smart grid driven by artificial intelligence (AI) and leverage by IoT and 5G. In addition, it discusses the challenges of next-generation smart-grids as it relate to the integration of AI, IoT and 5G for better smart grid architecture. Also, proffers possible solutions to some of the challenges and standards to support this novel trend. A corresponding future work will dwell on the implementation of the discussed integration of AI, IoT and 5G for next-generation smart grid, using Matlab, NS2/NS3, Open-daylight and Mininet as soft tools and compare with related literature.",,10.1109/ACCESS.2022.3140595,"Smart grids , Artificial intelligence , Next generation networking , 5G mobile communication , Internet of Things , Distribution networks , Market research "
134,Machine learning approach for automatic configuration and management of 5g platforms,"TA Khan, A Mehmood, JJD Rivera, ...",2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS),2576-8565,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8893119/?casa_token=bCApCmzQcUAAAAAA:qvmbW2o17Ag61PCrv-s0dl9HTjtoshPBwUJc-lGvqrlt0ymsGDoDo6vkAOAwJVvd4h5Vy1B9ve7KFA,"The automatic control over the network platforms is an esteem requirement of the operators. Recently, 5G with its aim to attain the internet of everything, it challenged researchers for the achievement of automatic control over the network platform. Furthermore, the 5G system consists of multi-domain network applications and platforms which make it complex to control and configure the network. Hence, the focus of this research is to enable easy configuration through high-level instructions and the use of machine learning for automatic control over the network infrastructure. The overall system consists of Intent-Base application that includes a machine learning model and it configures and controls the M-CORD-based network slicing test-bed.",,10.23919/APNOMS.2019.8893119,
135,Examining machine learning for 5G and beyond through an adversarial lens,"M Usama, I Ilahi, J Qadir, RN Mitra, ...",IEEE Internet Computing,1941-0131,25,2,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9314254/?casa_token=S6XUmOrhNt0AAAAA:_hcyRmPFRBR3FDZMja3jc8DYZnrgA8SmDleOxg8UalP05s-tck7XI9kmipYTqeIuE6cTEF8f9UsyMw,"Spurred by the recent advances in deep learning to harness rich information hidden in large volumes of data and to tackle problems that are hard to model/solve (e.g., resource allocation problems), there is currently tremendous excitement in the mobile networks domain around the transformative potential of data-driven artificial intelligence/machine learning (AI/ML) based network automation, control and analytics for 5G and beyond. In this article, we present a cautionary perspective on the use of AI/ML in the 5G context by highlighting the adversarial dimension spanning multiple types of ML (supervised/unsupervised/reinforcement learning) and support this through three case studies. We also discuss approaches to mitigate this adversarial ML risk, offer guidelines for evaluating the robustness of ML models, and call attention to issues surrounding ML oriented research in 5G more generally.",,10.1109/MIC.2021.3049190,"5G mobile communication , Modulation , Computational modeling , Security , Context modeling , Cloud computing , Signal to noise ratio , Resource management , Learning (artificial intelligence) , Analytical models , Deep learning "
136,A deep learning method for predictive channel assignment in beyond 5G networks,"S Sakib, T Tazrin, MM Fouda, ZM Fadlullah, ...",IEEE Network,1558-156X,35,1,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9237456/?casa_token=_oAeT37k1PEAAAAA:AsoQad2da6IkONgyD-juG9si8w1cYz1uyIOkfTVGI3wkFnnKpTJNSLYY0O3R0Ry42jQPZTmBfWeblQ,"In Beyond Fifth Generation (B5G) networks, Internet of Things (IoT) and massive Machine Type Communication (mMTC) traffic are anticipated to be offloaded by multi-hop, Device-to-Device (D2D)-enabled relay networks. The relays offer an energy and spectral-efficient solution to the rising problem of spectrum scarcity and overloading of cellular base stations. Moving beyond the conventional paradigm of the relay nodes employing channels on a specific band at a time, in this article, we aim to investigate how to simultaneously leverage multiple bands at a relay node to improve spectral efficiency. We address the challenge associated with dynamic channel conditions in the multi-band relay networks, and envision a deep learning-based predictive channel selection method to solve the problem. A 1-D (one-dimensional) Convolutional Neural Network (CNN) model is employed to predict the suitable channels across multiple bands with the best Signal-to-Interference-plus-Noise Ratio (SINR). The packets received from the source or previous relay node are scheduled to be transmitted to subsequent relay node/destination based on the best modulation and coding rates to transmit over the predicted band. Our envisioned approach, based on shallow and deep-CNN models, proposes two proactive channel assignment strategies, namely controlled and smart prediction. Our proposal is evaluated with several, comparable machine/deep learning methods. Experimental results, based on datasets, demonstrate encouraging performance of our proposed lightweight deep learning-based proactive channel selection in multi-band relay systems.",,10.1109/MNET.011.2000301,"Predictive models , Deep learning , Channel allocation , Data models , Signal to noise ratio , Relay networks (telecommunication) "
137,On recommendation-aware content caching for 6G: An artificial intelligence and optimization empowered paradigm,"Y Fu, KN Doan, TQS Quek",Digital Communications and Networks,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S2352864820301802,… of Artificial Intelligence (AI) and optimization techniques can be harnessed to address those core issues and facilitate the full implementation of RCC for the upcoming intelligent 6G era. …,,,
138,Deep learning based modulation classification for 5G and beyond wireless systems,"JC Clement, N Indira, P Vijayakumar, ...",Peer-to-peer networking …,,,,,2021,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s12083-020-01003-3&casa_token=nxQkgIPnBQ4AAAAA:8_d8jRmBjh0zArOCQMBEIXf97ra7AlD0XkPn3gCNxcfSKGeSBrfU_b1klXp4WFU8jf8JODjHzDDI58gC1A,"The 5G and beyond wireless networks will be more dynamic and heterogeneous, which needs to work on multistrand waveforms. One of the most significant challenges in such a …",,,
139,"GPU-based, LDPC decoding for 5G and beyond","C Tarver, M Tonnemacher, H Chen, ...",2020 IEEE International Symposium on Circuits and Systems (ISCAS),2158-1525,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9336349/,"Next-generation virtual radio access networks (vRAN) will benefit from the flexibility provided by virtualization in proposed Cloud-RAN configurations. These systems for 5G and beyond may consist of commodity hardware such as GPUs in data centers with multiple connected base stations (gNBs) flexibly receiving allocated resources depending on time-varying, real-time demands. In this paper, parallel reconfigurable algorithms and architectures for channel decoding are proposed. In particular, flexible rate and block length LDPC decoders for the new radio (NR) physical layer on GPU are characterized. We implement these GPU decoders using reduced word lengths of 8-bits to represent the log-likelihood ratios during decoding, and we utilize multiple GPU streams to process multiple blocks of codewords in parallel. These techniques allow our implementation to reduce the device transfer overhead and achieve the low-latency or high-throughput targets for 5G and beyond. Moreover, we integrate our decoder into the Open Air Interface (OAI) NR software stack to investigate virtualization capabilities when containerizing vRAN functionality such as the LDPC decoder.",,10.1109/ISCAS45731.2020.9181064,"Parity check codes , 5G mobile communication , Graphics processing units , Decoding , Throughput , Kernel "
140,Radar aided 6G beam prediction: Deep learning algorithms and real-world demonstration,"U Demirhan, A Alkhateeb",2022 IEEE Wireless Communications and Networking Conference (WCNC),1525-3511,,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9771564/?casa_token=10cPkPNYrygAAAAA:mwldwtWZupbJAah1ag0zN0uLO89vMluJnE6C_GzlW4OKAfplWP1tmAdNoIkc-yohUkTuHF9f8sHZGw,"Adjusting the narrow beams at millimeter wave (mmWave) and terahertz (THz) MIMO communication systems is associated with high beam training overhead, which makes it hard for these systems to support highly-mobile applications. This overhead can potentially be reduced or eliminated if sufficient awareness about the transmitter/receiver locations and the surrounding environment is available. In this paper, efficient deep learning solutions that leverage radar sensory data are developed to guide the mmWave beam prediction and significantly reduce the beam training overhead. Our solutions integrate radar signal processing approaches to extract the relevant features for the learning models, and hence optimize their complexity and inference time. The proposed machine learning based radar-aided beam prediction solutions are evaluated using a large-scale real-world mmWave radar/communication dataset and their capabilities were demonstrated in a realistic vehicular communication scenario. In addition to completely eliminating the radar/communication calibration overhead, the proposed algorithms are able to achieve around 90% top-5 beam prediction accuracy while saving 93% of the beam training overhead. This highlights a promising direction for addressing the training overhead challenge in mmWave/THz communication systems.",,10.1109/WCNC51071.2022.9771564,"Training , Deep learning , Machine learning algorithms , Signal processing algorithms , Millimeter wave radar , Prediction algorithms , Feature extraction "
141,Survey on artificial intelligence techniques in 5G networks,"A Abdellah, A Koucheryavy",J. Inf. Technol. Telecommun …,,,,,2020,researchgate.net,https://www.researchgate.net/profile/Ali-Refaee/publication/342174425_SURVEY_ON_ARTIFICIAL_INTELLIGENCE_TECHNIQUES_IN_5G_NETWORKS/links/5f1723eca6fdcc9626a45c3f/SURVEY-ON-ARTIFICIAL-INTELLIGENCE-TECHNIQUES-IN-5G-NETWORKS.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail,… Machine learning is ideally suitable for working in 5G networks … This is an ideal situation for 5G since it can transmit higher … Deep learning is a class of machine learning (ML) that uses …,,,
142,A machine learning approach for 5G SINR prediction,"R Ullah, SNK Marwat, AM Ahmad, S Ahmed, A Hafeez, ...",Electronics,,,,,2020,mdpi.com,https://www.mdpi.com/853142,Artificial Intelligence (AI) and Machine Learning (ML) are envisaged to play key roles in 5G networks. Efficient radio resource management is of paramount importance for network …,,,
143,Micro-safe: Microservices-and deep learning-based safety-as-a-service architecture for 6G-enabled intelligent transportation system,"C Roy, R Saha, S Misra, K Dev",IEEE Transactions on Intelligent Transportation Systems,1558-0016,23,7,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9548710/?casa_token=ssCPtoEMI7YAAAAA:kWtkP9MvaM9Pu7ld5ew30PvE7EUBbXqAUYOQeb6KdXtjgB_4jVvZ6je9s6IteD7_WbvNQROlQt5ePw,"In this paper, we propose a microservices and deep learning-based scheme, termed as Micro-Safe, for provisioning Safety-as-a-Service (Safe-aaS) in a 6G environment. A Safe-aaS infrastructure provides customized safety-related decisions dynamically to the registered end-users. As the decisions are time-sensitive in nature, the generation of these decisions should incur minimum latency and high accuracy. Further, scalability and extension of the coverage of the entire Safe-aaS platform are also necessary. Considering road transportation as the application scenario, we propose Safe-aaS, which is a microservices- and deep learning-based platform for provisioning ultra-low latency safety services to the end-users in a 6G scenario. We design the proposed solution in two stages. In the first stage, we develop the microservices-enabled application layer to improve the scalability and adaptability of the traditional Safe-aaS platform. Moreover, we apply the state space model to represent the decision parameters requested and the decision delivered to the end-users. During the second stage, we use deep learning models to improve the accuracy in the decisions delivered to the end-users. Additionally, we apply an assortment of activation functions to analyze and compare the accuracy of the decisions generated in the proposed scheme. Extensive simulation of our proposed scheme, Micro-Safe, demonstrates that latency is improved by 26.1 – 31.2%, energy consumption is reduced by 22.1 – 29.9%, throughput is increased by 26.1 – 31.7%, compared to the existing schemes.",,10.1109/TITS.2021.3110725,"6G mobile communication , Safety , Deep learning , Scalability , Delays , Cloud computing , Throughput "
144,Reconfigurable architecture of UFMC transmitter for 5G and its FPGA prototype,"V Kumar, M Mukherjee, J Lloret",IEEE Systems Journal,2373-7816,14,1,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8771165/?casa_token=eNifPzXl2d4AAAAA:EXz2wrV6DqAEGjMkQPsvrDnFZfqio4-GWTsjqtcqHV7-GB3424lVfZ1TlAhJGLWuYfZbeKZnmo53Iw,"A universal-filtered multicarrier (UFMC) system that is a generalization of filtered orthogonal frequency-division multiplexing (OFDM) and filter-bank-based multicarrier is being considered as a potential candidate for fifth-generation due to its robustness against intercarrier interference as in cyclic-prefix-based OFDM systems. However, real-time hardware realization of multicarrier systems is limited by a large number of arithmetic units for inverse fast Fourier transform and pulse-shaping filters. In this paper, we aim to propose a low-complexity and reconfigurable architecture for a baseband UFMC transmitter. To the best of our knowledge, the proposed architecture is the first reconfigurable architecture that has the flexibility to choose the number of subcarriers in a subband without any change in hardware resources. In addition, the proposed architecture selects the filter from a group of filters with a single selection line. Moreover, we use a commercially available field-programmable gate array device for real-time testing and analyzing the baseband UFMC signal. From the extensive experiments, we study the occupied bandwidth, main-lobe power, and sidelobe power of the baseband signal with different filters in real-time scenarios. Finally, we measure the quantization error in baseband signal generation for the proposed UFMC transmitter architecture and find comparable with the error bound.",,10.1109/JSYST.2019.2923549,"Transmitters , Baseband , Computer architecture , Real-time systems , 5G mobile communication , Field programmable gate arrays , Hardware "
145,vFPGAmanager: A virtualization framework for orchestrated FPGA accelerator sharing in 5G cloud environments,"S Pinneterre, S Chiotakis, M Paolino, ...",2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),2155-5052,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8436930/?casa_token=MjBEWpEBxNIAAAAA:HM342bU7WNVXGmiqBT33ThVrItkOqUYyOxFrJqLXbO48hBeixM7MXFjlINLv33cA9OyI2DS-7_Xukg,"Network operators are actively pushing towards the new 5G era and a crucial part to accomplish this is the Network Functions Virtualization (NFV). FPGAs and their hardware accelerators are a promising solution for NFV and 5G cloud environments because of their fast turnaround time and great speedup potential through application parallelism mapping on the reconfigurable fabric. Recently, consolidation reached a plateau in this field with lightweight virtualization techniques, that require a high overcommitment of FPGA accelerator resources to cope with numerous demands of guests. Although FPGAs can play an important role for the future 5G networks their capability to manage and control them from the upper layers of the software stack is inadequate. The lack of such support coupled with cloud integration and programmability issues can repel potential providers from utilizing FPGAs at their data centers. This paper presents the communication mechanism of the vFPGAmanager, an FPGA virtualization framework which can be orchestrated, monitored and enables accelerators overcommitment with direct guest access. These are key features to allow potential adopters of FPGA technology to include them in the next generation of NFV systems. The communication mechanism architecture is detailed and then benchmarked to show that even under heavy load on the system it demonstrates a minimal overhead to orchestrate and monitor the FPGA as a resource.",,10.1109/BMSB.2018.8436930,"Field programmable gate arrays , Virtualization , Context , Cloud computing , Registers , Acceleration "
146,A waveform parameter assignment framework for 6G with the role of machine learning,"A Yazar, H Arslan",IEEE Open Journal of Vehicular Technology,2644-1330,1,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9086145/,"5G enables a wide variety of wireless communications applications and use cases. There are different requirements associated with the applications, use cases, channel structure, network and user. To meet all of the requirements, several new configurable parameters are defined in 5G New Radio (NR). It is possible that 6G will have even higher number of configurable parameters based on new potential conditions. In line with this trend, configurable waveform parameters are also varied and this variation will increase in 6G considering the potential future necessities. In this paper, association of users and possible configurable waveform parameters in a cell is discussed for 6G communication systems. An assignment framework of configurable waveform parameters with different types of resource allocation optimization mechanisms is proposed. Most of all, the role and usage of machine learning (ML) in this framework is described. A case study with a simulation based dataset generation methodology is also presented.",,10.1109/OJVT.2020.2992502,"6G mobile communication , 5G mobile communication , Optimization , Resource management , Machine learning , Lattices "
147,5G joint artificial intelligence technology in the innovation and reform of university English education,X Sun,Wireless Communications and Mobile Computing,,,,,2021,hindawi.com,https://www.hindawi.com/journals/wcmc/2021/4892064/,"… This paper considers the issue of human subjectivity in the system of “5G + AI+Education” … of artificial intelligence, and a philosophical reflection on the application of artificial intelligence …",,,
148,An FPGA-oriented baseband modulator architecture for 4G/5G communication scenarios,"M Lopes Ferreira, J Canas Ferreira",Electronics,,,,,2018,mdpi.com,https://www.mdpi.com/384148,"… This work proposes an FPGA-oriented architecture for baseband downlink transmission suitable for DSA, C-RAN, and 4G/5G coexistence scenarios. The architecture supports three 5G …",,,
149,Architecture of FPGA based channel coding for 5G wireless network using high-level synthesis,MV Khoroshaylova,Bulletin of Voronezh State Technical University …,,,,,2018,,,,,,
150,"Machine learning for 5G security: Architecture, recent advances, and challenges","A Afaq, N Haider, MZ Baig, KS Khan, M Imran, I Razzak",Ad Hoc Networks,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S1570870521001785?casa_token=-RcgQLJ2XCsAAAAA:Vw5vCCQpPyYaHjkLeMgGSWCDPHGoaHgKajbqa_iIrzY781574ARJgtdVfKttcddu9-778sLVDxI,"… We also present discussions on the feasibility of different ML techniques to tackle threats at different points of 5G networks. In the context of 5G network security and machine learning, …",,,
151,Machine Learning for Intelligent-Reflecting-Surface-Based Wireless Communication towards 6G: A Review,"MAS Sejan, MH Rahman, BS Shin, JH Oh, YH You, ...",Sensors,,,,,2022,mdpi.com,https://www.mdpi.com/1424-8220/22/14/5405,"… for the sixth generation (6G) of communication networks. In addition, machine learning (ML) … -of-the-art on ML, especially on deep learning (DL)-based IRS-enhanced communication. …",,,
152,Security concerns on machine learning solutions for 6G networks in mmWave beam prediction,"FO Catak, M Kuzlu, E Catak, U Cali, D Unal",Physical Communication,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S1874490722000155,"… with lower latency than the 5G network, ie, approximately 1 … cases of 6G are still under definition, it is clear that 6G will be … difference of 6G technology is the use of artificial intelligence (…",,,
153,A programmable and fpga-accelerated gtp offloading engine for mobile edge computing in 5g networks,"CA Shen, DY Lee, CA Ku, MW Lin, ...",IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8845143/?casa_token=Ow4M_LW5bNYAAAAA:NnbyuojYj_ZvqdzzfdwTkqqey_tfLVtBsC3rrVIYfWItVVoaZb8AdMnHH8tBnYtHIp6UruRglgXChw,"This poster presents a programmable and FPGA-accelerated packet processing engine, performing the encapsulation and decapsulation of GPRS Tunneling Protocol (GTP) packets, for Mobile Edge Computing in 5G Networks. The proposed engine is designed by using the P4 language and is implemented to the FPGA platform by using the Xilinx SDNet tool. The implemented system is practically realized on the Xilinx FPGA platform and the detailed architecture is presented. The experimental results show that targeted throughput of 10Gbps per port is achieved given the packet-processing latency of 5μs.",,10.1109/INFCOMW.2019.8845143,
154,Efficient and reliable hybrid deep learning-enabled model for congestion control in 5G/6G networks,"S Khan, A Hussain, S Nazir, F Khan, A Oad, ...",Computer …,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S0140366421004217?casa_token=tC09Bq6L1yUAAAAA:KohntWt_eav2xNdt0CxHgagVlI8hJdd8md7Bih-wpCO8zV6bM8UGgqtJTe1gCnVvo6ZLHFwQxkg,"… Artificial intelligence (AI) and machine learning (ML) are playing an essential role in reconfiguring and optimizing the performance of a 5G/6G … , a hybrid deep learning-enabled efficient …",,,
155,Deep learning-driven opportunistic spectrum access (OSA) framework for cognitive 5G and beyond 5G (B5G) networks,"R Ahmed, Y Chen, B Hassan",Ad Hoc Networks,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S1570870521001529?casa_token=_omRI6p0mrcAAAAA:TwSTOwz-FkuEkNfDJLh5nB99JtVV_RLPwMkx7MBqHq-aMyay-vzpbnqlPCN9S9AMhgGr9XYzS6I,… to dynamically optimize the use of spectrum in 5G/B5G networks to solve the imminent … in the 5G/B5G cognitive radio (CR) network of IoTs and UAVs through the novel deep learning-…,,,
156,Intrusion detection system on IoT with 5G network using deep learning,"N Yadav, S Pande, A Khamparia, ...",… and Mobile Computing,,,,,2022,hindawi.com,https://www.hindawi.com/journals/wcmc/2022/9304689/,"… 5G network [6]. To satisfy customer demands and resolve conflicts in the 5G environment, a fundamental change in the 5G … are efficiently detected by 5G networks and cyberattacks that …",,,
157,Authentication and resource allocation strategies during handoff for 5G IoVs using deep learning,"SR Akhila, Y Alotaibi, OI Khalaf, S Alghamdi",Energies,,,,,2022,mdpi.com,https://www.mdpi.com/1996-1073/15/6/2006,"… Though 5G is yet to be deployed widely, it appears that 5G stands to … 5G is expected to power the future of people’s mobility … collection makes IoV one of the attractive applications of 5G. …",,,
158,Machine learning assisted security analysis of 5g-network-connected systems,"T Saha, N Aaraj, NK Jha",IEEE Transactions on Emerging Topics in Computing,2376-4562,10,4,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/document/9701880/?casa_token=bnTiWPPuheYAAAAA:MDQoQbNuBjBcIGfVm19Z--5CIV6iCjneZDYdBS3PXiKQtJZ9haORQQDxuAFQRIuJSkEv9CBZyMIx0w,"The core network architecture of telecommunication systems has undergone a paradigm shift in the fifth-generation (5G) networks. 5G networks have transitioned to software-defined infrastructures, thereby reducing their dependence on hardware-based network functions. New technologies, like network function virtualization and software-defined networking, have been incorporated in the 5G core network (5GCN) architecture to enable this transition. This transition has significantly improved network efficiency, performance, and robustness. However, this has also made the core network more vulnerable, as software systems are generally easier to compromise than hardware systems. This article presents a comprehensive security analysis framework for the 5GCN. The novelty of this approach lies in the creation and analysis of attack graphs of the software-defined and virtualized 5GCN through machine learning. This analysis points to 119 novel possible exploits in the 5GCN. We demonstrate that these potential exploits of 5GCN vulnerabilities generate five novel attacks on the 5G Authentication and Key Agreement protocol. We combine the attacks at the network, protocol, and application layers to generate complex attack vectors. In a case study, we use these attack vectors to find four novel security loopholes in WhatsApp running on a 5G network.",,10.1109/TETC.2022.3147192,"5G mobile communication , Security , Protocols , Computer architecture , Internet telephony , Freeware , Control systems "
159,Deep Learning for Fast and Reliable Initial Access in AI-Driven 6G mm Wave Networks,"TS Cousik, VK Shah, T Erpek, ...",IEEE Transactions on Network Science and Engineering,2334-329X,PP,99,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9869317/?casa_token=VAKnCKcisuEAAAAA:5El-LcGjgxAdctrzZimXZ492aDPxpmOT8X4VscYZpQR_skqK7E5X_v0zBDCid7aEzlKMtrjrl-6S7A,"We present DeepIA, a deep neural network (DNN) framework for fast and reliable initial access (IA) for artificial intelligence (AI)-driven 6G millimeter wave (mmWave) networks. DeepIA reduces the beam sweep time compared to a conventional exhaustive search-based IA process by utilizing only a subset of the available beams. DeepIA maps received signal strengths (RSSs) obtained from a subset of beams to the beam that is best oriented to the receiver. In both line of sight (LoS) and non-line of sight (NLoS) conditions, DeepIA reduces the IA time and outperforms the conventional IA&#x0027;s beam prediction accuracy. We show that DeepIA&#x0027;s accuracy saturates with the number of beams used, and also depends on the particular choice of the beams used. The choice of beams that are selected is consequential and improves accuracy by upto upto <inline-formula><tex-math notation=""LaTeX"">$35\%$</tex-math></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">$70\%$</tex-math></inline-formula> in NLoS and LoS channels. We find that, averaging multiple RSS snapshots further reduces the number of beams needed and achieves more than <inline-formula><tex-math notation=""LaTeX"">$95\%$</tex-math></inline-formula> accuracy in both LoS and NLoS conditions. We introduce interference into our models to understand impact on performance. Finally, we evaluate the beam prediction time of DeepIA through embedded hardware implementation and show the improvement over the baseline approach.",,10.1109/TNSE.2022.3201748,"Receivers , 6G mobile communication , Millimeter wave communication , 5G mobile communication , Transmitters , Antenna measurements , Time measurement "
160,Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review,"A Miglani, N Kumar",Computer Communications,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S0140366421002632?casa_token=OEqUR9hPNzwAAAAA:wG5N1NeLVsL_3nZTY_Ltkev5ajVbFfXKObQ6SLXsSg__rawSp6KfbvOw-MG2_tycsJIOJFGlTo4,"… basic concepts of blockchain and machine learning in this article. … machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning …",,,
161,Enabling machine learning with service function chaining for security enhancement at 5G edges,"B Feng, H Zhou, G Li, Y Zhang, K Sood, S Yu",IEEE Network,1558-156X,35,5,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9537924/?casa_token=4NjcpDaL71oAAAAA:PvuwXkmIlfaxGB4ht0U1c5_l6UXxIzajAh8A-Wuc4PThUsHN6TidgFz4LiRH_GdKh_DuSLi4WHN1Jg,"With massive sorts of terminals, devices, and machines connecting to 5G, a tremendous surge of data makes cyber-security a pressing issue, and conventional countermeasures are facing unprecedented challenges. Recently, with the rise of ML (Machine Learning) and SDN/NFV-based (Software-Defined Networks/Network Functions Virtualization) SFC (Service Function Chaining) techniques, how to leverage them for security enhancement in MEC (Multi-access/Mobile Edge Computing) has received much attention. Hence, in this article, we first propose an elastic framework to integrate ML with virtualized SFC, aiming at smart and efficient provision of different services at MEC. Then, we propose an ML-based anomaly detection algorithm used as a kind of service policy for SFC classifiers, which guides the latter for quick traffic classification and subsequent redirections of attack flows. Finally, we build a corresponding prototype system and evaluate the performance of the proposed algorithm through extensive experiments. Related results have confirmed the feasibility and advantages of the proposed framework and algorithm.",,10.1109/MNET.100.2000338,"Feature extraction , Security , Machine learning , Classification algorithms , Quality of service , Anomaly detection , Training data , 5G mobile communication "
162,Machine learning protocol for secure 5G handovers,"VO Nyangaresi, AJ Rodrigues, SO Abeka",International Journal of …,,,,,2022,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10776-021-00547-2&casa_token=bRsrO16F0QIAAAAA:MxAx0joi4Z8hPfxNyrIWvisEgMGNqR1bi3thbLB-td1OJ5NG4acdJJSJqwfvcMbWGKzkKMZLXxCjTelV8A,"… This paper presents a machine learning protocol that not only facilitates optimal selection of … improved 5G authentication and key agreement (5G AKA’) protocol. Specifically, using 5G …",,,
163,A survey of collaborative machine learning using 5G vehicular communications,"SV Balkus, H Wang, BD Cornet, ...",IEEE Communications Surveys & Tutorials,2373-745X,24,2,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9706268/?casa_token=BnIRBo4Ci-UAAAAA:VSGSP1VSnAaqJJnYGGFUtXp8CrPOBJdDcFrZJGE2LOjN2zbEP8LOS-UyV570S9vud41KVEZ64-haXg,"By enabling autonomous vehicles (AVs) to share data while driving, 5G vehicular communications allow AVs to collaborate on solving common autonomous driving tasks. AVs often rely on machine learning models to perform such tasks; as such, collaboration requires leveraging vehicular communications to improve the performance of machine learning algorithms. This paper provides a comprehensive literature survey of the intersection between machine learning for autonomous driving and vehicular communications. Throughout the paper, we explain how vehicle-to-vehicle (V2V) and vehicle-to-everything (V2X) communications are used to improve machine learning in AVs, answering five major questions regarding such systems. These questions include: 1) How can AVs effectively transmit data wirelessly on the road? 2) How do AVs manage the shared data? 3) How do AVs use shared data to improve their perception of the environment? 4) How do AVs use shared data to drive more safely and efficiently? and 5) How can AVs protect the privacy of shared data and prevent cyberattacks? We also summarize data sources that may support research in this area and discuss the future research potential surrounding these five questions.",,10.1109/COMST.2022.3149714,"Autonomous vehicles , Deep learning , Task analysis , Machine learning , Roads , 5G mobile communication , Neural networks "
164,Cooperative attacks detection based on artificial intelligence system for 5G networks,H Sedjelmaci,Computers &Electrical Engineering,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S004579062100063X?casa_token=4kRteHFVuFcAAAAA:XjtpcaHl5mBVc2s92n5vQuv7a6W0cUDj-Me8Gib-InBmF5mn7FN3zVtk0aaVQ4NvjqXX3miWDOU,"… Recently in [5,6] the authors develop intrusion detection systems (IDSs) based on a machine learning algorithms to detect and predict a couple of attacks targeting the 5G network. …",,,
165,A machine learning approach for SNR prediction in 5G systems,"K Saija, S Nethi, S Chaudhuri, ...",2019 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS),2153-1676,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9118097/?casa_token=gDCqf34QDa4AAAAA:a4vxjk7F9R8INqLyI6jNUmBpFIHEyn283c6G2qG1x5cRebCVIGTBytHvCXTAP_gJ-Vn-Ppvt7PI22g,"Channel State Information (CSI) feedback from the User Equipment (UE) on the uplink (UL) channel is an integral part of the 5G-NR standard. It allows next-generation Node-B (gNB) to obtain the information about the channel impairments, and schedule appropriate radio resources to the UE in order to maintain the required Quality of Service (QoS). Errors in the channel estimation by the UE increases erroneous downlink transmission and subsequently inefficient spectral use. There is an inherent tradeoff between CSI feedback periodicity from the UE and the accuracy and relevance of channel estimation within the periodicity by gNB. Since the CSI depends on the received Signal-to-Noise Ratio (SNR) at the UE. Hence, the goal of this work is to design and develop suitable methodologies to estimate a CSI parameter namely, Channel Quality Indicator (CQI) by predicting the SNR using the state-of-the-art Machine Learning (ML) techniques. In this paper, we present an experimental framework to predict CQI using SNR for different environmental scenarios and UE characteristics like delay profile and speed respectively. The experimental results show better performance in terms of CQI to SNR mapping, and error in prediction over current techniques.",,10.1109/ANTS47819.2019.9118097,
166,Leveraging machine learning for millimeter wave beamforming in beyond 5G networks,"BM ElHalawany, S Hashima, K Hatano, ...",IEEE Systems Journal,2373-7816,16,2,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9471834/?casa_token=adqG8Cz3IyQAAAAA:2Z62uYbmkxNDzFtXHhEbujypzG2TAjq6g9XBoAWrOrbrGMl4CxfSNv19Y0Rqz63m-6Q9cTa56hfzkQ,"Millimeter wave (mmWave) communication has attracted considerable attention as a key technology for the next-generation wireless communications thanks to its exceptional advantages. MmWave leads the way to achieve a high transmission quality with directed narrow beams from source to multiple destinations by adopting different antenna beamforming (BF) techniques, which have a pivotal role in establishing and maintaining robust links. However, realizing such BF gains in practice requires overcoming several challenges, such as severe signal deterioration, hardware constraints, and design complexity. The elevated complexity of configuring mmWave BF vectors encourages researchers to leverage relevant machine learning (ML) techniques for better BF configurations deployment in 5G and beyond. In this article, we summarize mmWave BF strategies employed for future wireless networks. Then, we provide a comprehensive overview of ML techniques plus its applications and promising contributions toward efficient mmWave BF deployment. Furthermore, we discuss mmWave BF’s future research directions and challenges. Finally, we discuss a single and concurrent mmWave BF case study by applying multiarmed bandit to confirm the superiority of ML-based methods over conventional ones.",,10.1109/JSYST.2021.3089536,"Training , Feature extraction , 5G mobile communication , Sensors , Recurrent neural networks , Location awareness , IEEE 802.11 Standard "
167,Deep learning based prediction of signal-to-noise ratio (SNR) for LTE and 5G systems,"T Ngo, B Kelley, P Rad",2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9272470/?casa_token=nAga-WFxTg8AAAAA:Q0sUPYDvrtd_BXzeYA3_Gl87nrQXy5bZUA4VdEj2s0fvYQqu1IxPhTUJ6MkDfPWwq5wJt2Ep_eCzJQ,"Deep learning (DL) is applied to predict signal-to-noise ratio (SNR) in de facto LTE and 5G systems in a non-data-aided (NDA) manner. Various channel conditions and impairments are considered, including modulation types, path delays, and Doppler shifts. Both time-domain and frequency-domain signal grids are evaluated as inputs for SNR prediction. A combination of convolutional neural network (CNN) and long short term memory (LSTM) - CNN-LSTM - is used as the SNR predictor. Learning both spatial and temporal features is known to improve DL prediction accuracy. Techniques employed to enhance performance are SNR range/resolution manipulation, binary prediction, and multiple input prediction. Computer simulation is conducted using MATLAB LTE, 5G, and DL toolboxes to generate OFDM signals, model fading channels with AWGN noise, and construct CNN-LSTM. Simulation results show, with off-line training, DL based prediction of SNR in LTE and 5G systems has better accuracy and latency than traditional estimation techniques. Specifically, SNR prediction for SNR range of [-4, 32] dB and resolution of 2 dB utilizing time-domain signals has an accuracy of 100%, hence normalized mean square error (NMSE) of zero, and a latency of 1 millisecond or less.",,10.1109/WINCOM50532.2020.9272470,"Signal to noise ratio , Modulation , 5G mobile communication , Estimation , Time-domain analysis , Long Term Evolution , Frequency-domain analysis "
168,Implementing 5G NR features in FPGA,"J Bishop, JM Chareau, ...",2018 European Conference on Networks and Communications (EuCNC),2575-4912,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8443214/?casa_token=SQUtheh0HH0AAAAA:Xmxf6reQwW9_dDG3ViR8Dl45Ha0d5VNmEmowpOsLbLLrnNY-XtxvdGhxVHfoJFuDJnzO61KV5f_UnA,"A set of physical layer features defined in the 3GPP 38 series standards (5G) were implemented in commercial off the shelf SDR products. The system created an over-the-air link operating at 3.5GHz, having 40MHz channel width and using 256QAM. The PHY layer was executed in the FPGA of the SDR transceiver; higher layers executed in the host PC controlling the radio frontend. With a 4 K video stream as payload, the link allowed us to experiment with: 5G NR numerologies; frequency shaping techniques for reducing OOB emissions and improving spectral utilization; higher order QAM modulation; and mixed numerology communications.",,10.1109/EuCNC.2018.8443214,"OFDM , Finite impulse response filters , Field programmable gate arrays , 5G mobile communication , Long Term Evolution , Downlink , Transceivers "
169,Digital predistortion for 5G small cell: GPU implementation and RF measurements,"P Pascual Campo, V Lampu, A Meirhaeghe, ...",Journal of Signal …,,,,,2020,Springer,https://link.springer.com/article/10.1007/s11265-019-01502-4,"… a GPU. Previous works have shown that … GPU is 1.050 GHz, and that the HD GPU can theoretically provide a maximum of 384 FLOPs per clock cycle [13], we can calculate that the GPU …",,,
170,Fiber-fed distributed antenna system in an FPGA software defined radio for 5G demonstration,"S Mahboob, RG Vaughan",IEEE Transactions on Circuits and Systems II: Express Briefs,1558-3791,67,2,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8684264/?casa_token=U7fABo8BCDMAAAAA:YcenIOmCmnjy78gH2E9MBSkxYcdYe_zoKJQpp_bipLcyXGo0rQRg4UXv-p-5BxDMA2WvZST3PvSQZg,"The implementation of high-speed wireless networks, such as currently used fourth generation (4G) systems and future 5G systems, feature challenging processing. Field programmable gate arrays (FPGAs) can straddle research and development for these current and future networks since they provide scaling through reconfigurable logic, high parallelism, and low power consumption. This brief demonstrates an FPGA circuit implementation, with measurements, of a minimal system (a 5G element or “unit cell”): a single-user mobile with antenna diversity and a distributed antenna system (DAS) at the base station. The demonstration system has a bandwidth of 20 MHz, runs at 2.4 GHz, and has two antennas at both the transmitting base station and at the receiving mobile. The modulation is orthogonal frequency division multiplexing (OFDM) with space-time block coding (STBC). The FPGA is a Virtex-6, used for software defined radio (SDR), and this can readily be scaled to handle larger-dimensioned, higher-capacity systems. The receiver has time-offset synchronization, frequency-offset, and channel estimation. The high-level algorithm design (Xilinx System Generator) for these functions and the OFDM-STBC, and the resources consumed on the FPGA during real-time implementation, are included. We also compare the use of coax and fiber for linking the distributed antennas, using off-the-shelf components. The approach used here of combining simulations with physical measurement of a minimal system is a practical way forward for assessing candidate systems for 5G.",,10.1109/TCSII.2019.2909762,"Field programmable gate arrays , OFDM , MIMO communication , 5G mobile communication , Receivers , Antennas , Bandwidth "
171,Machine Learning for Securing SDN based 5G network,"HA Alamri, V Thayananthan, J Yazdani",Int. J. Comput. Appl,,,,,2021,researchgate.net,https://www.researchgate.net/profile/Hassan-Alamri/publication/348535226_Machine_Learning_for_Securing_SDN_based_5G_Network/links/6002787e92851c13fe147c8c/Machine-Learning-for-Securing-SDN-based-5G-Network.pdf,"… The fifth-generation (5G) network supports many systems … When malicious users send DDoS attacks, the SDN based 5G … is to analyze the suitable machine learning (ML) for securing …",,,
172,"Intelligent zero trust architecture for 5G/6G networks: Principles, challenges, and the role of machine learning in the context of O-RAN","K Ramezanpour, J Jagannath",Computer Networks,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S1389128622003929?casa_token=0JaFH8EyJLcAAAAA:sKovoqEwF9PnzR3-utv5yLgR-ceZJo20AmCQ-PO-h06TLSQvqC660aKpfUxoabzRODqrnWx9EIE,"… machine learning data. Therefore, this work provides novel research directions to design machine learning based components that contribute towards i-ZTA for the future 5G/6G …",,,
173,Edge intelligence in softwarized 6G: Deep learning-enabled network traffic predictions,"S Zeb, MA Rathore, A Mahmood, ...",2021 IEEE Globecom Workshops (GC Wkshps),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9682131/,"The 6G vision is envisaged to enable agile network expansion and rapid deployment of new on-demand microservices (e.g., visibility services for data traffic management, mobile edge computing services) closer to the network’s edge IoT devices. However, providing one of the critical features of network visibility services, i.e., data flow prediction in the network, is challenging at the edge devices within a dynamic cloud-native environment as the traffic flow characteristics are random and sporadic. To provide the AI-native services for the 6G vision, we propose a novel edge-native framework to provide an intelligent prognosis technique for data traffic management in this paper. The prognosis model uses long short-term memory (LSTM)-based encoder-decoder deep learning, which we train on real time-series multivariate data records collected from the edge µ-boxes of a selected testbed network. Our result accurately predicts the statistical characteristics of data traffic and verifies the trained model against the ground truth observations. Moreover, we validate our novel framework with two performance metrics for each feature of the multivariate data.",,10.1109/GCWkshps52748.2021.9682131,"6G mobile communication , Measurement , Intelligent networks , Multi-access edge computing , Computational modeling , Microservice architectures , Telecommunication traffic "
174,Evolution toward artificial intelligence of things under 6G ubiquitous-X,"P Zhang, X Xu, X Qin, Y Liu, N Ma, ...",Journal of Harbin Institute …,,,,,2020,hit.alljournals.cn,http://hit.alljournals.cn/html/jhit_cn/2020/3/20200309.html,"… Abstract: With the evolution of cellular networks, 6G is a … act as the artificial intelligence assistance for 6G users, is the … Supported by 6G, IoT will step into the Artificial Intelligence of …",,,
175,Parallel and flexible 5G LDPC decoder architecture targeting FPGA,"J Nadal, A Baghdadi",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,1557-9999,29,6,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9420267/?casa_token=SbMPM6zJzD4AAAAA:9sP_uuyBbqbU0Jcwx2xT1xdyGZ0GfWxOXWmTEW84mhrGPj8ojrgqhwGEdNSfN1Q6mimJD2Bxq1dIpQ,"The quasi-cyclic (QC) low-density parity-check (LDPC) code is a key error correction code for the fifth generation (5G) of cellular network technology. Designed to support several frame sizes and code rates, the 5G LDPC code structure allows high parallelism to deliver the high demanding data rate of 10 Gb/s. This impressive performance introduces challenging constraints on the hardware design. Particularly, allowing such high flexibility can introduce processing rate penalties on some configurations. In this context, a novel highly parallel and flexible hardware architecture for the 5G LDPC decoder is proposed, targeting field-programmable gate array (FPGA) devices. The architecture supports frame parallelism to maximize the utilization of the processing units, significantly improving the processing rate. The controller unit was carefully designed to support all 5G configurations and to avoid update conflicts. Furthermore, an efficient data scheduling is proposed to increase the processing rate. Compared to the recent related state of the art, the proposed FPGA prototype achieves a higher processing rate per hardware resource for most configurations.",,10.1109/TVLSI.2021.3072866,"5G mobile communication , Decoding , Parity check codes , Hardware , Quantization (signal) , Parallel processing , Field programmable gate arrays "
176,FPGA-SDR integration and experimental validation of a joint DA ML SNR and doppler spread estimator for 5G cognitive transceivers,"H Haggui, S Affes, F Bellili",IEEE Access,2169-3536,7,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8726401/,"In a multi-connected, multi-technology, and pervasive mobile infrastructure, such as what is being planned for 5G, artificial intelligence and cognition will play a major role. An important goal of future mobile infrastructures is to self-adapt their characteristics to their operating conditions, at the physical link, as well as at the network and application layers, which gives rise to a new paradigm known as context-aware cognitive radio (CR). CR transceivers (CTRs) mostly incorporate a cognitive engine that relies on various sensorial entities, which attempt to provide sufficient information about the quality of the link through the estimation of various key channel parameters. Two important parameters are required in a wide range of CTR architectures: the signal-to-noise ratio (SNR) and the Doppler spread. Within this context, we tackle the hardware design and integration of a joint data-aided (DA) maximum likelihood (ML) SNR and Doppler spread estimator recently shown to outperform main state-of-the-art solutions both in terms of accuracy and complexity. We propose a deep-pipelined and resource-efficient architecture for the outlined joint DA ML estimator, and we integrate our design on an FPGA-based software-defined radio (SDR) platform. We finally validate and test this prototype in real time under realistic over-the-air propagation conditions reproduced by a highly-scalabile channel emulator. Compared to its MATLAB floating-point version, our hardware prototype suggests negligible losses in performance despite the existence of several hardware impairments, thereby confirming its very strong potential and attractiveness for possible integration in future 5G CTRs.",,10.1109/ACCESS.2019.2919978,"Doppler effect , Signal to noise ratio , Maximum likelihood estimation , Hardware , Field programmable gate arrays , 5G mobile communication , Computer architecture "
177,Potential technologies and applications based on deep learning in the 6G networks,"Z Zheng, L Wang, F Zhu, L Liu",Computers and Electrical Engineering,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S0045790621003426?casa_token=W4890RBcpXMAAAAA:52zeMimHdqezzJGpElFil6ukwZVAdbGvPLNCGJALgG5Scmxsywldfyoj1xNb7PQxqZV4j3pK7wU,"… deep learning greatly. Finally, we introduce some potential applications based on deep learning in 6G … We believe the combination of 6G networks and deep learning would unleash …",,,
178,Unsupervised machine learning in 6g networks-state-of-the-art and future trends,"VP Rekkas, S Sotiroudis, P Sarigiannidis, ...",2021 10th International Conference on Modern Circuits and Systems Technologies (MOCAST),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9493388/?casa_token=uf5ifVEnX1IAAAAA:mCz3YQxSjEofb2wfomDwJoTfyYTobCxcCj2il41xWbckgj6vAJ7cp3ci9K8kYk0TBmFXNa8DveO7HQ,"Wireless communication systems play a very crucial role for business, commercial, health and safety applications. With the commercial deployment of fifth generation (5G), academic and industrial research focuses on the sixth generation (6G) of wireless communication systems. Artificial Intelligence (AI) and especially Machine Learning (ML), will be a key component of 6G systems. Here, we present an up-to-date review of future 6G wireless systems and the role of unsupervised ML techniques in them.",,10.1109/MOCAST52088.2021.9493388,"6G mobile communication , Wireless communication , Machine learning algorithms , Circuits and systems , Focusing , Machine learning , Market research "
179,Artificial intelligence and machine learning in 5G and beyond: a survey and perspectives,"A Haidine, FZ Salmam, A Aqqal, ...",… Technologies for 5G and …,,,,,2021,books.google.com,https://books.google.com/books?hl=en&lr=&id=hIc_EAAAQBAJ&oi=fnd&pg=PA47&dq=%22artificial+intelligence%22%7C%22machine+learning%22%7C%22deep+learning%22+%22intelligent+radio%22%7C%22cognitive+radio%22%7C%225g%22%7C%226g%22&ots=qlASjQ28hp&sig=IQ1a93dFOAtd3Jk8TimXN8raoz4,"… Using new radio interface based on massive MIMO, 5G has overcame some of these … In this chapter, we describe the role of artificial intelligence and machine learning in 5G and …",,,
180,Performance analysis of deep learning-based routing protocol for an efficient data transmission in 5G WSN communication,"G Arya, A Bagwari, DS Chauhan",IEEE Access,2169-3536,10,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9676565/,"For the past few years, huge interest and dramatic development have been shown for the Internet of Things (IoT) based constrained Wireless sensor network (WSN) to achieve efficient resource utilization and better service delivery. IoT requires a better communication network for data transmission between heterogeneous devices and an optimally deployed energy-efficient WSN. The clustering technique applied for WSN node deployment needs to be efficient; therefore, the entire architecture can obtain a better network lifetime. While clustering, the entire network is partitioned into various clusters. Moreover, the cluster head (CH) selection process also needs proper attention for achieving efficient data communication towards the sink node via selected CH and also for increasing the node reachability within the cluster. In this proposed framework, an energy efficient deep belief network (DBN) based routing protocol is developed, which achieves better data transmission through the selected path. Due to this the packet delivery ratio (PDR) gets improved. In this framework, initially, the nodes in the whole network is grouped as clusters using a reinforcement learning (RL) algorithm, which assigns a reward for the nodes that belong to the particular cluster. Then, the CH required for efficient data communication is selected using a Mantaray Foraging Optimization (MRFO) algorithm. The data is transmitted to the sink node via the selected CH using an efficient deep learning approach. At last, the performance of proposed deep network based routing protocol is evaluated using different evaluation metrics they are network lifetime, energy consumption, number of alive nodes, and packet delivery rate. Finally, the evaluated results are compared with few existing algorithms. Among all these algorithms, the proposed DBN routing protocol has achieved better network lifetime.",,10.1109/ACCESS.2022.3142082,"Routing , Wireless sensor networks , Routing protocols , Clustering algorithms , Optimization , 5G mobile communication , Machine learning algorithms "
181,Deep learning and onion routing-based collaborative intelligence framework for smart homes underlying 6g networks,"NK Jadav, R Gupta, MD Alshehri, ...",IEEE Transactions on Network and Service Management,2373-7379,19,3,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9749122/?casa_token=MeK-lc3LcpYAAAAA:VdiOqJ-tZzWLgErplZZEmdzkjBFVBem9G-IwYOw9lAGV9go4hmwiVfrb7UqwWAFvWqcYtXxQ9y7Azg,"Sensor communication in the smart home environment is still in its infancy as the information exchange between sensors is vulnerable to security threats. Many traditional solutions use single-layer or multi-layer (i.e., onion routing protocol) encryption/decryption algorithms. But, in the traditional onion routing protocol, if the directory server is compromised, it may not track the malicious onion nodes within the onion network. It questioned the path anonymity of the onion routing protocol. Motivated by this, we proposed a blockchain and onion routing (OR)-based secure and trusted framework in the paper. The anonymity of the proposed OR network is maintained by storing and tracking the onion nodes threshold values through the blockchain network. A long short-term memory (LSTM) model is also utilized to classify the sensors data requests as malicious and non-malicious. The performance of the proposed system is evaluated with different performance metrics such as F1 score and accuracy. The LSTM model significantly improves the initial detection rate of malicious data requests from smart home sensors. Over these benefits, we considered the entire communication via 6G channel, reducing the overall communication latency. Additionally, the OR network is simulated over the shadow simulator to analyze the OR network’s performance considering parameters such as packet delivery ratio and malicious onion node detection rate.",,10.1109/TNSM.2022.3164715,"Smart homes , Routing , Encryption , Blockchains , Cryptography , Artificial intelligence , Routing protocols "
182,Experimental demonstration of 5G fronthaul and backhaul convergence based on FPGA-based active optical transport,"A Farhadi-Beldachi, E Huques-Satas, ...",2018 European Conference on Optical Communication (ECOC),,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8535380/?casa_token=eog9oTusHhYAAAAA:GrMsWFRYV9eIzcUt5f3k4gYF2w4hzSOKyzkjdimg7HzECb4RVR6pUu6Mf3Gy_EiZ4ZDCMJNR4p7m8A,We demonstrate a dynamic frame-based optical network in support of a 5G transport supporting both Backhaul and Fronthaul functionalities exploiting FPGA-based real-time optical active technologies. This solution is successfully evaluated over a city field trial with <;1dB power penalty.,,10.1109/ECOC.2018.8535380,"Ethernet , 5G mobile communication , Optical fibers , Clocks , Optical fiber networks , Urban areas "
183,A survey: Distributed Machine Learning for 5G and beyond,"O Nassef, W Sun, H Purmehdi, M Tatipamula, ...",Computer Networks,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S1389128622000421?casa_token=k443C0aBzpoAAAAA:HAasy962nsf8VRRahWu6K0pfh5rWbcjCuJvUODKAQjgwnYcsOAhj_SpAkMKd9OsY6PZctCqneWQ,"… Distributed ML exploits recent Artificial Intelligence … 5G and Beyond. These different aspects do not only pertain to 5 G , but will also enable careful design of distributed machine learning …",,,
184,Deep learning techniques for advancing 6G communications in the physical layer,"S Zhang, J Liu, TK Rodrigues, ...",IEEE Wireless Communications,1558-0687,28,5,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9508930/?casa_token=-aECpzXTCN4AAAAA:YBbcyFxXZJyPPJJX-nuQW90E-SDx61xIE6ktrw6IoZUr1v-rNpVwo8NeTzBDcJM4xsc943HRNhOzGg,"As current 5G communication systems cannot fulfill the stringent requirements brought by emerging applications, 6G will innovatively employ deep learning (DL) techniques to fundamentally rethink the communication systems design problem from the bottom to top layers. Although recent evidence has shown the power of DL techniques in the communication domain, the exploration and utilization of DL techniques in communication systems is still in its infancy and should come in a progressive manner. To effectively and efficiently implement DL techniques in future 6G communications in the physical layer, we give some potential deployment strategies and key enabling technologies that relate to 6G in terms of joint design of block-structured and end-to-end DL, integration of model-driven and data-driven DL, combination of online and offline training, ubiquitous learning and explainable DL techniques.",,10.1109/MWC.001.2000516,"6G mobile communication , Wireless communication , Communication systems , Training data , Complexity theory , Receivers , Mathematical model "
185,AI-based vehicular network toward 6G and IoT: Deep learning approaches,"MY Chen, MH Fan, LX Huang",ACM Transactions on Management …,,,,,2021,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3466691?casa_token=wai9SoiATJwAAAAA:RMO_aHsRr5zzdLeMjDy91C1cDCQgIwKEYMfigtgPpa__2Dp99s2bOMgSr13NL0DLgniNruIwPCF82w,"… Recently, deep learning (DL) has emerged as a powerful artificial intelligence (AI) … To address potentially dangerous driver behavior, this study applies deep learning approaches to …",,,
186,"Toward native artificial intelligence in 6G networks: System design, architectures, and paradigms","J Wu, R Li, X An, C Peng, Z Liu, J Crowcroft, ...",arXiv preprint arXiv …,,,,,2021,arxiv.org,https://arxiv.org/abs/2103.02823,… and 6G is envisioned to go far beyond the communication-only purpose. There is coming to a consensus that 6G will treat Artificial Intelligence (… architecture design in 6G and deserves a …,,,
187,Artificial intelligence image recognition based on 5G deep learning edge algorithm of Digestive endoscopy on medical construction,"L Yang, Z Li, S Ma, X Yang",Alexandria Engineering Journal,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S1110016821004774,"… the image based on 5G Deep learning edge algorithm to … on 5G deep learning edge algorithm under artificial intelligence … accuracy, the 5G deep learning edge algorithm in this paper …",,,
188,Deep transfer learning-based network traffic classification for scarce dataset in 5G IoT systems,"J Guan, J Cai, H Bai, I You",International Journal of Machine Learning …,,,,,2021,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s13042-021-01415-4&casa_token=bLCbVcKeos0AAAAA:OLAUFCZqBa3b1DTAJqbI-zmvEGT-efL_LOF7BdDJX_z7TLOYSZscvw1E5cVE1Qcur_AJ3bQW-gLc2icIiA,"… on deep transfer learning for 5G IoT scenarios with scarce labeled data … dataset are used to label the data samples, the classification accuracy is close to the results of full training dataset…",,,
189,Integrated sensing and communication for 6G: Ten key machine learning roles,"U Demirhan, A Alkhateeb",IEEE Communications Magazine,1558-1896,PP,99,,2023,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/10049816/?casa_token=Eksfwy5snngAAAAA:aRjiwyvwHaGJa5515X2ncvaFQ8jU9dztClO8vp-jOfWKHLioZ39XBFCsEWlJNglM0vGMfowEOFhtwA,"Integrating sensing and communication is a defining theme for future wireless systems. This is motivated by the promising performance gains, especially as they assist each other, and by the better utilization of the wireless and hardware resources. Realizing these gains in practice, however, is subject to several challenges where leveraging machine learning can provide a potential solution. This article focuses on ten key machine learning roles for joint sensing and communication, sensingaided communication, and communication-aided sensing systems, explains why and how machine learning can be utilized, and highlights important directions for future research. The article also presents real-world results for some of these machine learning roles based on the large-scale real-world dataset DeepSense 6G, which could be adopted in investigating a wide range of integrated sensing and communication problems.",,10.1109/MCOM.006.2200480,"Sensors , Machine learning , Hardware , Wireless communication , Interference cancellation , Optimization , Wireless sensor networks "
190,"Deep learning era for future 6G wireless communications—theory, applications, and challenges","SKB Sangeetha, R Dhaya",Artificial Intelligent Techniques for …,,,,,2022,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119821809.ch8,"… of 6G wireless communication with the detail of how deep learning made a contribution to 6G … In 6G, we intend to see Artificial Intelligence (AI) at the edges of the network in action with …",,,
191,Artificial Intelligence in Wireless Communications-Evolution Towards 6G Mobile Networks,"TB Iliev, EP Ivanova, IS Stoyanov, ...","2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",2623-8764,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9597147/?casa_token=mh7e0dTcfa4AAAAA:46AB9h5HAd_Lt2n6GVTKFsG3fXFydf_rTGCq8LJZnTPd10sq1gz4PqLa5NS7_K8zae2vZIAI55Htag,"With the deployment of the 5G in wireless communications, the researchers' interest is focused on the sixth generation networks. This forthcoming generation is expected to replace the 5G network by the end of 2030. Artificial intelligence is one of the leading technologies in 5G, beyond 5G, and future 6G networks. Intelligence is endowing the tendency to throw open the capabilities of the 5G networks and the future 6G mobile wireless networks by leveraging the universal infrastructure, open network architectures, software-defined networking, network function virtualization, multi-access edge computing, vehicular network, etc. This discussion is aimed at providing, in a comprehensive manner, how artificial intelligence can be integrated into different applications and finally, we analyse and discuss the opportunities and main technical challenges of the wireless communication standards, present novel approaches, and recent results that will encourage the development and implementation of the sixth generation networks.",,10.23919/MIPRO52101.2021.9597147,"6G mobile communication , Underwater communication , Three-dimensional displays , 5G mobile communication , Wireless networks , Terahertz materials , Ultra reliable low latency communication "
192,Deep learning-aided 6G wireless networks: A comprehensive survey of revolutionary PHY architectures,"B Ozpoyraz, AT Dogukan, Y Gevez, U Altun, ...",arXiv preprint arXiv …,,,,,2022,arxiv.org,https://arxiv.org/abs/2201.03866,"… Abstract—Deep learning (DL) has proven its unprecedented … thoroughly intelligent society with 6G wireless networks, new … the way for fascinating applications of 6G. In particular, we …",,,
193,Machine learning for physical layer in 5G and beyond wireless networks: A survey,"J Tanveer, A Haider, R Ali, A Kim",Electronics,,,,,2022,mdpi.com,https://www.mdpi.com/article/10.3390/electronics11010121,"… With their enhanced speed, 5G networks are prone … 5G technologies that emphasize machine learning-based solutions to cope with existing and future challenges. First, we discuss 5G …",,,
194,Adaptive artificial intelligence for resource-constrained connected vehicles in cybertwin-driven 6g network,"S Shen, C Yu, K Zhang, S Ci",IEEE Internet of Things Journal,2372-2541,8,22,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9502164/?casa_token=29a5vTJ0XD0AAAAA:Zq5ZqMe1QbJpV-tF1uJll5EncN8DVisKCyHPuHBX1o91zkKG-PREntikleTIPjX6Mrx2OAlAONnODg,"The emerging technology of cybertwin is expected to bring revolutionary benefits to the sixth-generation (6G) network in respect of communication, resources allocation, and digital asset management. Empowered by ubiquitous artificial intelligence (AI), cybertwin is capable of adjusting the requests for computing resources to support network services by analyzing user’s demands for quality of experience and resource scarcity in the market. For resource-constrained applications, such as connected vehicles in the 6G network, cybertwin can intelligently determine the time-varying requests of computing resources for various vehicles at different times. However, the current service architecture executes AI algorithms with universal configurations for all vehicles. This causes the difficulty of customizing the complexity of AI algorithms to maintain adaptive to cybertwin’s decisions on dynamic resources allocation. In this article, we propose an adaptive AI framework based on efficient feature selection to cooperate with cybertwin’s resource allocation. This proposed framework can adaptively customizing AI model complexity with available computing resources. Specifically, we systematically characterize the aggregated impacts of all feature combinations on the modeling outcomes of AI algorithms. By utilizing nonadditive measures, the interactions among features can be quantified to indicate their contributions to the modeling process. Then, we propose an efficient algorithm to obtain accurate interaction measures for adaptive feature selection to balance the tradeoff between modeling accuracy and computational overhead. Finally, extensive simulations are conducted to validate that our proposed framework substantially reduces the overhead of AI algorithms while guaranteeing desired modeling accuracy for cybertwin-driven connected vehicles in 6G.",,10.1109/JIOT.2021.3101231,"Artificial intelligence , Computational modeling , Connected vehicles , Adaptation models , Dimensionality reduction , 6G mobile communication , Complexity theory "
195,"Remote control of a robot rover combining 5g, ai, and gpu image processing at the edge","F Civerchia, F Giannone, K Kondepu, ...",Optical Fiber …,,,,,2020,opg.optica.org,https://opg.optica.org/abstract.cfm?uri=OFC-2020-M3Z.10,… Edge technologies can also benefit from the development of edge micro data-centers [4] in which the traditional elaboration based on Central Processing Units (CPU) is …,,,
196,High-performance AES-128 algorithm implementation by FPGA-based SoC for 5G communications,"P Visconti, R Velazquez, S Capoccia, ...",International Journal of …,,,,,2021,academia.edu,https://www.academia.edu/download/68310994/59_1570680428_25252_EM_4feb21_14jan21_14oct20_Rz.pdf,"… based on the Xilinx ZCU102 FPGA board is presented, suitable for 5G communications. In … with the other sections of the 5G communication apparatus, synchronization and control …",,,
197,Artificial intelligence for 6G networks: Technology advancement and standardization,"MK Shehzad, L Rose, MM Butt, ...",IEEE Vehicular Technology Magazine,1556-6080,17,3,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9768336/?casa_token=_LTj-yLkWgQAAAAA:9B5_u4KWru8g7Hf4g100ZICNQpS7KiznygMrIwNCyiHY-VDHQK1b7VUkZRoUtzfhMyqWMaa4KwP6Aw,"With the deployment of 5G networks, standards organizations have started working on the design phase for 6G networks. 6G networks will be immensely complex, requiring more deployment time, cost, and management efforts. On the other hand, mobile network operators demand these networks to be intelligent, self-organizing, and cost-effective to reduce operating expenses (OPEX). Machine learning (ML), a branch of artificial intelligence (AI), is the answer to many of these challenges by providing pragmatic solutions, which can entirely change the future of wireless network technologies. By using some case study examples, we briefly examine the most compelling problems, particularly at the physical layer (PHY) and link layer in cellular networks, where ML can bring significant gains. We also review standardization activities in relation to the use of ML in wireless networks and a future timeline on the readiness of standardization bodies to adapt to these changes. Finally, we highlight major issues in ML use in wireless technology, and provide potential directions to mitigate some of them in 6G wireless networks.",,10.1109/MVT.2022.3164758,"6G mobile communication , Wireless networks , Artificial intelligence , Principal component analysis , Channel estimation , Cellular networks , Unsupervised learning , Standards "
198,A comprehensive review on artificial intelligence/machine learning algorithms for empowering the future IoT toward 6G era,"MR Mahmood, MA Matin, P Sarigiannidis, ...",IEEE Access,2169-3536,10,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9861650/,"The evolution of the wireless network systems over decades has been providing new services to the users with the help of innovative network and device technologies. In recent times, the 5G network systems are about to be deployed which creates the opportunity to realize massive connectivity with high throughput, low latency, high energy efficiency and security. It also focuses on providing massive Internet of Things (IoT) network connectivity as well as services for good health, large-scale agricultural and industrial production, intelligent traffic control and electricity generation, transmission and distribution systems. However, the ever-increasing number of user devices is directing the researchers towards beyond 5G systems to allocate these user devices with higher bandwidth. Researches on the 6G wireless network systems have already begun to provide higher bandwidth availability for densely connected larger network devices with QoS surety. Researchers are leveraging artificial intelligence (AI)/machine learning (ML) for enhancing future IoT network operations and services. This paper attempts to discuss AI/ML algorithms that can help in developing energy efficient, secured and effective IoT network operations and services. In particular, our article concentrates on the major issues and factors that influence the design of the communication systems for future IoT with the integration of AI/ML. It also highlights application domains, including smart healthcare, smart agriculture, smart transportation, smart grid and smart industry that can operate efficiently and securely. Finally, this paper ends with the discussion on future research scopes with these algorithms in addressing the open issues of the future IoT network systems.",,10.1109/ACCESS.2022.3199689,"6G mobile communication , Communication systems , 5G mobile communication , Internet of Things , Wireless networks , Network systems , Energy efficiency , Artificial intelligence , Machine learning "
199,Machine learning for 5G MIMO modulation detection,"HB Chikha, A Almadhor, W Khalid",Sensors,,,,,2021,mdpi.com,https://www.mdpi.com/1009436,… -output (MIMO) systems for 5G communications in the presence of spatially correlated … between the random committee and the AdaBoost machine learning techniques (MLTs) at low …,,,
200,Vehicle artificial intelligence system based on intelligent image analysis and 5G network,"B Liu, C Han, X Liu, W Li",International Journal of Wireless Information …,,,,,2021,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10776-021-00535-6&casa_token=Tbj8PwPfV-wAAAAA:_Cmi3Wj_IZrP2wSqYte1rXt6I1WYyVq1wtzA0HHuK9dbBnpH-OKeU8DVTArAOl1bg52oZxd3NMvCk9LE_Q,"… As an outcome of 5G, a shift in how we communicate traffic flow could be increased by explaining that intelligent traffic management systems on a 5G network could help improve traffic …",,,
201,Analysis of network slicing for management of 5G networks using machine learning techniques,"R Singh, A Mehbodniya, JL Webber, ...",… and Mobile Computing,,,,,2022,hindawi.com,https://www.hindawi.com/journals/wcmc/2022/9169568/,"… Additionally, 5G networks provide higher availability, extremely high capacity, increased … Network slicing is an architectural framework for 5G networks that is intended to accommodate a …",,,
202,An FPGA-based 1-bit digital transmitter with 800-MHz bandwidth for 5G millimeter-wave active antenna systems,"M Tanio, S Hori, N Tawa, T Kuwabara, ...",2018 IEEE/MTT-S International Microwave Symposium - IMS,2576-7216,,,,2018,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8439663/,"An FPGA-based 1-bit digital transmitter with an 800-MHz bandwidth is presented for 5G millimeter wave (mmW) active antenna systems (AASs). To achieve over 20-GHz operation of the 2nd-order delta sigma modulator (DSM), a two-stage time-in-terleaved accumulator with double look-ahead blocks is proposed and implemented in a field-programmable gate array (FPGA). This transmitter achieves an 800-MHz bandwidth in the 27-GHz band under conditions satisfying the 3GPP specification, which verifies that this transmitter is a promising candidate for 5G mmW communication.",,10.1109/MWSYM.2018.8439663,"Bandwidth , 5G mobile communication , Field programmable gate arrays , Signal to noise ratio , Radio transmitters , 3GPP "
203,Improved dropping attacks detecting system in 5g networks using machine learning and deep learning approaches,"A Mughaid, S AlZu'bi, A Alnajjar, E AbuElsoud, ...",Multimedia Tools and …,,,,,2022,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11042-022-13914-9&casa_token=aMT8CX7TjNUAAAAA:wsJGWZLJCbftyr9eKzc5yPac0Uf-m_63W6lvZDy6jrYbRf41tpps5qYysUbt78qiRuqBGgbN8KMNtjWr9Q,"… The primary enhancement in 5G is the speed, which may be … to the Internet, and 5G will demand considerable improvements … for wireless cyberattack detection in 5G networks based on …",,,
204,Reduction of satellite images size in 5G networks using machine learning algorithms,"TVK Moorthy, AK Budati, S Kautish, ...",IET …,,,,,2022,Wiley Online Library,https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cmu2.12354,"… In 5G era, Deep learning methods are widely utilized in … The 5G technology incorporates distributed cloud within the network… the combination of Artificial Intelligence (AI) and 5G network. …",,,
205,"Deep learning-based solutions for 5G network and 5G-enabled Internet of vehicles: advances, meta-data analysis, and future direction",MS Almutairi,Mathematical Problems in Engineering,,,,,2022,hindawi.com,https://www.hindawi.com/journals/mpe/2022/6855435/,… This section presents the papers that design hybrid deep learning for solving machine learning problem in 5G wireless mobile network. Luo et al. [67] employed the hybrid of CNN and …,,,
206,FPGA based technical solutions for high throughput data processing and encryption for 5G communication: A review,"P Visconti, R Velazquez, CDV Soto, ...",TELKOMNIKA …,,,,,2021,telkomnika.uad.ac.id,http://telkomnika.uad.ac.id/index.php/TELKOMNIKA/article/view/18400,"… Also, the FPGA platforms offer optimal performances in terms … of main FPGA devices’ applications in the 5G networks/… In particular, different FPGA implementations of 5G building …",,,
207,Demonstration of FPGA-based A-IFoF/mmWave transceiver integration in mobile infrastructure for beyond 5G transport,"P Toumasis, K Kanta, K Tokas, ...",2021 European Conference on Optical Communication (ECOC),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9606061/?casa_token=c2kVWY8kIkMAAAAA:ZwyP02wXibuZ0sTHgyVYFPXJFUrIjE9NFMyoFsV26w6ljKWS_EE0G7YeR4tlb8D7pQIojQQtVPZZ8A,"We demonstrate the successful operation of an FPGA-based A-IFoF/mmWave transceiver into an existing MNO infrastructure, delivering 4K video streaming and IP-calls over mobile core network. Physical layer connectivity was successfully established, with EVM measurements of &#x003C;10% for QPSK waveforms propagated through different optical-wireless network segments.",,10.1109/ECOC52684.2021.9606061,"Phase shift keying , OFDM , Optical propagation , Europe , Streaming media , Optical variables measurement , Optical fiber networks "
208,GPU-based LDPC decoding for vRAN systems in 5G and beyond,"C Tarver, M Tonnemacher, H Chen, ...",2020 IEEE International Symposium on Circuits and Systems (ISCAS),2158-1525,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9181064/?casa_token=llHbCO297roAAAAA:-mHxGTTaOBXwR2pStYX4dINF0CNCsOH10E2Ts04_bjKtKVrWNy9SO6gk2daOFaJqBgyicrUpV7FdYg,"Next-generation virtual radio access networks (vRAN) will benefit from the flexibility provided by virtualization in proposed Cloud-RAN configurations. These systems for 5G and beyond may consist of commodity hardware such as GPUs in data centers with multiple connected base stations (gNBs) flexibly receiving allocated resources depending on time-varying, real-time demands. In this paper, parallel reconfigurable algorithms and architectures for channel decoding are proposed. In particular, flexible rate and block length LDPC decoders for the new radio (NR) physical layer on GPU are characterized. We implement these GPU decoders using reduced word lengths of 8-bits to represent the log-likelihood ratios during decoding, and we utilize multiple GPU streams to process multiple blocks of codewords in parallel. These techniques allow our implementation to reduce the device transfer overhead and achieve the low-latency or high-throughput targets for 5G and beyond. Moreover, we integrate our decoder into the Open Air Interface (OAI) NR software stack to investigate virtualization capabilities when containerizing vRAN functionality such as the LDPC decoder.",,10.1109/ISCAS45731.2020.9181064,"Parity check codes , 5G mobile communication , Graphics processing units , Decoding , Throughput , Kernel "
209,5G NR LDPC decoding performance comparison between GPU & FPGA platforms,"A Aronov, L Kazakevich, J Mack, ...","2019 IEEE Long Island Systems, Applications and Technology Conference (LISAT)",2641-8053,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8816821/?casa_token=R8a2np9u8WIAAAAA:-QI_SUC5Og00EwUeBr8_UCFpb348SdYcWHYmIw4-K_b-sI-Gvc_Um2ql7y6iVL13qnS6oQIfA6fjNg,"Fifth Generation NR is the global standard for the wireless air interface that promises to deliver high reliability and low end-to-end latency while delivering ultra-high-speed data. The 3rd Generation Partnership Project (3GPP) has standardized Low Density Parity Check (LDPC) coding as the solution to satisfy the channel coding demands of 5G NR. The trend towards virtualization of traditionally hardware functions to reduce development and equipment costs motivates a GPU based SDR platform for 5G NR. To that end, we developed an optimized 3GPP compliant LDPC decoder [1]. Performance data was collected on this enhanced decoder algorithm hosted on both a field programmable gate array (FPGA) and a graphic processing unit (GPU) platform. The advantages and disadvantages of FPGA and GPU technology for LDPC decoding are discussed. Both implementations align with Standards, but we show that the GPU solution exhibits larger latency primarily due to memory accesses. Future work for improving the LDPC decoder latency on the GPU is described.",,10.1109/LISAT.2019.8816821,
210,FPGA implementation of LDPC decoder for 5G NR with parallel layered architecture and adaptive normalization,"A Katyushnyj, A Krylov, A Rashich, ...",2020 IEEE International Conference on Electrical Engineering and Photonics (EExPolytech),,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9243997/?casa_token=uu_IummbfLoAAAAA:ddesAZATKnwQyj4Xgj8SnD_XHOnB5BZ4XAN9tWu9AJHRzCMGhZ4c4cuine2oRMPItrbMxph9mEqulg,This paper presents the FPGA ASIC-like implementation of LDPC decoder for 5G NR BG2 and single lifting factor. The proposed implementation has two main features: architectural and algorithmic. The first is the parallel layered architecture with special offsets and inter-layer network which provides great scalability for variable lifting factors support. The second one is an adaptive normalization approach to increase decoder BER performance. The proposed implementation requires relatively small number resources of Xilinx Kintex Ultrascale FPGA and provides 1 information bit/s/cycle throughput for 10 iterations.,,10.1109/EExPolytech50912.2020.9243997,"5G mobile communication , Scalability , Throughput , Parity check codes , Decoding , Table lookup , Field programmable gate arrays "
211,Parallel-processing-based digital predistortion architecture and FPGA implementation for wide-band 5G transmitters,"H Huang, J Xia, S Boumaiza",2019 IEEE MTT-S International Microwave Conference on Hardware and Systems for 5G and Beyond (IMC-5G),,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9160360/,"This paper presents a bandwidth-scalable and hardware-efficient parallel-processing-based D PD architecture for wide-band 5G transmitters. By computing multiple data samples at each clock cycle in parallel, the proposed DPD architecture extends the bandwidth of a conventional serial DPD architecture, as limited by the maximum FPGA clock rate, to a much higher rate that is proportional to the number of parallel data paths. With a cross-bar structure devised to reroute the intermediate computation results between the parallel data paths, it allows advanced DPD model with memory and cross-terms to be constructed efficiently. F or proof-of-concept, the pruned Complexity-Reduced-Volterra (CRV) DPD with four parallel data paths has been implemented using an Xilinx Ultrascale+ FPGA to achieve a total linearization bandwidth of 1.25 GHz. Subsequently, a 28 GHz power amplifier modulated with 400 MHz QAM64 signals has been successfully linearized in the proposed DPD system in real-time.",,10.1109/IMC-5G47857.2019.9160360,"Field programmable gate arrays , Bandwidth , Engines , Clocks , Hardware , Computer architecture , Predistortion "
212,OPEX-limited 5G RAN slicing: an over-dataset constrained deep learning approach,"H Chergui, C Verikoukis",ICC 2020 - 2020 IEEE International Conference on Communications (ICC),1550-3607,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9149051/?casa_token=CTV_nOcw2oAAAAAA:umoQLHIw2eDpVhCrJ9m0xfoccsi2YXgeMBX4ORbD9xasOHqbI-dO1-irWGTjAlCd8vL-diV2VOSpIw,"In this paper, we investigate the concept of OPEX-limited resource provisioning as a key component in fifth generation (5G) radio access networks (RAN) slicing. The different RAN slices' tenants (i.e. logical operators) are dynamically allocated isolated portions of physical resource blocks (PRBs), baseband processing resources and backhaul capacity. To achieve this dynamic resource allocation, we rely on key performance indicators (KPIs) datasets stemming from a live cellular network endowed with traffic probes. These datasets are used to train a new class of deep neural networks (DNNs) models where OPEX requirements, formulated as non-convex non-differentiable violation rate constraints, are also dataset-dependent. The designed constrained DNNs are then optimized via a non-zero sum two-player game strategy. In this respect, we highlight the effect of the different hyperparameters on the respect of the OPEX limitations, while ensuring a dynamic RAN resource orchestration that follows the slices' traffics trends.",,10.1109/ICC40277.2020.9149051,"Optimization , Machine learning , Neural networks , Pricing , Facebook , Resource management , Cloud computing "
213,A federated deep learning empowered resource management method to optimize 5G and 6G quality of services (QoS),"H Alsulami, SH Serbaya, EH Abualsauod, ...",… and Mobile Computing,,,,,2022,hindawi.com,https://www.hindawi.com/journals/wcmc/2022/1352985/,"… -edge machine learning techniques, this article gives readers an insight about how 5G vehicular … as a novel resource management technique to optimize 5G and 6G quality of services. …",,,
214,FPGA Design of Spatially Modulated Single-Input-Multiple-Output Signals in 5G Diversity Receivers,"R Ayoubi, J Daba","2019 IEEE International Conference on Communication, Networks and Satellite (Comnetsat)",,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8844099/?casa_token=LbiGm-pR1BAAAAAA:e6Dkt0tjbYLfIM8C6yTYOfN20JMpmPfxK-HmBgc4QeGmWA7dNgBC9YDR8K-VXpYIA3h7FHJIcKnG3Q,"In this work, FPGA implementation of a new optimal generalized receiver diversity combining scheme, termed Generalized Maximum Ratio Combining (GMRC), is implemented for transmission via 5G multiple-input-multiple-output (MIMO) channels. The MIMO channels comprise binary phase shift keying-spatially modulated (BPSK-SM) single-input-multiple-output (SIMO) channels that are conceived from robust selective combining of transmit diversity channels. The main disadvantage of GMRC is the fundamental nature of its analysis, which prompts us to investigate the feasibility of a FPGA implementation using a pipeline structure. Such implementation can serve as a practical test-bed for real-life wireless applications. Prior published FPGA implementation applied brute-force technique that led to the use of several square root blocks, which are slow and resource-hungry. In this work, all operations are transformed into addition and multiplication operations only, which are efficient in current FPGA technology due to the availability of such operations at the hardware level. Another important feature of the implementation is pipelining, which further leads to an improved clock cycle and subsequently higher throughput. Using the FPGA implementation on the SIMO channel, the design can be extended to the hardware of spatially modulated hyper-MIMO based 5th generation networks.",,10.1109/COMNETSAT.2019.8844099,"Diversity reception , Fading channels , Field programmable gate arrays , Pipeline processing , Hardware , Receiving antennas , Estimation error "
215,A vision on the artificial intelligence for 6G communication,"TB Ahammed, R Patgiri, S Nayak",ICT Express,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S2405959522000741,"… With the advent of various fields of artificial intelligence, 6G … Artificial intelligence and 6G communication technology will … the scope of artificial intelligence in making a revolutionized 6G …",,,
216,Overview of distributed machine learning techniques for 6G networks,"E Muscinelli, SS Shinde, D Tarchi",Algorithms,,,,,2022,mdpi.com,https://www.mdpi.com/1679718,"… learning approaches for 6G and wireless communications. We … to machine learning, wireless communication, applications of machine learning in wireless communication, upcoming 6G …",,,
217,Collaborative machine learning for energy-efficient edge networks in 6G,"X Huang, K Zhang, F Wu, S Leng",IEEE Network,1558-156X,35,6,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9687500/?casa_token=-vjCTEaSseYAAAAA:W8B8VW3Kz1aiiEJgV4T0GTU1OhNK1SyIeZ5qTLeBmW8NrmOiGQgscwcx3yums9A9oF3ejljbzRFH3g,"To fulfill the diversified requirements of the emerging Internet of Everything (IoE) applications, the future sixth generation (6G) mobile network is envisioned as a heterogeneous, ultra-dense, and highly dynamic intelligent network. Edge intelligence is a vital solution to enable various intelligent services to improve the quality of experience of resource-constrained end users. However, it is very challenging to coordinate the independent but interrelated edge nodes in a decentralized learning manner to improve their strategies. In this article, we propose a decentralized and collaborative machine learning architecture for intelligent edge networks to achieve ubiquitous intelligence in 6G. Considering energy efficiency to be an essential factor in building sustainable edge networks, we design a multi-agent deep reinforcement learning (DRL)-empowered computation offloading and resource allocation scheme to minimize the overall energy consumption while ensuring the latency requirement. Further, to decrease the computing complexity and signaling overhead of the training process, we design a federated DRL scheme. Numerical results demonstrate the effectiveness of the proposed schemes.",,10.1109/MNET.100.2100313,"6G mobile communication , Training data , Energy consumption , System performance , Collaboration , Computer architecture , Collaborative work "
218,Role of 5G and artificial intelligence for research and transformation of english situational teaching in higher studies,"H Yu, S Nazir",Mobile Information Systems,,,,,2021,hindawi.com,https://www.hindawi.com/journals/misy/2021/3773414/,We live in a modern and technological society run by intelligent and human-like machines and systems. This is due to the advancements in the field of artificial intelligence. The …,,,
219,Future OFDM-based communication systems towards 6G and beyond: machine learning approaches,"FH Juwono, R Reine",Green Intelligent Systems and …,,,,,2021,tecnoscientifica.com,https://tecnoscientifica.com/journal/gisa/article/view/34,… OFDM has been proposed as the modulation waveform for 6G and beyond networks due to … use of artificial intelligence techniques such as machine learning (ML) and deep learning (DL…,,,
220,Performance enhancement of FSO communication system using machine learning for 5G/6G and IoT applications,"LJS Kumar, P Krishnan, B Shreya, MS Sudhakar",Optik,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S0030402621019343?casa_token=imTFck7Nc2YAAAAA:2m1NArzu7doqR2juHUExk9l5G9o7raM4FYFSv1f0Wu7uY9xn_8U6e1UmrSvCRMZTr4XnNw105bQ,… the benefits of incorporating machine learning into 6G FSO networks. Machine learning has attracted many considerations in wireless optical communication. Machine learning is very …,,,
221,5G traffic prediction based on deep learning,Z Gao,Computational Intelligence and Neuroscience,,,,,2022,hindawi.com,https://www.hindawi.com/journals/cin/2022/3174530/,"… e 5G network traffic is increasing exponentially and showing a trend of diversity and … performance of the 5G network, this paper makes an accurate prediction of the 5G network and …",,,
222,Research on clothing design based on 5G network and FPGA,T Ma,Microprocessors and Microsystems,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120309029?casa_token=dT6dFU6N7bwAAAAA:1Q45wOvP9L_m6KmmWNA0XPhA0rseJfWnqi5TGJMS5ULl_KNTtZcMvkOH0IV1c_rOqyWR1LZ52jE,"Mobile phone technology is thought to have developed in huge demand in the next few years, it exists. 5 G and related techniques, better data transmission rates, improved productivity, …",,,
223,NVIDIA Aerial GPU Hosted AI-on-5G,"A Kelkar, C Dick",2021 IEEE 4th 5G World Forum (5GWF),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9605055/?casa_token=naBd2ZuZdXUAAAAA:-4s-sOrZR6wVumOCg4ildLB84_oof1GmRd1rmRZqZ7NQFGCyBDpxknTgNG5XXZw9153UBTE95kM_BA,"In this paper we present the NVIDIA hyper-converged platform supporting 5G connectivity and Mobile Edge Computing (MEC). 5G connectivity is realized with our Aerial [1] GPU-based cloud native 5G gNB. We introduce AI-on-5G on a converged accelerator to showcase our innovation in being able to host Aerial vRAN baseband processing, AI/ML training and inference, data analytics and other workloads. In other words, a data center at the edge that is provisioned with 5G connectivity as a service. We describe 3 uses-cases that highlight how existing NVIDIA AI/ML development frameworks, together with Aerial, can be leveraged to bring Industry 4.0 to reality.As an open platform Aerial is positioned to be industry transformational by providing researchers with a platform for next generation wireless and AI research.Aerial seeds the research ecosystem with a first-class out-of-the-box (OOB) experience with a standards compliant 5G NR PHY. Researchers can run the supplied 3GPP compliant test vectors, and perform over-the-air experiments, using standard servers equipped with a GPU-based PCIe card. The PHY code base can be tailored to support research that combines AI/ML with 5G wireless.",,10.1109/5GWF52925.2021.00019,"Wireless communication , Training , Base stations , Technological innovation , 5G mobile communication , Computational modeling , Graphics processing units "
224,Medical intelligent infusion monitoring system based on 5G network and FPGA,"C Bei, Y Liu, J Gu",Microprocessors and microsystems,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120305500?casa_token=4O29CXSr21EAAAAA:L20aDn-JxxbE79O87UW0Mtza8PdznGKJUQ9hLblIJ1zj0flW6nMg-j4CD8YDmZRywviVKxVNooU,"… on FPGA and 5 G technology. The entire system includes software and hardware, and the hardware part is realized using FPGA… types of failures, the FPGA and 5G-based falling droplets …",,,
225,Performance evaluation of offloading LDPC decoding to an FPGA in 5G baseband processing,"F Kaltenberger, H Wang, ...",WSA 2021; 25th International ITG Workshop on Smart Antennas,,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9739186/,"Offloading of computationally expensive physical layer processing such as the forward error correction, is one of the key enablers for a fully virtualized open radio access network (RAN). In this paper we show such an offloading architecture and will demonstrate it using the OpenAirInterface 5G New Radio open source software and the Xilinx T1 telco accelerator card. We will show the feasibility and the potential savings of such an architecture.",,,
226,A High Throughput and Flexible Rate 5G NR LDPC Encoder on a Single GPU,"S Liao, Y Zhan, Z Shi, L Yang",2021 23rd International Conference on Advanced Communication Technology (ICACT),1738-9445,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9370366/?casa_token=oFXpU2xxUi0AAAAA:eWm719zKCP49LsLoRlEAc6RTt_PI9HNq5MUYz0nN3_Bi_TR0vNoUXdnQvJ-iGIMSWKtJWiYqj_Jz8A,"In order to build a high performance low-density parity-check (LDPC) communication link simulation platform, high speed LDPC encoding for information sequence is required. In this paper, a high and flexible throughput LDPC encoding implementation based on a single GPU is proposed. We discuss the parallelism of the LDPC encoding algorithm employs the core parity check bits and single diagonal parity check bits for the fifth generation new ratio. We implement the parallel LDPC encoder on CUDA platform. The experimental results show that our LDPC encoding module achieves a 38-62Gbps throughput for the rate from 1/2 to 8/9 on a single GPU. The results also demonstrate that parallel simulation tasks based on GPUs can achieve a good trade-off between performance and cost.",,10.23919/ICACT51234.2021.9370366,"5G mobile communication , Graphics processing units , Parallel processing , Throughput , Parity check codes , Encoding , Task analysis "
227,Research and implementation of ecpri processing module for fronthaul network on fpga in 5g–nr gnodeb base station,"DT Kiet, TM Hieu, NQ Hung, ...","2020 4th International Conference on Recent Advances in Signal Processing, Telecommunications & Computing (SigTelCom)",,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9199019/,"Fronthaul is associated with a new and different type of Radio Access Network architecture consisting of centralized Baseband Units (BBUs) and Remote Radio Units (RRUs) [2] . In order to meet the fifth-generation wireless (5G) challenges of increased traffic and data flows, a new connection protocol for the fronthaul network called Evolved Common Public Radio Interface (eCPRI) has been published. This paper aims to design a system that supports this interface to transmit data from the BBUs to RRUs in 5G Radio Access Network (RAN). By referencing the specification of eCPRI, we develop a hardware system on FPGA to encapsulate and extract required headers for this protocol. The system is compatible with a 10Gb Ethernet subsystem that provides 10Gb Ethernet MAC and a Physical Interface. On top of the system, we develop drivers and applications to configure and run the system.",,10.1109/SigTelCom49868.2020.9199019,"IP networks , Protocols , 5G mobile communication , Field programmable gate arrays , Signal processing , Radio access networks "
228,FPGA based design and prototyping of efficient 5G QC-LDPC channel decoding,"J Nadal, A Baghdadi",2020 International Workshop on Rapid System Prototyping (RSP),2150-5500,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9244853/?casa_token=AaXFN4UHgwcAAAAA:MDZ3-NPDX0MgdNHk4dkEzJdt6Ec5Niy6JMVfSYY11hrbhCs7ZT28cYt3iPfvnM1Y6nCLONooO5K4Nw,"The Quasi-Cyclic (QC) Low-Density ParityCode (LDPC) is the key error correction code for the 5th Generation (5G) of cellular network technology. Designed to support several frame sizes and code rates, the 5G LDPC code structure allows high parallelism to deliver the high demanding data rate of 10 Gb/s. This impressive performance introduces challenging constraints on the hardware design. Particularly, allowing such high flexibility can introduce processing rate penalties on some configurations. In this context, a novel efficient and flexible hardware architecture for the 5G LDPC decoder is proposed, targeting Field Programmable Gate Array (FPGA) devices and supporting all 5G configurations. The architecture supports frame parallelism to maximize the utilization of the processing units, significantly improving the processing rate. Compared to a recent commercial 5G LDPC decoder, the proposed FPGA prototype achieves a higher processing rate for most configurations while having similar complexity.",,10.1109/RSP51120.2020.9244853,"5G mobile communication , Parallel processing , Throughput , Parity check codes , Hardware , Decoding , Field programmable gate arrays "
229,FPGA-based adaptive space–time compression towards 5G MIMO fronthaul,"P Zhu, Y Yoshida, K Kitayama",Optics Communications,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S0030401819310156?casa_token=3XqCvxflIEoAAAAA:sSTkhgvIKODrT7A0SpvuKC0IRF7nZGLaRGLbaNHBRgoaV0vL1lcWHIR--K0IxA2H9WDhlmD7I7g,"… In this paper, we target 5G MIMO FH, and present field-programmable gate array (FPGA) based … We experimentally demonstrate 8-antenna MIMO FH uplink enabled by the FPGA-based …",,,
230,FPGA implementation of FBMC baseband modular for 5G wireless communication,"R Keerthana, S Rajaram","2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)",,1,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8993290/?casa_token=T9V4O-H90dgAAAAA:eNYyZ-pMmwOBmhS4tHU7RKfgUj0dR4vRqvqtmfbHyFsF_uPsg4prWH1jNnHYd0x5WlnD88S_g0Ohkg,"In recent years research, selection in regard to a potential waveform successor for the fifth generation (5G) telecommunication systems has been focused. On a contrary with OFDM, Filter-bank Multicarrier (FBMC) modulation has been put forward as a next valid waveform to be used in 5G systems because to its finer spectral efficiency and reduced out-of-band emissions. As an outcome, baseband processors to be developed in hardware, as the current state of the work was done, evaluating former design proposals for instance Polyphase Network and Frequency Spreading FBMC Transmitter. Based on the research, architecture to be realized was chosen. The system technologically advanced on comparison with a prevailing software model using MATLAB. The reconfiguration, concise size, high computational power makes FPGA efficient in DSP applications. The developed work involves modelling and execution of FBMC baseband modulator and evaluates key metrics concerning performance, resource utilization and power consumption on FPGA. The proposed architecture is simulated and synthesized using Verilog HDL in Xilinx ISE Design Suite 12.1 and to be implemented on Spartan -3 XC3S50 FPGA board. The recommended structure can be progressed further to surge the complexity of the hardware structure as a result of which a reduced amount of resource could be used to model prototype hardware for the presented FBMC scheme.",,10.1109/ICICICT46008.2019.8993290,
231,FPGA demonstration of adaptive low-latency high-fidelity analog-to-digital compression for beyond-5G wireless-wired conversion,"P Zhu, Y Yoshida, K Kitayama",2019 24th OptoElectronics and Communications Conference (OECC) and 2019 International Conference on Photonics in Switching and Computing (PSC),,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8818151/,Time-domain analog-to-digital compression (ADX) is designed and implemented on FPGA for low-complexity high-fidelity wireless-wired signal conversion. We demonstrate 50ns processing latency and $EVM < 0.6\%\ over > 30dB$ input power range for 4096QAM-modulated OFDM and single-carrier radio signals.,,10.23919/PS.2019.8818151,"Field programmable gate arrays , Wireless communication , OFDM , Bandwidth , Real-time systems , Decoding , Time-domain analysis "
232,Deep learning enabled irs for 6g intelligent transportation systems: A comprehensive study,"W Song, S Rajak, S Dang, R Liu, J Li, ...",IEEE Transactions on Intelligent Transportation Systems,1558-0016,PP,99,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9806434/?casa_token=i0T2Nb39d9oAAAAA:RAOAas3oKyOHr7ddLEJDvN3i-XFGJkhFjUZYeBpoQZbij9W8Z5c4n83NgxID7hx1vcqAIU9FjJz4kg,"Intelligent Transportation Systems (ITS) play an increasingly significant role in our life, where safe and effective vehicular networks supported by sixth-generation (6G) communication technologies are the essence of ITS. Vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications need to be studied to implement ITS in a secure, robust, and efficient manner, allowing massive connectivity in vehicular communications networks. Besides, with the rapid growth of different types of autonomous vehicles, it becomes challenging to facilitate the heterogeneous requirements of ITS. To meet the above needs, intelligent reflecting surfaces (IRS) are introduced to vehicular communications and ITS, containing the reflecting elements that can intelligently configure incident signals from and to vehicles. As a novel vehicular communication paradigm at its infancy, it is key to understand the latest research efforts on applying IRS to 6G ITS as well as the fundamental differences with other existing alternatives and the new challenges brought by implementing IRS in 6G ITS. In this paper, we provide a big picture of deep learning enabled IRS for 6G ITS and appraise most of the important literature in this field. By appraising and summarizing the existing literature, we also point out the challenges and worthwhile research directions related to IRS aided 6G ITS.",,10.1109/TITS.2022.3184314,"6G mobile communication , Channel estimation , MIMO communication , Optimization , Wireless networks , Deep learning , Array signal processing "
233,"Balancing QoS and Security in the Edge: Existing Practices, Challenges, and 6G Opportunities with Machine Learning","ZM Fadlullah, B Mao, N Kato",IEEE Communications Surveys & Tutorials,2373-745X,24,4,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9831429/?casa_token=ZKtPekxXjVUAAAAA:FigFKe-gdeoqq9q9Nch-zyL2R_x6hoR7OkfblJMd9-NMDZ6fh_Yei1HgChd8TiNIHc--cgNQcCVQEQ,"While the emerging 6G networks are anticipated to meet the high-end service quality demands of the mobile edge users in terms of data rate and delay satisfaction, new attack surfaces and zero-day attacks continue to pose significant threats to their successful realization and rollouts. Traditionally, most service provisioning techniques considered security metrics separately from the Quality of Service (QoS) and Quality of Expectation (QoE) parameters. The QoS/QoE parameters include data throughput, experienced delay, tolerable latency, jitter, resource utilization rate, spectral efficiency, energy efficiency, fairness, and other emerging key performance indicators (KPIs). Also, there are various security attributes, such as encryption key strength, authentication strength, network anomaly score, privacy metric, and so on. Typically, the resource allocation optimization techniques to maximize the security aspects to protect the communication of mobile users or user equipment (UEs) have an adverse effect on the service quality. Therefore, a key research gap exists in balancing service quality and security levels in communication networks that has been either overlooked or identified in a rather scattered manner by researchers in the recent decade. Thus, a comprehensive survey of the state-of-the-art to clearly address this research gap and outline the possible solutions is yet to appear in the existing literature. In this paper, we address this by surveying the existing practices, challenges, and opportunities in the emerging 6G (i.e., beyond 5G) networks, where various AI (Artificial Intelligence)-based techniques such as deep learning meet the classical optimization techniques, to balance the service performance and security levels. Several networking topologies with relevant use-cases are included in the survey to discuss the existing and emerging trends of isolated as well as joint treatment of service and security levels. Lessons learned from each use-case are provided to demonstrate a clear road map for the interested readers and researchers in emerging networks to construct a natively combined service and security ecosystem, specifically in the network edge.",,10.1109/COMST.2022.3191697,"Quality of service , Security , 6G mobile communication , Optimization , 5G mobile communication , Measurement , Throughput "
234,A deep learning assisted software defined security architecture for 6G wireless networks: IIoT perspective,"MA Rahman, MS Hossain",IEEE Wireless Communications,1558-0687,29,2,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9801721/?casa_token=PsDJWUGSCywAAAAA:LkQ8tUa8SNu9MBYRVllUnTp98rTpQmhX5scZNB22eHcK7rM6XGdCbn3CVwOMC5KaIcwy3-S-obdgrg,"The 6G wireless network is expected to drive cyber-physical systems (CPS) from merely connected things to securely connected intelligence. While 6G will offer real-time communication between cyber and physical entities, due to convergence of operational technology (OT) and information technology (IT) networks, security, and trustworthiness of the massive amount of data shared between cyber and physical entities will remain of great concern. Attackers having AI capability will be able to mount massive numbers of automated and novel attacks on the future 6G network. Human security specialists teaming with an AI-powered adaptive defense mechanism will be needed to counter emerging AI-based attacks on the massively connected CPS through 6G wireless networks. 6G networks are expected to add industrial immunity to IT, OT, and IIoT networks with the help of AI. 6G is expected to offer deep learning (DL) assisted security function virtualization (SFV) to support software defined security (SDS) architecture for dynamic defense mechanisms, intelligently monitor network traffic anomalies at different network endpoints and segments, and offer increased visibility across attack surfaces. In this article, we study the security challenges in 6G networks posed by the recent convergence of OT and IT networks and propose distributed DL-assisted SDS for 6G vertical that will autonomously detect, localize, and isolate security threats via SFV. Finally, we present future directions and the challenges ahead.",,10.1109/MWC.006.2100438,"6G mobile communication , Deep learning , Wireless networks , Computer architecture , Software , Real-time systems , Security , Cyber-physical systems "
235,GPF+: A Novel Ultrafast GPU-Based Proportional Fair Scheduler for 5G NR,"Y Huang, S Li, YT Hou, W Lou",IEEE/ACM Transactions on Networking,1558-2566,30,2,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9582828/?casa_token=oEGvs3OEjhkAAAAA:PL5FuGa9fl8u0wvpDEViLn4RYlaHLqGa_w9T_Brj_uR6eumeMb4OgHGDKvqoP6mfat4SS0hF3ejL8g,"5G NR is designed to operate over a broad range of frequency bands and support new applications with ultra-low latency requirements. To support its extremely diverse operating conditions, multiple OFDM numerologies have been defined in the 5G standards. Under these numerologies, it is necessary to perform scheduling with a time resolution of  $\sim 100 \mathrm {\mu s}$ . This requirement poses a new challenge beyond existing LTE and cannot be satisfied by any existing LTE schedulers. In this paper, we present the design of GPF+, which is a GPU-based proportional fair (PF) scheduler with timing performance under  $100 \mathrm {\mu s}$ . GPF+ is an improvement over our GPF in Huang et al. (2018). The key ideas include decomposing the original scheduling problem into a large number of small and independent sub-problems and selecting a subset of sub-problems from the most promising search space to fit into a GPU. By implementing GPF+ on an off-the-shelf NVIDIA Tesla V100 GPU, we show that GPF+ is able to achieve near-optimal PF performance with timing performance under  $100 \mathrm {\mu s}$ . GPF+ represents the fastest GPU-based PF scheduler that can meet the new real-time requirement in 5G NR.",,10.1109/TNET.2021.3118005,"Graphics processing units , 5G mobile communication , Scheduling , Long Term Evolution , Processor scheduling , Optimal scheduling , OFDM "
236,A GPU Hyperconverged Platform for 5G vRAN and Multi-Access Edge Computing,"A Kelkar, C Dick",2021 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),0840-7789,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9569133/?casa_token=0PN_O8gg3ZQAAAAA:BKXBM7_DYSJfQREo-neOD1RLLB8LhlXGyxWyDVtIYXf04eSQ037EVB8Cx6aoaZQ0jIhSikl__afdDQ,"In this paper we present the NVIDIA hyper-converged platform supporting 5G connectivity and Mobile Edge Computing (MEC). 5G connectivity is realized with our Aerial [1] GPU-based cloud native 5G gNB. We introduce AI-on-5G on a converged accelerator to showcase our innovation in being able to host Aerial vRAN baseband processing, AI/ML training and inference, data analytics and other workloads. In other words, a data center at the edge that is provisioned with 5G connectivity as a service. We describe 3 uses-cases that highlight how existing NVIDIA AI/ML development frameworks, together with Aerial, can be leveraged to bring Industry 4.0 to reality. As an open platform Aerial is positioned to be industry transformational by providing researchers with a platform for next generation wireless and AI research. Aerial seeds the research ecosystem with a first-class out-of-the-box (OOB) experience with a standards compliant 5G NR PHY. Researchers can run the supplied 3GPP compliant test vectors, and perform over-the-air experiments, using standard servers equipped with a GPU-based PCIe card. The PHY code base can be tailored to support research that combines AI/ML with 5G wireless.",,10.1109/CCECE53047.2021.9569133,"Wireless communication , Training , Base stations , Technological innovation , 5G mobile communication , Computational modeling , Graphics processing units "
237,Channel estimation using deep learning on an fpga for 5g millimeter-wave communication systems,"PK Chundi, X Wang, M Seok",IEEE Transactions on Circuits and Systems I: Regular Papers,1558-0806,69,2,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9576112/?casa_token=RNpdgTXSaLMAAAAA:9J4q1Q1c91ae6L2xMnWe1OT0pqiV2dKfwSAakhhDnGLzsj1hpe5HhtL2s6eaKbZ5_Afn8rZ8CgKMJQ,"5G millimeter-wave (mmWave) communication systems enable exciting new applications by significantly reducing the latency and increasing the data rate. However, this comes at a large computational cost, which results in long latency and large energy consumption. In this work, we aim to address this challenge in the problem of channel estimation of such systems through a set of algorithm-hardware co-optimizations. First of all, we employed a model-based neural network to improve the rate of convergence. We also optimized the neural network and achieved improved loss while using approximately the same number of operations. Furthermore, we were able to reduce the computational complexity through the use of sparsity inherent in mmWave channels. The proposed neural network for the channel estimation scales the computational complexity by more than two orders. Based on these innovations, we implemented a channel estimation subsystem on Zynq 7020 FPGA. The subsystem obtains an improvement in latency of up to ~10X and an improvement in energy consumption of up to ~300X over CPU and GPU based systems.",,10.1109/TCSI.2021.3117886,"Channel estimation , Neural networks , 5G mobile communication , Hardware , Field programmable gate arrays , Transmitting antennas , Millimeter wave communication "
238,News development in the 5G network era based on machine learning and FPGA,W Wang,Microprocessors and Microsystems,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120305482?casa_token=5YV4pgDcNZ4AAAAA:xo_xpSudCMsOt0E8YiNC_kWsKklIt9gL_TrmUNG8Jy38PjIr4FaJZKjcrgHqHOrnXIv8KwM48Yw,… of the FPGA of things to come of the 5G network arranging. Since it must be a potential for FPGA … effectiveness and can be utilized to assemble a necessary aspect of the 5G foundation. …,,,
239,FPGA-based Design and Optimization of a 5G-NR DU Receiver,"FDL Coutinho, HS Silva, ...",2021 Telecoms Conference (ConfTELE),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9435579/?casa_token=_dKgAQwH0joAAAAA:mVHP7gr-i0TPrixhtsaj4t1YVLCx-O6lGaarj0Rt0dA8Al7Vf7IJ42UgdHGu4-LnlgDp85WZfDZERQ,"In this paper, a Fifth-Generation New Radio (5G-NR) Distributed Unit (DU) receiver case study is carried out to evaluate the trade-offs between different design parameters. The 5G-NR DU receiver is modelled using a fast implementation flow, from the behavioral model to the Field-Programmable Gate Array (FPGA) validation. The goal of this paper is to optimize the area, power, and DU receiver overall performance of the Register-Transfer Level (RTL) implementations from a high-level model by varying the model input data type, i.e., the number of quantized bits at the input of the processing chain. Matlab and Simulink are used to implement the 5G-NR DU behavioral model, and its synthesis is performed by the Hardware Description Language (HDL) Coder automated tool. The 5G-NR DU receiver is implemented in a ZCU102 evaluation kit, containing an XCZU9EG-FFVB1156-2-E device. The trade-offs are evaluated by analyzing the Error Vector Magnitude (EVM), the coarse symbol timing detection, the resource utilization, the power, throughput, the maximum operating frequency, and the latency for different modulation schemes. The results showed a direct dependence of the input data type on these design parameters, while the modulation scheme is almost agnostic, providing reliable information for an optimized DU implementation.",,10.1109/ConfTELE50222.2021.9435579,"Frequency modulation , Software packages , Receivers , Throughput , New Radio , Data models , Timing "
240,Hierarchical Scheduling with FPGA-based Accelerator for Flexible 5G Mobile Networks,"Y Arikawa, T Sakamoto, ...",2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring),1090-3038,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9128593/?casa_token=jyWLRgdqwooAAAAA:NgHv3Rl1Ymb2bfxcZulC9h2aUBi_rreZLTMSXdNUdq7VjPWpX3t2bXALoLcADknN-_obWJDqeSYCTA,"Towards the deployment of flexible 5G mobile networks, this paper presents a hierarchical coordinated radio-resource scheduling method along with a scheme for its practical hardware implementation in a field-programmable gate array (FPGA). The scheduler in the 5G era will have to quickly perform coordinated scheduling for a huge number of transmission antennas and support flexibility for advanced scheduling algorithms. To meet these requirements, we designed an FPGA-based scheduler that hierarchically controls the radio transmissions of all transmission antennas beyond a central unit. Experimental measurements reveal that the FPGA-based hierarchical coordinated scheduler shows comparable processing speed on the radio-frame time scale. Moreover, in numerical simulations, the overall system throughput with the hierarchical coordinated scheduler is 1.25 times higher than that obtained by the conventional method when the number of transmission antennas and mobile terminals are 32 and 256, respectively. Our proposed scheduling method will enable the deployment of flexible 5G mobile networks.",,10.1109/VTC2020-Spring48590.2020.9128593,"Processor scheduling , Throughput , Field programmable gate arrays , Scheduling , Interference , 5G mobile communication , Copper "
241,"FPGA demonstration of adaptive spacetime compression towards high-fidelity, low-latency 5G fronthaul","P Zhu, Y Yoshida, K Kitayama",45th European Conference on Optical Communication (ECOC 2019),,,,,2019,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9125473/?casa_token=qLZCQwovoVkAAAAA:CsfYVaCgceXpZHIF0VJ8dl_7neuXxpj4DHlqgHf8Zf4UA80PyQ7xGs0kGLqtKTTxbmc397QHGAgrIg,"We experimentally demonstrate 8-antenna MIMO fronthaul uplink enabled by the first FPGA-based adaptive space-time compressor. 48Gb/s CPRI-equivalent rate encapsulating 1024QAM 5G NR-grade signals is transported using as low as 2.5GBd optical PAM4, achieving <1.25% EVM and 50ns latency for compression.",,10.1049/cp.2019.0745,
242,A survey of machine learning algorithms for 6g wireless networks,"A Patil, S Iyer, RJ Pandya",arXiv preprint arXiv:2203.08429,,,,,2022,arxiv.org,https://arxiv.org/abs/2203.08429,"… Replacing traditional algorithms with deep learning AI techniques have … 6G communication platforms. In this chapter, we review/survey the ML techniques which are applicable to the 6G …",,,
243,AI-based computer vision using deep learning in 6G wireless networks,"MM Kamruzzaman, O Alruwaili",Computers and Electrical Engineering,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S0045790622004694?casa_token=J0o3I-t_uIwAAAAA:leirtTfyYGejlmoLnFD74c35bENe62wSJtFfaRghomrLdOvjRAUl81li4CPN3b3oZMXNhHTmuGs,"… , artificial intelligence (AI) has recently been used. Therefore, in this paper, the 6 G wireless network is used along with Deep Learning to … This research uses deep learning – efficiency …",,,
244,A sustainable deep learning framework for fault detection in 6G Industry 4.0 heterogeneous data environments,"T Mezair, Y Djenouri, A Belhadi, G Srivastava, ...",Computer …,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S014036642200055X,… the hyper-parameters of the different deep learning models should be carefully analyzed and tuned. The common parameters of the deep learning models used in this work are the …,,,
245,FPGA-accelerated SmartNIC for supporting 5G virtualized Radio Access Network,"JC Borromeo, K Kondepu, N Andriolli, L Valcarenghi",Computer Networks,,,,,2022,Elsevier,https://www.sciencedirect.com/science/article/pii/S1389128622001189,… The approach proposed in this paper accelerates the 5G gNB stack Low-PHY functions in an FPGA-based SmartNIC. This work focuses on a dual-split scenario where Option 8 is …,,,
246,GPU-accelerated partially linear multiuser detection for 5G and beyond URLLC systems,"M Mehlhose, G Marcus, D Schäufele, DA Awan, ...",IEEE Access,2169-3536,10,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9810242/,"We have implemented a recently proposed partially linear multiuser detection algorithm in reproducing kernel Hilbert spaces (RKHSs) on a GPU-accelerated platform. Our proof of concept combines the robustness of linear detection and non-linear detection for the non-orthogonal multiple access (NOMA) based massive connectivity scenario. Mastering the computation of the vast number of inner products (which involve kernel evaluations) is a challenge in ultra-low latency (ULL) applications due to the sub-millisecond latency requirement. To address the issue, we propose a massively parallel implementation of the detection of user data in a received orthogonal frequency-division multiplexing (OFDM) radio frame. The result is a GPU-accelerated real-time OFDM receiver that enables detection latency of less than one millisecond that complies with the requirements of 5th generation (5G) and beyond ultra-reliable and low latency communications (URLLC) systems. Moreover, the parallelization and acceleration techniques explored and demonstrated in this study can be extended to many signal processing algorithms in Hilbert spaces, such as projection onto convex sets (POCS) and adaptive projected subgradient method (APSM) based algorithms. Results and comparisons with the state-of-the-art confirm the effectiveness of our approach.",,10.1109/ACCESS.2022.3187040,"Signal processing algorithms , Kernel , Nonlinear filters , Multiuser detection , Maximum likelihood detection , Hilbert space , OFDM "
247,FPGA Design of an efficient EEG signal transmission through 5G wireless network using optimized pilot based channel estimation: a telemedicine application,"KBS Kumar, BR Sujatha",Wireless Personal Communications,,,,,2022,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11277-021-09305-2&casa_token=SOwvm1xDORcAAAAA:mAiuBkmiQ7T0EpYTIMn7XmPc62xIEMYV1HATbfaiSJXQx4MKGM3oxpLq4SLfpnp_YAELB0WhSR2esMs57g,"… In this work, an FPGA design for an enhanced squirrel search algorithm optimized pilot based channel estimation method is proposed to transmit the biomedical EEG signal through 5G …",,,
248,Modern accounting data analysis platform based on 5G network and FPGA,J Lin,Microprocessors and Microsystems,,,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120305457?casa_token=lQzvQuBEy5MAAAAA:OGEhVxuLFNx60o1KCAAn5V25H2z5lE-y9BIN49_D3cnqaeAJPFktwO3_gq8g8NTU_Q0NdynKtrs,"… to 5G, key innovation patterns and business drivers that shape the way to 5G applications … lastly a proposed development model of 5G investigation and organizations changing from …",,,
249,Fifth Generation (5G) New Radio (NR) Channel Codes Contenders Based on Field-Programmable Gate Arrays (FPGA): A Review Paper,"AA Hamad, MK Ibrahim, AA Al-hayder, ...",Journal of University of …,,,,,2019,iasj.net,https://www.iasj.net/iasj/download/9d35d729771fae3a,… This paper presents a survey on the current literatures that deals with FPGA-based decoder … FPGA it can take hours [10]. This is mainly due to the ability of parallel processing that FPGA …,,,
250,Unmasking Concealed 5G Privacy Identity with Machine Learning and GPU in 12 mins,VH Tea,,,,,,2020,techrxiv.org,https://www.techrxiv.org/articles/preprint/Unmasking_Concealed_5G_Privacy_Identity_with_Machine_Learning_and_GPU_in_12_mins/13187636,"… concealed 5G … (GPU) that is able to unmask a concealed 5G identity in ~12 minutes with an untrained neural-network, or ~0.015 milliseconds with a pre-trained neural-network. The 5G …",,,
251,Research on automatic evaluation method of Mandarin Chinese pronunciation based on 5G network and FPGA,"Z Wang, Q Wu",Microprocessors and Microsystems,,,,,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120306840?casa_token=IK61Q-vzmsEAAAAA:lsKN-LaXapiKeFW4kBtET4niQ2M3Q5vELMFymjSVyh_puUvBnBHM4BCpCx8wWoqilKnA9dlkuWY,… It is the native FPGA and 5G regarding learner pronunciation … and measuring capabilities from 4G to 5G is not a simple step up. … method is using 5G and FPGA in our proposed method. …,,,
252,FPGA implementation of new LM-SPIHT colored image compression with reduced complexity and low memory requirement compatible for 5G,"YM Tabra, B Sabbar",International …,,,,,2019,download.garuda.kemdikbud.go.id,http://download.garuda.kemdikbud.go.id/article.php?article=1493229&val=152&title=FPGA%20implementation%20of%20new%20LM-SPIHT%20colored%20image%20compression%20with%20reduced%20complexity%20and%20low%20memory%20requirement%20compatible%20for%205G,"The revolution in 5G mobile systems require changes to how image is handled. These changes are represented by the required processing time, the amount of space for uploading and …",,,
253,Build and deploy GPU-accelerated 5G virtual Radio Access Networks (vRAN),N Aerial,,,,,,2022,Accessed: Jun,,,,,
254,GPF: A GPU-Based Design to Achieve Scheduling for 5G NR,"Y Huang, S Li, YT Hou, W Lou",Proc. MobiCom 2018,,,,,2019,,,,,,
255,A Cognitive Radio Spectrum Sensing Implementation Based on Deep Learning and Real Signals,"M Saber, A Chehri, AE Rharras, R Saadane, ...",Innovations in Smart …,,,,,2021,Springer,https://link.springer.com/chapter/10.1007/978-3-030-66840-2_70,"… In this paper, we used a deep learning model for spectrum sensing in cognitive radio … Therefore, a simplified spectrum sensing implementation has been proposed based on real signals …",,,
256,6G and Artificial Intelligence Technologies for Dementia Care: Literature Review and Practical Analysis,"Z Su, BL Bentley, D McDonnell, J Ahmad, J He, ...",Journal of Medical …,,,,,2022,jmir.org,https://www.jmir.org/2022/4/e30503/,"… technologies such as 5G, 6G is considerably … between 6G and AI will be that of symbiotic [65]—6G will lay the groundwork needed for the ecosystem to exist, whereas AI will make the 6G…",,,
257,Real-Time GPU-Accelerated Machine Learning Based Multiuser Detection for 5G and Beyond,"M Mehlhose, D Schäufele, DA Awan, G Marcus, ...",arXiv preprint arXiv …,,,,,2022,arxiv.org,https://arxiv.org/abs/2201.05024,"… Therefore, in order to realize our goal of real-time GPUaccelerated multiuser detection for 5G … schedule operations across the memory hierarchy of the graphics processing unit (GPU). …",,,
258,Slow-Envelope Shaping Function FPGA Implementation for 5G NR Envelope Tracking PA,"W Li, N Bartzoudis, JR Fernández, ...",2022 International Workshop on Integrated Nonlinear Microwave and Millimetre-Wave Circuits (INMMiC),2689-548X,,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9762194/,"This paper focuses on the FPGA implementation of a slew-rate reduction (SR) shaping function for envelope tracking (ET) power amplifiers (PAs). The SR envelope has been proved effective to trade-off power efficiency and linearity in ET PA systems where the envelope tracking modulator (ETM) is bandwidth limited. However, the implementation issues need to be addressed when targeting high clock rates to cope with current 5G new radio wide-band signals. This paper shows the FPGA implementation of the SR envelope generation. We explore the use of high-level synthesis (HLS) for the SR envelope generation to evaluate the performance and resource usage of the hardware architecture. The HLS design is also compared with a hand-written hardware description language (HDL) version. An in-depth analysis shows strengths and limitations of the HLS design to meet the timing constraints when considering a throughput of 614.4 MSa/s.",,10.1109/INMMiC54248.2022.9762194,"Target tracking , 5G mobile communication , Prototypes , Power amplifiers , Modulation , Throughput , Reliability engineering "
259,A GPU accelerated framework for monitoring LTE/5G interference to DVB-T systems,"F Mangiatordi, E Pallotti",2021 AEIT International Annual Conference (AEIT),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9626937/?casa_token=xBTH5bZOUToAAAAA:jPBwpKl3Vnr3knvbCThdJM5eyO5Us-lfK0ktvTLHH3lJyMj72fbRau_KaaH1nDuTp71Ta90Xt0gk-A,"This work presents a new computation system to evaluate the interference maps between the radio mobile signal and the DVB-T signal based on a multiprocessor computing architecture, such as GPU systems (Graphics Processing Units) designed for high-performance parallel computing. The goal is to overcome the time complexity of current approaches based on sequential implementations on multicores CPUs that require several hours to generate and update the interference maps of the Italian territory. The new system’s design considers new parallel computing methodologies to consider rapid changes in the GPU hardware and uses a high-performance distributed SQL engine on RAPIDs to create an efficient and scalable framework for processing a massive volume of simulation data radio. Carrying out test on the radio-electric data of the Italian provinces with the most significant territorial extension shows compression of at least a factor of 30 in the execution times, which for the entire Italian territory fall below 20 minutes.",,10.23919/AEIT53387.2021.9626937,"Solid modeling , Parallel programming , Memory management , Graphics processing units , Interference , Parallel processing , Servers "
260,FPGA-based ordered statistic decoding architecture for B5G/6G URLLC IIoT networks,"C Kim, D Rim, J Choe, D Kam, G Park, ...",2021 IEEE Asian Solid-State Circuits Conference (A-SSCC),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9634714/?casa_token=T43wgz9sjY0AAAAA:TEjfKbmDrYuulMEUS4_3i9_BcU4NDdZzIAUJ5_osREkTcDEWVHieWrVBSr4DOfk0RYNbzddtq6Bh4Q,"The ordered statistic decoding (OSD) approach for short-length BCH codes has been continuously considered as one of the promising error-correction codes by achieving a block error rate (BLER) of less than $10^{-6}$, which is attractive to the ultra-reliable and low-latency communication (URLLC) for industrial IoT (IIOT) solutions [1], [2]. However, it is hard to directly realize the conventional OSD algorithm because of the compute-intensive Gaussian elimination and iterative reprocessing steps. Based on the recent segmentation discarding decoding (SDD) approach [3], in this work, we newly present an ultralow-latency OSD architecture reducing the decoding latency by 12 times, which is implemented at an FPGA-based verification platform.",,10.1109/A-SSCC53895.2021.9634714,"Error analysis , Conferences , Computer architecture , Ultra reliable low latency communication , Iterative algorithms , Solid state circuits , Error correction codes "
261,FPGA Implementation of a Wideband Multi-Gb/s 5G BF-OFDM Transceiver,"JB Doré, M Laugeois, N Cassiau, ...",2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),2475-6490,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9482424/?casa_token=JTl7XB4WOzYAAAAA:jXLXLwUTk1JZ7WgvEhDrOdBnNrMP3sLv9H5PjsQKkSpusQOgEsaRJcP8T5PIOwKUFwl_QCKP64eOhw,"This paper describes a Field Programmable Gate Array (FPGA) implementation of a multi-Gb/s Block Filtered (BF) OFDM transceiver, fully 5G NR compatible. The main obstacles for such a work are (i) the support of multiple configurations and parameters, (ii) the high bandwidth w.r.t the board clock frequency and (iii) the intrinsic complexity of BF-OFDM. We prove that despite these barriers an hardware implementation of this waveform is possible, even with a bandwidth up to 400 MHz. We based our developments on the following pillars: smart layout of the basic modules, parallelization of dedicated functions design and ad hoc architecture. Measurements and complexity analysis demonstrate the high flexibility of BF-OFDM.",,10.1109/EuCNC/6GSummit51104.2021.9482424,"5G mobile communication , Transmitters , OFDM , Filter banks , Transceivers , Hardware , Complexity theory "
262,Optimizing 5G VPN+ Transport Networks with Vector Packet Processing and FPGA Cryptographic Offloading,"B Dzogovic, B Santos, B Feng, VT Do, N Jacot, ...",Mobile Web and …,,,,,2021,Springer,https://link.springer.com/chapter/10.1007/978-3-030-83164-6_7,… 5G infrastructure for IoT in healthcare and address a performance issue that arises as a result to the encryption in the transport network between the 5G … an additional FPGA hardware to …,,,
263,FPGA Implementation of a 4G/5G Multimode DU Downlink Transmission Chain,"JD Domingues, HS Silva, ...",2021 Telecoms Conference (ConfTELE),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9435553/?casa_token=uRVW9O3snCkAAAAA:JSmedIzEVONDkZh_vjXW6f-OH09Y3D8S0SJJGFpV7NNqbPvGt4fLaRxUVHuIeqbTLEBkd5Ldq1g-7A,"This paper presents a multimode Distributed Unit (DU) transmission chain implemented in Field-Programmable Gate Array (FPGA). A careful analysis is taken towards the physical layer differences between Fourth Generation Long-Term Evolution (4G-LTE) and the Fifth Generation New Radio (5G-NR) of mobile networks to determine the fundamental changes in each generation's DU architecture. The DU supports both 4G-LTE and 5G-NR to be modulated parallely in real-time, and is developed in this work by using high level tools and Register-Transfer Level (RTL) code, from Matlab and Simulink to VHDL, to obtain the final implementation of a single DU. On the 4G side, the DU supports the maximum LTE channel bandwidth of 20 MHz, and the 5G counterpart supports a channel bandwidth from 5 MHz to 100 MHz. The results are validated in Matlab, Simulink, RTL and in real-time, culminating in a final FPGA implementation, with an EVM of 0.24% for 4G and 1.60% for 5G.",,10.1109/ConfTELE50222.2021.9435553,"VHDL , Software packages , Bandwidth , New Radio , Physical layer , Downlink , Real-time systems "
264,Centralized Single FPGA Real Time Zero Forcing Massive MIMO 5G Basestation Hardware and Gateware,"A Benzin, D Osterland, M Dill, ...",2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),1948-3244,,,,2020,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9154248/?casa_token=EvImzLNcKCMAAAAA:PEYiLHZpykB_KQM1ZE68EuSAXXGfc8Gqt0GYX_r03mnSG0Q9RZ1tjsYGA2K-77hYjbveWQempTrS_w,In the following a massive MIMO 5G <; 6 GHz base station implementation is presented which is capable of realtime zero forcing precoding on a single central signal processing (CSP) FPGA. The built prototype is capable of simultaneously driving M = 196 separate RF ports all delivering samples to the CSP FPGA. Each RF chain's ADCs and DACs are running at 40 MSPS at full roll-out. The power consumption of the remote radio head is 1.56 W per RF port when running at a sample rate of 15.36 MSPS. The system allows for hardware-in-the-loop operation and real-time baseband signal processing with a round trip delay of 278 μs when processing 64 antennas and 8 simultaneous user streams for an 5GNR-like OFDMA system with 1024 sub-carriers and 50 resource blocks (600 used subcarriers) with a sample frequency of 15.36 MHz and a central signal processing clock of 184.32 MHz. The reciprocity calibration system runs completely internal to the system and doesn't radiate signals for the calibration procedure. Furthermore the central single-FPGA signal processing architecture allows for simplified implementation of algorithms and maintenance of the system.,,10.1109/SPAWC48557.2020.9154248,"Field programmable gate arrays , Radio frequency , Calibration , Clocks , Signal processing , Real-time systems , Hardware "
265,Real‐time multi‐GPU‐based 8KVR stitching and streaming on 5G MEC/Cloud environments,"HK Lee, GM Um, SY Lim, J Seo, M Gwak",ETRI Journal,,,,,2022,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.4218/etrij.2021-0210,"… on a local machine and the 5G multi-access edge computing (… up to 8 K 30 fps over 5G MEC/cloud networks. The proposed … Following the introduction, we describe the multi-GPU-based …",,,
266,FPGA-based radio-resource scheduler for 5G mobile in NFV environments,"Y Arikawa, T Sakamoto, S Shigematsu",IEICE Communications …,,,,,2019,jstage.jst.go.jp,https://www.jstage.jst.go.jp/article/comex/8/7/8_2019XBL0048/_article/-char/ja/,"… For implementation of the coordinated radio-resource scheduler with an FPGAbased HWA in 5G mobile systems, this paper discusses two types of functional interfaces and three types …",,,
267,Development of an FPGA-based High-Speed Wireless Communication System in the 60GHz frequency band for CERN facilities and 5G deployment,M Jaoua,,,,,,2018,diva-portal.org,https://www.diva-portal.org/smash/record.jsf?pid=diva2:1203832,… [7] to program and communicate with the FPGA. The FPGA is programmed to generate the … which itself gets demodulated at the FPGA to find the exact sent bits as shown in figure 3.1. …,,,
268,FPGA Implementation of 5G NR Primary and Secondary Synchronization,"AR Kumar, KL Kishore",CMC …,,,,,2022,TECH SCIENCE PRESS 871 …,,,,,
269,FPGA Acceleration of 3GPP Channel Model Emulator for 5G New Radio,"NA Shah, MT Lazarescu, R Quasso, S Scarpina, ...",IEEE Access,2169-3536,10,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9944632/,"The channel model is by far the most computing intensive part of the link level simulations of multiple-input and multiple-output (MIMO) fifth-generation new radio (5GNR) communication systems. Simulation effort further increases when using more realistic geometry-based channel models, such as the three-dimensional spatial channel model (3DSCM). Channel emulation is used for functional and performance verification of such models in the network planning phase. These models use multiple finite impulse response (FIR) filters and have a very high degree of parallelism which can be exploited for accelerated execution on Field Programmable Gate Array (FPGA) and Graphics Processing Unit (GPU) platforms. This paper proposes an efficient re-configurable implementation of the 3rd generation partnership project (3GPP) 3DSCM on FPGAs using a design flow based on high-level synthesis (HLS). It studies the effect of various HLS optimization techniques on the total latency and hardware resource utilization on Xilinx Alveo U280 and Intel Arria 10GX 1150 high-performance FPGAs, using in both cases the commercial HLS tools of the producer. The channel model accuracy is preserved using double precision floating point arithmetic. This work analyzes in detail the effort to target the FPGA platforms using HLS tools, both in terms of common parallelization effort (shared by both FPGAs), and in terms of platform-specific effort, different for Xilinx and Intel FPGAs. Compared to the baseline general-purpose central processing unit (CPU) implementation, the achieved speedups are 65X and 95X using the Xilinx UltraScale+ and Intel Arria FPGA platform respectively, when using a Double Data Rate (DDR) memory interface. The FPGA-based designs also achieved ~3X better performance compared to a similar technology node NVIDIA GeForce GTX 1070 GPU, while consuming ~4X less energy. The FPGA implementation speedup improves up to 173X over the CPU baseline when using the Xilinx UltraRAM (URAM) and High-Bandwidth Memory (HBM) resources, also achieving 6X lower latency and 12X lower energy consumption than the GPU implementation.",,10.1109/ACCESS.2022.3221124,"Channel models , Field programmable gate arrays , Computational modeling , 3GPP , Graphics processing units , Three-dimensional displays , Optimization , 5G mobile communication , Hardware acceleration "
270,Architectural Implementation of AES based 5G Security Protocol on FPGA,"U Rahim, MF Siddiqui, MA Javed, ...",2022 32nd International Telecommunication Networks and Applications Conference (ITNAC),2474-1531,,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9998367/?casa_token=8ZrY7vtAT-sAAAAA:0JqkBqnvGRStpPedpLvuGP77QdVn_A7Gmub464XmJvNaNH0WutB2suX46OJps3Kfn_GqY94jsZaa0A,"Confidentiality and integrity security are the key challenges in future 5G networks. To encounter these challenges, various signature and key agreement protocols are being implemented in 5G systems to secure high-speed mobile-to-mobile communication. Many security ciphers such as SNOW 3G, Advanced Encryption Standard (AES), and ZUC are used for 5G security. Among these protocols, the AES algorithm has been shown to achieve higher hardware efficiency and throughput in the literature. In this paper, we implement the AES algorithm on Field Programmable Gate Array (FPGA) and real-time performance factors of the AES algorithm were exploited to best fit the needs and requirements of 5G. In addition, several modifications such as partial pipelining and deep pipelining (partial pipelining with sub-module pipelining) are implemented on Virtex 6 FPGA ML60S board to improve the throughput of the proposed design.",,10.1109/ITNAC55475.2022.9998367,"Protocols , 5G mobile communication , Snow , Simulation , Throughput , Real-time systems , Security "
271,Spark Distributed Real-Time Data and GPU Parallel Computing Based on 5G Virtual Reality,"Y Chang, D Chang, L Li, Z Qiao",IEEE Consumer Electronics Magazine,2162-2256,PP,99,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9735325/?casa_token=_A7YNDtptC8AAAAA:7kVA2Hgt8wnD-H3-u5oGVtmpXRixAK5Qr5cBeO4nPsiRypVM0awcsPnEgJBqq3vCrqLNwD4zmqEhoA,"5G virtual reality has attracted many manufacturers and users with its unique immersion, interactivity and imagination characteristics, which has become the focus of new markets. The purpose of this article is to use Spark distributed real-time data system and GPU parallel computing to quickly process and analyze data. This article mainly designs a general-purpose real-time data analysis and processing system based on Spark, which mainly includes new ETL and real-time processing engine modules, and is committed to achieving higher real-time performance than traditional Hadoop. And realize fast calculation. At the same time there is universality and stability. Includes real-time flow calculations. Fast batch processing and machine learning The various types of data computers are included in this article by preparing the cutting device and adjusting the cutting output. The device is ready to effectively terminate the CUDA environment. The cudamalloc function is used to allocate a linear space of bytes to the device, and then transfer the data from the host to the device to determine the number of GPU blocks and threads. GPU parallel computing can increase the data processing speed by 27%, while the secondary programming algorithm can reduce the optimization time of the cup by 12%.",,10.1109/MCE.2022.3159349,"Peer-to-peer computing , Real-time systems , Graphics processing units , Quadratic programming , Optimization , Sparks , Information entropy "
272,FPGA Implementation of FBMC Transceiver for 5G Technologies,VK Singh,2022 International Conference on Computer Communication and Informatics (ICCCI),2329-7190,,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9741009/?casa_token=ZiBhr3PFQhwAAAAA:oFA0SHldKJGUFUc69-cVfkcqeChYumU1NEladwiMIJHltaWlaHPdZSKRsVWRTnQPAq8wX5u3a6CpiA,"Filter bank Multicarrier (FBMC) is a clever procedure developed from OFDM which settle the vast majority of these issues by adopting a separating strategy to multicarrier correspondence framework. FBMC signs can undoubtedly meet the Adjacent Channel Leakage Ratio (ACLR) and they don't utilize cyclic prefix in this manner works on unearthly effectiveness. Because of the incorporation of band-restricted heartbeat molding channels into the sign model in FBMC procedure, the plan of effective handset designs for multicarrier frameworks turns into a difficult errand. An efficient FBMC hardware architecture is designed and implementation is presented. It includes both transmitter and receiver. The design uses pipelined 8-point IFFT/FFT and polyphase filter (PP-F) for the processing at transmitter and receiver module respectively, which indicate that the processing block contain 8 inputs data. It also includes serial to parallel and parallel to serial converter module. Finally, comparative performance analysis of proposed FBMC system over OFDM systems is performed in terms of Signal-to-noise ratio (SNR), Bit error rate (BER), Latency and Throughput. The Results show that proposed FBMC transceiver is better in terms of performance, hardware complexity overhead and power consumption as compared to OFDM system.",,10.1109/ICCCI54379.2022.9741009,"Transmitters , OFDM , Bit error rate , Filter banks , Receivers , Throughput , Telephone sets "
273,A hardware/software co-design approach to prototype 6G mobile applications inside the GNU Radio SDR Ecosystem using FPGA hardware accelerators,"CM Karle, M Kreutzer, J Pfau, J Becker",International Symposium on …,,,,,2022,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3535044.3535049?casa_token=QDRG48sEFbMAAAAA:zTiVbgeFWI-7Fsjm2MR3GmP4vstZdINTYA8roS3AJqrVXc1sqpxd_matbNBC5RXS3reDrQJWLgmqgQ,"… Usage of the FPGA is restricted to these pre-synthesized modules, which communicate using custom FPGA routers on the FPGA. In this work, the FPGA also acts as a standalone …",,,
274,5G Security: FPGA Implementation of SNOW-V Stream Cipher,"L Pyrgas, P Kitsos",2021 24th Euromicro Conference on Digital System Design (DSD),,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9556387/?casa_token=h4ieSLaPI8QAAAAA:gsTCPY0GiDxe3Mo4LOJPCcxxuerthX7dMBrY4Y6fiMvIC0TUWI7F3sPqiU9FTOws47D5Xxe3xDo39A,"In this paper, a very compact architecture the newest member of the SNOW family of stream ciphers, called SNOW-V, is presented. The proposed architecture has a 128-bit datapath and is pipelined in key areas in order to achieve the maximum possible frequency while using only a small number of hardware resources. The design was coded using the VERILOG hardware description language and the BASYS3 board (Artix 7 XC7A35T) was the target of the hardware implementation. The proposed implementation utilizes only 2109 FPGA LUTs and 1352 FFs and reaches a data throughput of 2.6 Gbps at 224 MHz clock frequency.",,10.1109/DSD53832.2021.00064,"Ciphers , 5G mobile communication , Snow , Throughput , Hardware , Table lookup , Security "
275,Machine Learning Enhanced CPU-GPU Simulation Platform for 5G System,"Y Ouyang, C Yin, T Zhou, Y Jin","… MONAMI 2021, Virtual Event, October 27 …",,,,,2022,Springer,https://link.springer.com/chapter/10.1007/978-3-030-94763-7_3,"… introduce the 5G system-level simulation program used to verify CPU-GPU heterogeneous parallel mechanism. Subsequently, the heterogeneous parallel mechanism of CPU-GPU was …",,,
276,Aerial: a GPU hyper-converged platform for 5G,"A Kelkar, C Dick",Proceedings of the SIGCOMM'21 Poster and Demo …,,,,,2021,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3472716.3472864?casa_token=kH3h_A11cecAAAAA:NFuO0ucyx405qi1VzzBl3d1ZsYBjtwp8FMM5rxfTCqk-lZ-knMQVPNH7YQn_9xaziXAEZxDB4Yd4Ow,"… a GPU-accelerated 5G signal processing pipeline, including cuPHY [3] for L1 5G PHY, … efficiency by keeping all physical layer processing within the GPU’s high-performance memory. A …",,,
277,FPGA-based Implementation and Evaluation of Realtime OFDM Phase Compensation in 5G,"H Nguyen, S Nguyen",2021 International Conference on Advanced Technologies for Communications (ATC),2162-1020,,,,2021,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9598312/?casa_token=istlThtUggcAAAAA:y-bNKKtQddABerSeojP0Un6ET6KuKtrX3YgAMWEu6aKoUZ-GIa-6CWxSAo8J4IhL46g4PwuZrzPhsg,"In the New Radio (NR) standard which is standardized by 3GPP as a candidate for 5G mobile communication system, Orthogonal Frequency Division Multiplexing (OFDM) has been selected as the waveform for the air interface. Unlike the 4G Long Term Evolution (LTE), where the carrier frequency between the transmitter and receiver are always at the same locations, in NR, they can be at different frequencies. In such case, this leads to the phase ramped up at the receiver which cannot be recovered if the OFDM symbol is not equipped with reference signal for channel estimation and equalization. In this paper, we present our model for this issue and how to eliminate it by compensating the phase difference at both transmitter and receiver. We also implement this approach in Field Programmable Gate Array (FPGA) and validate it in real hardware testbed. The result show that with phase compensation, the received signal constellation is significantly improved.",,10.1109/ATC52653.2021.9598312,"Constellation diagram , OFDM , Radio transmitters , Receivers , Hardware , Generators , Frequency division multiplexing "
278,FPGA implementation of polar codes for 5G eMBB control channels,"G Aparna, R Swathi, MK Joseph, ...",… Journal of Ultra …,,,,,2021,inderscienceonline.com,https://www.inderscienceonline.com/doi/abs/10.1504/IJUWBCS.2021.119139,"… broad band (eMBB) control channel standards like 5G is a challenging issue at present. Polar … , ISE Navigator 14.2, HDL synthesiser on the target FPGA device XC6vlx760-1-ffl760 for …",,,
279,Design of a Real-Time DSP Engine on RF-SoC FPGA for 5G Networks,"V Kitsakis, K Kanta, I Stratakos, G Giannoulis, ...",Optical Network Design …,,,,,2020,Springer,https://link.springer.com/chapter/10.1007/978-3-030-38085-4_46,"… /directions in emerging 5G technologies that will benefit from FPGA platforms and, … links in 5G networks. The proposed design exploits a state-of-the-art system-on-chip FPGA tailored for …",,,
280,Implementation of Decimator Filter in 5G system for Area and Power Optimization Using FPGA,"ME Christopher, D Bhoomika, ...",… -systems and Signal …,,,,,2022,pices-journal.com,http://www.pices-journal.com/ojs/index.php/pices/article/view/360,One of the key components in the baseband section of a 5G system is the filter decimator unit. This unit helps to remove the excess bandwidth and reduce the sampling frequency of the …,,,
281,A 5G Based Demodulator On FPGA,"B Shashikala, A Kumar",Perspectives in Communication …,,,,,2021,pices-journal.com,http://pices-journal.com/ojs/index.php/pices/article/view/341,… A BPSK modulation technique is used for modulating message signals in a 5G environment. … Both proposed and existing systems are implemented in Spartan 3E FPGA. Simulation is …,,,
282,FPGA accelerated verification of multiple Application-Specific Instruction-set Processor based 5G fronthaul IP,H Haidak,,,,,,2022,aaltodoc.aalto.fi,https://aaltodoc.aalto.fi/handle/123456789/117332,… ohjelmoitavalle porttimatriisille (FPGA) joka mahdollistaa … FPGA Test-bench on luotu todentamaan Nokian 5G fronthaul… Prosessoinnin jälkeen Fronthaul IP lähettää tietoa ulos FPGA:…,,,
283,Throughput Analysis with Effect of Dimensionality Reduction on 5G Dataset using Machine Learning and Deep Learning Models,M Supriya,2022 International Conference on Industry 4.0 Technology (I4Tech),,,,,2022,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9952579/,"5G or ZTE the latest improvement to the existing 4G communication standard. These technologies could be evaluated by various metrics called performance indicators among which throughput plays a major role. Throughput is the measure of the rate of data transferred to the device. Higher the throughput, better is the performance of the network. This work models and analyses the throughput obtained with the variations observed on the identified parameters on which it depends on. Here the problem is analysed as a regression problem and hence regressor models are applied. Multiple models ranging from statistical to probabilistic and machine learning to deep recurrent networks are analysed with a 10 fold cross validation. Also, the effect of dimensionality reduction is applied to the dataset and the performance is observed. It is noticed from the work that the top performing models are consistent in performance measured using the regression metrics.",,10.1109/I4Tech55392.2022.9952579,"Dimensionality reduction , Performance evaluation , Analytical models , 5G mobile communication , Receivers , Predictive models , Throughput "
284,Lightweight testbed for machine learning evaluation in 5G networks,"CE Hernández Chulde, ...",JITEL 2019-XIV …,,,,,2019,upcommons.upc.edu,https://upcommons.upc.edu/handle/2117/185190,"… To evaluate the adoption of machine learning in 5G networks, an … testbed, which utilizes the benefits of container lightweight virtualization technology to create machine learning network …",,,
285,Shared dataset framework toward applications of machine learning and mathematical optimization for 6G,"K Maruta, Y Ida, Y Hou, O Muta, H Okada, ...",IEICE Technical Report; …,,,,,2020,ieice.org,https://www.ieice.org/ken/paper/202010220CaC/eng/,"… (in English) In the research of applying machine learning and mathematical optimization to … , if a dataset suitable for research purposes is not available, efforts to create a dataset with …",,,
