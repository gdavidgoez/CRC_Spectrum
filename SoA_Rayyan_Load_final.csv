,title,authors,journal,issn,volume,issue,pages,year,publisher,url,abstract,notes,doi,keywords,,,,,,,,,,
0,6G: The Next Frontier: From Holographic Messaging to Artificial Intelligence Using Subterahertz and Visible Light Communication,"Emilio Calvanese Strinati , Sergio Barbarossa , Jose Luis Gonzalez-Jimenez , Dimitri Ktenas , Nicolas Cassiau , Luc Maret , Cedric Dehos ",IEEE Vehicular Technology Magazine,1556-6080,14,3,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8792135,"With its ability to provide a single platform enabling a variety of services, such as enhanced mobile broadband communications, virtual reality, automated driving, and the Internet of Things, 5G represents a breakthrough in the design of communication networks. Nevertheless, considering the increasing requests for new services and predicting the development of new technologies within a decade, it is already possible to envision the need to move beyond 5G and design a new architecture incorporating innovative technologies to satisfy new needs at both the individual and societal levels.",,10.1109/MVT.2019.2921162,"5G mobile communication , Semantics , Three-dimensional displays , Cloud computing , Machine learning ",,,,,,,,,,
1,Future Intelligent and Secure Vehicular Network Toward 6G: Machine-Learning Approaches,"Fengxiao Tang , Yuichi Kawamoto , Nei Kato , Jiajia Liu ",Proceedings of the IEEE,1558-2256,108,2,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8926369,"As a powerful tool, the vehicular network has been built to connect human communication and transportation around the world for many years to come. However, with the rapid growth of vehicles, the vehicular network becomes heterogeneous, dynamic, and large scaled, which makes it difficult to meet the strict requirements, such as ultralow latency, high reliability, high security, and massive connections of the next-generation (6G) network. Recently, machine learning (ML) has emerged as a powerful artificial intelligence (AI) technique to make both the vehicle and wireless communication highly efficient and adaptable. Naturally, employing ML into vehicular communication and network becomes a hot topic and is being widely studied in both academia and industry, paving the way for the future intelligentization in 6G vehicular networks. In this article, we provide a survey on various ML techniques applied to communication, networking, and security parts in vehicular networks and envision the ways of enabling AI toward a future 6G vehicular network, including the evolution of intelligent radio (IR), network intelligentization, and self-learning with proactive exploration.",,10.1109/JPROC.2019.2954595,"Vehicle dynamics , Resource management , Security , Array signal processing , Machine learning , OFDM , Vehicle-to-everything , Vehicular ad hoc networks ",,,,,,,,,,
2,Quantum Machine Learning for 6G Communication Networks: State-of-the-Art and Vision for the Future,"Syed Junaid Nawaz , Shree Krishna Sharma , Shurjeel Wyne , Mohammad N. Patwary , Md. Asaduzzaman ",IEEE Access,2169-3536,7,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8681450,"The upcoming fifth generation (5G) of wireless networks is expected to lay a foundation of intelligent networks with the provision of some isolated artificial intelligence (AI) operations. However, fully intelligent network orchestration and management for providing innovative services will only be realized in Beyond 5G (B5G) networks. To this end, we envisage that the sixth generation (6G) of wireless networks will be driven by on-demand self-reconfiguration to ensure a many-fold increase in the network performance and service types. The increasingly stringent performance requirements of emerging networks may finally trigger the deployment of some interesting new technologies, such as large intelligent surfaces, electromagnetic-orbital angular momentum, visible light communications, and cell-free communications, to name a few. Our vision for 6G is a massively connected complex network capable of rapidly responding to the users' service calls through real-time learning of the network state as described by the network edge (e.g., base-station locations and cache contents), air interface (e.g., radio spectrum and propagation channel), and the user-side (e.g., battery-life and locations). The multi-state, multi-dimensional nature of the network state, requiring the real-time knowledge, can be viewed as a quantum uncertainty problem. In this regard, the emerging paradigms of machine learning (ML), quantum computing (QC), and quantum ML (QML) and their synergies with communication networks can be considered as core 6G enablers. Considering these potentials, starting with the 5G target services and enabling technologies, we provide a comprehensive review of the related state of the art in the domains of ML (including deep learning), QC, and QML and identify their potential benefits, issues, and use cases for their applications in the B5G networks. Subsequently, we propose a novel QC-assisted and QML-based framework for 6G communication networks while articulating its challenges and potential enabling technologies at the network infrastructure, network edge, air interface, and user end. Finally, some promising future research directions for the quantum- and QML-assisted B5G networks are identified and discussed.",,10.1109/ACCESS.2019.2909490,"5G mobile communication , Communication networks , Quantum computing , Machine learning , Wireless networks , Parallel processing , Quantum communication ",,,,,,,,,,
3,Channel State Information Prediction for 5G Wireless Communications: A Deep Learning Approach,"Changqing Luo , Jinlong Ji , Qianlong Wang , Xuhui Chen , Pan Li ",IEEE Transactions on Network Science and Engineering,2334-329X,7,1,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8395053,"Channel state information (CSI) estimation is one of the most fundamental problems in wireless communication systems. Various methods, so far, have been developed to conduct CSI estimation. However, they usually require high computational complexity, which makes them unsuitable for 5G wireless communications due to employing many new techniques (e.g., massive MIMO, OFDM, and millimeter-Wave (mmWave)). In this paper, we propose an efficient online CSI prediction scheme, called OCEAN, for predicting CSI from historical data in 5G wireless communication systems. Specifically, we first identify several important features affecting the CSI of a radio link and a data sample consists of the information of the features and the CSI. We then design a learning framework that is an integration of a CNN (convolutional neural network) and a long short term with memory (LSTM) network. We also further develop an offline-online two-step training mechanism, enabling the prediction results to be more stable when applying it to practical 5G wireless communication systems. To validate OCEAN's efficacy, we consider four typical case studies, and conduct extensive experiments in the four scenarios, i.e., two outdoor and two indoor scenarios. The experiment results show that OCEAN not only obtains the predicted CSI values very quickly but also achieves highly accurate CSI prediction with up to 2.650-3.457 percent average difference ratio (ADR) between the predicted and measured CSI.",,10.1109/TNSE.2018.2848960,"Wireless communication , 5G mobile communication , Estimation , Channel estimation , MIMO communication , Oceans , Fading channels ",,,,,,,,,,
4,"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions","Manuel Eugenio Morocho-Cayamcela , Haeyoung Lee , Wansu Lim ",IEEE Access,2169-3536,7,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8844682,"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",,10.1109/ACCESS.2019.2942390,"Wireless communication , Training , 5G mobile communication , Supervised learning , Task analysis , Reinforcement learning ",,,,,,,,,,
5,"6G Visions: Mobile ultra-broadband, super internet-of-things, and artificial intelligence","Lin Zhang , Ying-Chang Liang , Dusit Niyato ",China Communications,1673-5447,16,8,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8820755,"With a ten-year horizon from concept to reality, it is time now to start thinking about what will the sixth-generation (6G) mobile communications be on the eve of the fifth-generation (5G) deployment. To pave the way for the development of 6G and beyond, we provide 6G visions in this paper. We first introduce the state-of-the-art technologies in 5G and indicate the necessity to study 6G. By taking the current and emerging development of wireless communications into consideration, we envision 6G to include three major aspects, namely, mobile ultra-broadband, super Internet-of-Things (IoT), and artificial intelligence (AI). Then, we review key technologies to realize each aspect. In particular, teraherz (THz) communications can be used to support mobile ultra-broadband, symbiotic radio and satellite-assisted communications can be used to achieve super IoT, and machine learning techniques are promising candidates for AI. For each technology, we provide the basic principle, key challenges, and state-of-the-art approaches and solutions.",,10.23919/JCC.2019.08.001,"6G mobile communication , 5G mobile communication , Absorption , Wireless communication , Artificial intelligence , Bandwidth , 3GPP ",,,,,,,,,,
6,"Deep Learning for Physical-Layer 5G Wireless Techniques: Opportunities, Challenges and Solutions","Hongji Huang , Song Guo , Guan Gui , Zhen Yang , Jianhua Zhang , Hikmet Sari , Fumiyuki Adachi ",IEEE Wireless Communications,1558-0687,27,1,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8786074,"The new demands for high-reliability and ultra-high capacity wireless communication have led to extensive research into 5G communications. However, current communication systems, which were designed on the basis of conventional communication theories, significantly restrict further performance improvements and lead to severe limitations. Recently, the emerging deep learning techniques have been recognized as a promising tool for handling the complicated communication systems, and their potential for optimizing wireless communications has been demonstrated. In this article, we first review the development of deep learning solutions for 5G communication, and then propose efficient schemes for deep learning-based 5G scenarios. Specifically, the key ideas for several important deep learning-based communication methods are presented along with the research opportunities and challenges. In particular, novel communication frameworks of NOMA, massive multiple-input multiple-output (MIMO), and millimeter wave (mmWave) are investigated, and their superior performances are demonstrated. We envision that the appealing deep learning- based wireless physical layer frameworks will bring a new direction in communication theories and that this work will move us forward along this road.",,10.1109/MWC.2019.1900027,"MIMO communication , Deep learning , 5G mobile communication , Channel estimation , Wireless communication , NOMA ",,,,,,,,,,
7,Artificial-Intelligence-Enabled Intelligent 6G Networks,H. Yang and A. Alphones and Z. Xiong and D. Niyato and J. Zhao and K. Wu,IEEE Network,1558-156X     VO  - 34,34,6,272-280,2020,IEEE,https://ieeexplore.ieee.org/abstract/document/9237460/?casa_token=KQN2dYbgKmYAAAAA:4fFRic3vEQwlMGgdajmKYPauvBQSbf7AEn-xvRds9_UpR7HqQuXd31lBK__Yt-UW6FgLqhcDprcCTw,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.",,10.1109/MNET.011.2000195,Intelligence,,,,,,,,,,
8,Ten Challenges in Advancing Machine Learning Technologies toward 6G,"Nei Kato , Bomin Mao , Fengxiao Tang , Yuichi Kawamoto , Jiajia Liu ",IEEE Wireless Communications,1558-0687,27,3,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9061001,"As the 5G standard is being completed, academia and industry have begun to consider a more developed cellular communication technique, 6G, which is expected to achieve high data rates up to 1 Tb/s and broad frequency bands of 100 GHz to 3 THz. Besides the significant upgrade of the key communication metrics, Artificial Intelligence (AI) has been envisioned by many researchers as the most important feature of 6G, since the state-of-the-art machine learning technique has been adopted as the top solution in many extremely complex scenarios. Network intelligentization will be the new trend to address the challenges of exponentially increasing number of connected heterogeneous devices. However, compared with the application of machine learning in other fields, such as computer games, current research on intelligent networking still has a long way to go to realize the automatically- configured cellular communication systems. Various problems in terms of communication system, machine learning architectures, and computation efficiency should be addressed for the full use of this technique in 6G. In this paper, we analyze machine learning techniques and introduce 10 most critical challenges in advancing the intelligent 6G system.",,10.1109/MWC.001.1900476,"Machine learning , 5G mobile communication , Machine learning algorithms , Measurement , Network security , Physical layer ",,,,,,,,,,
9,A Self-Adaptive Deep Learning-Based System for Anomaly Detection in 5G Networks,"Lorenzo Fernández Maimó , Ángel Luis Perales Gómez , Félix J. García Clemente , Manuel Gil Pérez , Gregorio Martínez Pérez ",IEEE Access,2169-3536,6,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8283694,"The upcoming fifth-generation (5G) mobile technology, which includes advanced communication features, is posing new challenges on cybersecurity defense systems. Although innovative approaches have evolved in the last few years, 5G will make existing intrusion detection and defense procedures become obsolete, in case they are not adapted accordingly. In this sense, this paper proposes a novel 5G-oriented cyberdefense architecture to identify cyberthreats in 5G mobile networks efficient and quickly enough. For this, our architecture uses deep learning techniques to analyze network traffic by extracting features from network flows. Moreover, our proposal allows adapting, automatically, the configuration of the cyberdefense architecture in order to manage traffic fluctuation, aiming both to optimize the computing resources needed in each particular moment and to fine tune the behavior and the performance of analysis and detection processes. Experiments using a well-known botnet data set depict how a neural network model reaches a sufficient classification accuracy in our anomaly detection system. Extended experiments using diverse deep learning solutions analyze and determine their suitability and performance for different network traffic loads. The experimental results show how our architecture can self-adapt the anomaly detection system based on the volume of network flows gathered from 5G subscribers' user equipments in real-time and optimizing the resource consumption.",,10.1109/ACCESS.2018.2803446,"Anomaly detection , 5G mobile communication , Machine learning , Botnet , Computer architecture , Feature extraction ",,,,,,,,,,
10,Deep Learning-Based Traffic Safety Solution for a Mixture of Autonomous and Manual Vehicles in a 5G-Enabled Intelligent Transportation System,"Keping Yu , Long Lin , Mamoun Alazab , Liang Tan , Bo Gu ",IEEE Transactions on Intelligent Transportation Systems,1558-0016,22,7,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9303409,"It is expected that a mixture of autonomous and manual vehicles will persist as a part of the intelligent transportation system (ITS) for many decades. Thus, addressing the safety issues arising from this mix of autonomous and manual vehicles before autonomous vehicles are entirely popularized is crucial. As the ITS system has increased in complexity, autonomous vehicles exhibit problems such as a low intention recognition rate and poor real-time performance when predicting the driving direction; these problems seriously affect the safety and comfort of mixed traffic systems. Therefore, the ability of autonomous vehicles to predict the driving direction in real time according to the surrounding traffic environment must be improved and researchers must work to create a more mature ITS. In this paper, we propose a deep learning-based traffic safety solution for a mixture of autonomous and manual vehicles in a 5G-enabled ITS. In this scheme, a driving trajectory dataset and a natural-driving dataset are employed as the network inputs to long-term memory networks in the 5G-enabled ITS: the probability matrix of each intention is calculated by the softmax function. Then, the final intention probability is obtained by fusing the mean rule in the decision layer. Experimental results show that the proposed scheme achieves intention recognition rates of 91.58% and 90.88% for left and right lane changes, respectively, effectively improving both accuracy and real-time intention recognition and improving the lane change problem in a mixed traffic environment.",,10.1109/TITS.2020.3042504,"Vehicles , Hidden Markov models , Safety , Manuals , 5G mobile communication , Real-time systems , Autonomous vehicles ",,,,,,,,,,
11,5G MIMO Data for Machine Learning: Application to Beam-Selection Using Deep Learning,"Aldebaro Klautau , Pedro Batista , Nuria González-Prelcic , Yuyang Wang , Robert W. Heath ",2018 Information Theory and Applications Workshop (ITA),,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8503086,"The increasing complexity of configuring cellular networks suggests that machine learning (ML) can effectively improve 5G technologies. Deep learning has proven successful in ML tasks such as speech processing and computational vision, with a performance that scales with the amount of available data. The lack of large datasets inhibits the flourish of deep learning applications in wireless communications. This paper presents a methodology that combines a vehicle traffic simulator with a ray-tracing simulator, to generate channel realizations representing 5G scenarios with mobility of both transceivers and objects. The paper then describes a specific dataset for investigating beam-selection techniques on vehicle-to-infrastructure using millimeter waves. Experiments using deep learning in classification, regression and reinforcement learning problems illustrate the use of datasets generated with the proposed methodology.",,10.1109/ITA.2018.8503086,"5G mobile communication , MIMO communication , Ray tracing , Computational modeling , Receivers ",,,,,,,,,,
12,Artificial Intelligence-Enabled Cellular Networks: A Critical Path to Beyond-5G and 6G,"Rubayet Shafin , Lingjia Liu , Vikram Chandrasekhar , Hao Chen , Jeffrey Reed , Jianzhong Charlie Zhang ",IEEE Wireless Communications,1558-0687,27,2,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9040202,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.",,10.1109/MWC.001.1900323,"Cellular networks , Receivers , Complexity theory , MIMO communication , 5G mobile communication , Deep learning ",,,,,,,,,,
13,From 4G to 5G: Self-organized network management meets machine learning,"J Moysen, L Giupponi",Computer Communications,0140-3664,129,,248-268,2018,Elsevier,https://www.sciencedirect.com/science/article/pii/S0140366418300380?casa_token=eLIUDmwIc_8AAAAA:4Kq1gZayOEcqQbMQQY_hhVj_Z61XK1juTqZSofmawf9QP4O5Ei6q9kgPojDqRkrprJoxgqyIrmU,"Self-organization as applied to cellular networks is usually referred to Selforganizing Networks (SONs), and it is a key driver for improving Operations,Administration, and Management (OAM) activities. SON aims at reducing the cost of installation and management of 4G and future 5G networks, by simplifying operational tasks through the capability to configure, optimize and heal itself. To satisfy 5G network management requirements, this autonomous management vision has to be extended to the end to end network. In literature and also in some instances of products available in the market, Machine Learning (ML) has been identified as the key tool to implement autonomous adaptability and take advantage of experience when making decisions. In this paper, we survey how 5G network management, with an end-to-end perspective of the network, can significantly benefit from ML solutions. We review and provide the basic concepts and taxonomy for SON, network management and ML. We analyze the available state of the art in the literature, standardization, and in the market. We pay special attention to 3rd Generation Partnership Project (3GPP) evolution in the area of network management and to the data that can be extracted from 3GPP networks, in order to gain knowledge and experience in how the network is working, and improve network performance in a proactive way. Finally, we go through the main challenges associated with this line of research, in both 4G and in what 5G is getting designed, while identifying new directions for research.",,10.1016/j.comcom.2018.07.015,"Network management, Self-organizing networks, Mobile networks, Machine learning, Big data",,,,,,,,,,
14,DeepCog: Cognitive Network Management in Sliced 5G Networks with Deep Learning,"Dario Bega , Marco Gramaglia , Marco Fiore , Albert Banchs , Xavier Costa-Perez ",IEEE INFOCOM 2019 - IEEE Conference on Computer Communications,0743-166X,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8737488,"Network slicing is a new paradigm for future 5G networks where the network infrastructure is divided into slices devoted to different services and customized to their needs. With this paradigm, it is essential to allocate to each slice the needed resources, which requires the ability to forecast their respective demands. To this end, we present DeepCog, a novel data analytics tool for the cognitive management of resources in 5G systems. DeepCog forecasts the capacity needed to accommodate future traffic demands within individual network slices while accounting for the operator's desired balance between resource overprovisioning (i.e., allocating resources exceeding the demand) and service request violations (i.e., allocating less resources than required). To achieve its objective, DeepCog hinges on a deep learning architecture that is explicitly designed for capacity forecasting. Comparative evaluations with real-world measurement data prove that DeepCog's tight integration of machine learning into resource orchestration allows for substantial (50% or above) reduction of operating expenses with respect to resource allocation solutions based on state-of-the-art mobile traffic predictors. Moreover, we leverage DeepCog to carry out an extensive first analysis of the trade-off between capacity overdimensioning and unserviced demands in adaptive, sliced networks and in presence of real-world traffic.",,10.1109/INFOCOM.2019.8737488,"Resource management , Deep learning , Base stations , Neural networks , Maximum likelihood detection , Nonlinear filters , 5G mobile communication ",,,,,,,,,,
15,Improving Traffic Forecasting for 5G Core Network Scalability: A Machine Learning Approach,"Imad Alawe , Adlen Ksentini , Yassine Hadjadj-Aoul , Philippe Bertin ",IEEE Network,1558-156X,32,6,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8553653,"5G is expected to provide network connectivity to not only classical devices (i.e., tablets, smartphones, etc.) but also to the IoT, which will drastically increase the traffic load carried over the network. 5G will mainly rely on NFV and SDN to build flexible and on-demand instances of functional networking entities via VNFs. Indeed, 3GPP is devising a new architecture for the core network, which replaces point-to-point interfaces used in 3G and 4G by producer/consumer-based communication among 5G core network functions, facilitating deployment over a virtual infrastructure. One big advantage of using VNFs is the possibility of dynamic scaling, depending on traffic load (i.e., instantiate new resources to VNFs when the traffic load increases and reduce the number of resources when the traffic load decreases). In this article, we propose a novel mechanism to scale 5G core network resources by anticipating traffic load changes through forecasting via ML techniques. The traffic load forecast is achieved by using and training a neural network on a real dataset of traffic arrival in a mobile network. Two techniques were used and compared: RNN, more specifically LSTM; and DNN. Simulation results show that the forecast-based scalability mechanism outperforms the threshold-based solutions, in terms of latency to react to traffic change, and delay to have new resources ready to be used by the VNF to react to traffic increase.",,10.1109/MNET.2018.1800104,"5G mobile communication , Scalability , 3GPP , Telecommunication traffic , Cloud computing , Networked control systems , Delays , Traffic control , Load management , Software defined networking , Smart devices , Dynamic scheduling ",,,,,,,,,,
16,A Tutorial on Ultrareliable and Low-Latency Communications in 6G: Integrating Domain Knowledge Into Deep Learning,"Changyang She , Chengjian Sun , Zhouyou Gu , Yonghui Li , Chenyang Yang , H. Vincent Poor , Branka Vucetic ",Proceedings of the IEEE,1558-2256,109,3,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9369424,"As one of the key communication scenarios in the fifth-generation and also the sixth-generation (6G) mobile communication networks, ultrareliable and low-latency communications (URLLCs) will be central for the development of various emerging mission-critical applications. State-of-the-art mobile communication systems do not fulfill the end-to-end delay and overall reliability requirements of URLLCs. In particular, a holistic framework that takes into account latency, reliability, availability, scalability, and decision-making under uncertainty is lacking. Driven by recent breakthroughs in deep neural networks, deep learning algorithms have been considered as promising ways of developing enabling technologies for URLLCs in future 6G networks. This tutorial illustrates how domain knowledge (models, analytical tools, and optimization frameworks) of communications and networking can be integrated into different kinds of deep learning algorithms for URLLCs. We first provide some background of URLLCs and review promising network architectures and deep learning frameworks for 6G. To better illustrate how to improve learning algorithms with domain knowledge, we revisit model-based analytical tools and cross-layer optimization frameworks for URLLCs. Following this, we examine the potential of applying supervised/unsupervised deep learning and deep reinforcement learning in URLLCs and summarize related open problems. Finally, we provide simulation and experimental results to validate the effectiveness of different learning algorithms and discuss future directions.",,10.1109/JPROC.2021.3053601,"Deep learning , 6G mobile communication , Knowledge engineering , Ultra reliable low latency communication , Reliability , Network architecture , Optimization , Reinforcement learning , Low latency communication , Unsupervised learning ",,,,,,,,,,
17,Deep Learning for Hybrid 5G Services in Mobile Edge Computing Systems: Learn From a Digital Twin,"Rui Dong , Changyang She , Wibowo Hardjawana , Yonghui Li , Branka Vucetic ",IEEE Transactions on Wireless Communications,1558-2248,18,10,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8764584,"In this paper, we consider a mobile edge computing system with both ultra-reliable and low-latency communications services and delay tolerant services. We aim to minimize the normalized energy consumption, defined as the energy consumption per bit, by optimizing user association, resource allocation, and offloading probabilities subject to the quality-of-service requirements. The user association is managed by the mobility management entity (MME), while resource allocation and offloading probabilities are determined by each access point (AP). We propose a deep learning (DL) architecture, where a digital twin of the real network environment is used to train the DL algorithm off-line at a central server. From the pre-trained deep neural network (DNN), the MME can obtain user association scheme in a real-time manner. Considering that the real networks are not static, the digital twin monitors the variation of real networks and updates the DNN accordingly. For a given user association scheme, we propose an optimization algorithm to find the optimal resource allocation and offloading probabilities at each AP. The simulation results show that our method can achieve lower normalized energy consumption with less computation complexity compared with an existing method and approach to the performance of the global optimal solution.",,10.1109/TWC.2019.2927312,"Delays , Servers , Resource management , Energy consumption , Task analysis , Reliability , Machine learning algorithms ",,,,,,,,,,
18,A Deep-Learning-Based Radio Resource Assignment Technique for 5G Ultra Dense Networks,"Yibo Zhou , Zubair Md. Fadlullah , Bomin Mao , Nei Kato ",IEEE Network,1558-156X,32,6,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8553651,"Recently, deep learning has emerged as a state-of-the-art machine learning technique with promising potential to drive significant breakthroughs in a wide range of research areas. The application of deep learning for network traffic control, however, remains immature due to the difficulty in uniquely characterizing the network traffic features as an appropriate input and output dataset to the learning structures. The network traffic features are anticipated to be even more dynamic and complex in the UDNs of the emerging 5G networks with high traffic demands coupled with beamforming and massive MIMO technologies. Therefore, it is critical for 5G network operators to carry out radio resource control in an efficient manner instead of adopting the simple conventional F/TDD. This is because the conventional uplink-downlink configuration change in the existing dynamic TDD method, typically used for resource assignment in beamforming and massive-MIMO-based UDNs, is prone to repeated congestion. In this article, we address this issue and discuss how to leverage the deep LSTM learning technique to make localized prediction of the traffic load at the UDN base station (i.e., the eNB). Based on localized prediction, our proposed algorithm executes the appropriate action policy a priori to avoid/alleviate the congestion in an intelligent fashion. Simulation results demonstrate that our proposal outperforms the conventional method in terms of packet loss rate, throughput, and MOS.",,10.1109/MNET.2018.1800085,"5G mobile communication , Array signal processing , MIMO communication , Telecommunication traffic , Dynamic scheduling , Traffic control , Networked traffic control , Predictive models , Deep learning ",,,,,,,,,,
19,Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenges,"Cheng-Xiang Wang , Marco Di Renzo , Slawomir Stanczak , Sen Wang , Erik G. Larsson ",IEEE Wireless Communications,1558-0687,27,1,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9023918,"5G wireless communication networks are currently being deployed, and B5G networks are expected to be developed over the next decade. AI technologies and, in particular, ML have the potential to efficiently solve the unstructured and seemingly intractable problems by involving large amounts of data that need to be dealt with in B5G. This article studies how AI and ML can be leveraged for the design and operation of B5G networks. We first provide a comprehensive survey of recent advances and future challenges that result from bringing AI/ML technologies into B5G wireless networks. Our survey touches on different aspects of wireless network design and optimization, including channel measurements, modeling, and estimation, physical layer research, and network management and optimization. Then ML algorithms and applications to B5G networks are reviewed, followed by an overview of standard developments of applying AI/ML algorithms to B5G networks. We conclude this study with future challenges on applying AI/ML to B5G networks.",,10.1109/MWC.001.1900292,"Artificial intelligence , Channel estimation , Massive MIMO , 5G mobile communication , Loss measurement , Wireless networks ",,,,,,,,,,
20,"Machine Learning for 6G Wireless Networks: Carrying Forward Enhanced Bandwidth, Massive Access, and Ultrareliable/Low-Latency Service","Jun Du , Chunxiao Jiang , Jian Wang , Yong Ren , Merouane Debbah ",IEEE Vehicular Technology Magazine,1556-6080,15,4,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9206115,"To satisfy the expected plethora of demanding services, the future generation of wireless networks (6G) has been mandated as a revolutionary paradigm to carry forward the capacities of enhanced broadband, massive access, and ultrareliable and lowlatency service in 5G wireless networks to a more powerful and intelligent level. Recently, the structure of 6G networks has tended to be extremely heterogeneous, densely deployed, and dynamic. Combined with tight quality of service (QoS), such complex architecture will result in the untenability of legacy network operation routines. In response, artificial intelligence (AI), especially machine learning (ML), is emerging as a fundamental solution to realize fully intelligent network orchestration and management. By learning from uncertain and dynamic environments, AI-/ML-enabled channel estimation and spectrum management will open up opportunities for bringing the excellent performance of ultrabroadband techniques, such as terahertz communications, into full play. Additionally, challenges brought by ultramassive access with respect to energy and security can be mitigated by applying AI-/ML-based approaches. Moreover, intelligent mobility management and resource allocation will guarantee the ultrareliability and low latency of services. Concerning these issues, this article introduces and surveys some state-of-the-art techniques based on AI/ML and their applications in 6G to support ultrabroadband, ultramassive access, and ultrareliable and lowlatency services.",,10.1109/MVT.2020.3019650,"6G mobile communication , 5G mobile communication , Artificial intelligence , Channel estimation , Radio spectrum management , Neural networks ",,,,,,,,,,
21,"Edge Artificial Intelligence for 6G: Vision, Enabling Technologies, and Applications","Khaled B. Letaief , Yuanming Shi , Jianmin Lu , Jianhua Lu ",IEEE Journal on Selected Areas in Communications,1558-0008,40,1,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9606720,"The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from “connected things” to “connected intelligence”. However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems.",,10.1109/JSAC.2021.3126076,"Artificial intelligence , 6G mobile communication , Task analysis , Sensors , Communication system security , Training , Standards ",,,,,,,,,,
22,6G white paper on machine learning in wireless communication networks,"S Ali, W Saad, N Rajatheva, K Chang, ...",arXiv preprint arXiv …,,,,1–29,2020,arxiv.org,https://arxiv.org/abs/2004.13875,"The focus of this white paper is on machine learning (ML) in wireless communications. 6G wireless communication networks will be the backbone of the digital transformation of societies by providing ubiquitous, reliable, and near-instant wireless connectivity for humans and machines. Recent advances in ML research has led enable a wide range of novel technologies such as selfdriving vehicles and voice assistants. Such innovation is possible as a result of the availability of advanced ML models, large datasets, and high computational power. On the other hand, the ever-increasing demand for connectivity will require a lot of innovation in 6G wireless networks, and ML tools will play a major role in solving problems in the wireless domain. In this paper, we provide an overview of the vision of how ML will impact the wireless communication systems. We first give an overview of the ML methods that have the highest potential to be used in wireless networks. Then, we discuss the problems that can be solved by using ML in various layers of the network such as the physical layer, medium access layer, and application layer. Zero-touch optimization of wireless networks using ML is another interesting aspect that is discussed in this paper. Finally, at the end of each section, important research questions that the section aims to answer are presented.",,10.48550/arXiv.2004.13875 ,"Computer Science,  Information Theory, Electrical Engineering and Systems Science, Signal Processing",,,,,,,,,,
23,Explainable Artificial Intelligence for 6G: Improving Trust between Human and Machine,Weisi Guo ,IEEE Communications Magazine,1558-1896,58,6,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9141213,"As 5G mobile networks are bringing about global societal benefits, the design phase for 6G has started. Evolved 5G and 6G will need sophisticated AI to automate information delivery simultaneously for mass autonomy, human machine interfacing, and targeted healthcare. Trust will become increasingly critical for 6G as it manages a wide range of mission-critical services. As we migrate from traditional mathematical model-dependent optimization to data-dependent deep learning, the insight and trust we have in our optimization modules decrease. This loss of model explainability means we are vulnerable to malicious data, poor neural network design, and the loss of trust from stakeholders and the general public -- all with a range of legal implications. In this review, we outline the core methods of explainable artificial intelligence (XAI) in a wireless network setting, including public and legal motivations, definitions of explainability, performance vs. explainability trade-offs, and XAI algorithms. Our review is grounded in case studies for both wireless PHY and MAC layer optimization and provide the community with an important research area to embark upon.",,10.1109/MCOM.001.2000050,"Mathematical model , Computational modeling , Wireless communication , Optimization , Analytical models , Deep learning , 6G mobile communication ",,,,,,,,,,
24,Artificial Intelligence in 5G Technology: A Survey,"Manuel Eugenio Morocho Cayamcela , Wansu Lim ",2018 International Conference on Information and Communication Technology Convergence (ICTC),2162-1233,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8539642,"A fully operative and efficient 5G network cannot be complete without the inclusion of artificial intelligence (AI) routines. Existing 4G networks with all-IP (Internet Protocol) broadband connectivity are based on a reactive conception, leading to a poorly efficiency of the spectrum. AI and its subcategories like machine learning and deep learning have been evolving as a discipline, to the point that nowadays this mechanism allows fifth-generation (5G) wireless networks to be predictive and proactive, which is essential in making the 5G vision conceivable. This paper is motivated by the vision of intelligent base stations making decisions by themselves, mobile devices creating dynamically-adaptable clusters based on learned data rather than pre-established and fixed rules, that will take us to a improve in the efficiency, latency, and reliability of the current and real-time network applications in general. An exploration of the potential of AI-based solution approaches in the context of 5G mobile and wireless communications technology is presented, evaluating the different challenges and open issues for future research.",,10.1109/ICTC.2018.8539642,"Wireless communication , 5G mobile communication , Supervised learning , Artificial neural networks , Task analysis , Adaptation models ",,,,,,,,,,
25,Artificial Intelligence Defined 5G Radio Access Networks,"Miao Yao , Munawwar Sohul , Vuk Marojevic , Jeffrey H. Reed ",IEEE Communications Magazine,1558-1896,57,3,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8663985,"Massive multiple-input multiple-output antenna systems, millimeter-wave communications, and ultra-dense networks have been widely perceived as the three key enablers that facilitate the development and deployment of 5G systems. This article discusses the intelligent agent that combines sensing, learning, and optimizing to facilitate these enablers. We present a flexible, rapidly deployable, and cross-layer artificial intelligence (AI)-based framework to enable the imminent and future demands on 5G and beyond. We present example AI-enabled 5G use cases that accommodate important 5G-specific capabilities and discuss the value of AI for enabling network evolution.",,10.1109/MCOM.2019.1800629,"5G mobile communication , Wireless communication , Resource management , Autonomous vehicles , Artificial intelligence , Wireless sensor networks ",,,,,,,,,,
26,An Online Context-Aware Machine Learning Algorithm for 5G mmWave Vehicular Communications,"Gek Hong Sim , Sabrina Klos , Arash Asadi , Anja Klein , Matthias Hollick ",IEEE/ACM Transactions on Networking,1558-2566,26,6,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8472783,"Millimeter-Wave (mmWave) bands have become the de-facto candidate for 5G vehicle-to-everything (V2X) since future vehicular systems demand Gbps links to acquire the necessary sensory information for (semi)-autonomous driving. Nevertheless, the directionality of mmWave communications and its susceptibility to blockage raise severe questions on the feasibility of mmWave vehicular communications. The dynamic nature of 5G vehicular scenarios and the complexity of directional mmWave communication calls for higher context-awareness and adaptability. To this aim, we propose an online learning algorithm addressing the problem of beam selection with environment-awareness in mmWave vehicular systems. In particular, we model this problem as a contextual multi-armed bandit problem. Next, we propose a lightweight context-aware online learning algorithm, namely fast machine learning (FML), with proven performance bound and guaranteed convergence. FML exploits coarse user location information and aggregates the received data to learn from and adapt to its environment. Furthermore, we demonstrate the feasibility of a real-world implementation of FML by proposing a standard-compliant protocol based on the existing architecture of cellular networks and the forthcoming features of 5G. We also perform an extensive evaluation using realistic traffic patterns derived from Google Maps. Our evaluation shows that FML enables mmWave base stations to achieve near-optimal performance on average within 33 mins of deployment by learning from the available context. Moreover, FML remains within ~ 5% of the optimal performance by swift adaptation to system changes (i.e., blockage, traffic).",,10.1109/TNET.2018.2869244,"Structural beams , 5G mobile communication , Base stations , Context modeling , Machine learning algorithms , Direction-of-arrival estimation , Array signal processing ",,,,,,,,,,
27,Deep Learning-Based mmWave Beam Selection for 5G NR/6G With Sub-6 GHz Channel Information: Algorithms and Prototype Validation,"Min Soo Sim , Yeon-Geun Lim , Sang Hyun Park , Linglong Dai , Chan-Byoung Chae ",IEEE Access,2169-3536,8,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9034044,"In fifth-generation (5G) communications, millimeter wave (mmWave) is one of the key technologies to increase the data rate. To overcome this technology’s poor propagation characteristics, it is necessary to employ a number of antennas and form narrow beams. It becomes crucial then, especially for initial access, to attain fine beam alignment between a next generation NodeB (gNB) and a user equipment (UE). The current 5G New Radio (NR) standard, however, adopts an exhaustive search-based beam sweeping, which causes time overhead of a half frame for initial beam establishment. In this paper, we propose a deep learning-based beam selection, which is compatible with the 5G NR standard. To select a mmWave beam, we exploit sub-6 GHz channel information. We introduce a deep neural network (DNN) structure and explain how we estimate a power delay profile (PDP) of a sub-6 GHz channel, which is used as an input of the DNN. We then validate its performance with real environment-based 3D ray-tracing simulations and over-the-air experiments with a mmWave prototype. Evaluation results confirm that, with support from the sub-6 GHz connection, the proposed beam selection reduces the beam sweeping overhead by up to 79.3 %.",,10.1109/ACCESS.2020.2980285,"5G mobile communication , Hidden Markov models , Channel models , Machine learning , Delays , Antenna arrays , Array signal processing ",,,,,,,,,,
28,DeepSlice: A Deep Learning Approach towards an Efficient and Reliable Network Slicing in 5G Networks,"Anurag Thantharate , Rahul Paropkari , Vijay Walunj , Cory Beard ","2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8993066,"Existing cellular communications and the upcoming 5G mobile network requires meeting high-reliability standards, very low latency, higher capacity, more security, and high-speed user connectivity. Mobile operators are looking for a programmable solution that will allow them to accommodate multiple independent tenants on the same physical infrastructure and 5G networks allow for end-to-end network resource allocation using the concept of Network Slicing (NS). Data-driven decision making will be vital in future communication networks due to the traffic explosion and Artificial Intelligence (AI) will accelerate the 5G network performance. In this paper, we have developed a `DeepSlice' model by implementing Deep Learning (DL) Neural Network to manage network load efficiency and network availability, utilizing in-network deep learning and prediction. We use available network Key Performance Indicators (KPIs) to train our model to analyze incoming traffic and predict the network slice for an unknown device type. Intelligent resource allocation allows us to use the available resources on existing network slices efficiently and offer load balancing. Our proposed DeepSlice model will be able to make smart decisions and select the most appropriate network slice, even in case of a network failure.",,10.1109/UEMCON47517.2019.8993066,,,,,,,,,,,
29,5G Vehicular Network Resource Management for Improving Radio Access Through Machine Learning,"Sahrish Khan Tayyaba , Hasan Ali Khattak , Ahmad Almogren , Munam Ali Shah , Ikram Ud Din , Ibrahim Alkhalifa , Mohsen Guizani ",IEEE Access,2169-3536,8,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8951149,"The current cellular technology and vehicular networks cannot satisfy the mighty strides of vehicular network demands. Resource management has become a complex and challenging objective to gain expected outcomes in a vehicular environment. The 5G cellular network promises to provide ultra-high-speed, reduced delay, and reliable communications. The development of new technologies such as the network function virtualization (NFV) and software defined networking (SDN) are critical enabling technologies leveraging 5G. The SDN-based 5G network can provide an excellent platform for autonomous vehicles because SDN offers open programmability and flexibility for new services incorporation. This separation of control and data planes enables centralized and efficient management of resources in a very optimized and secure manner by having a global overview of the whole network. The SDN also provides flexibility in communication administration and resource management, which are of critical importance when considering the ad-hoc nature of vehicular network infrastructures, in terms of safety, privacy, and security, in vehicular network environments. In addition, it promises the overall improved performance. In this paper, we propose a flow-based policy framework on the basis of two tiers virtualization for vehicular networks using SDNs. The vehicle to vehicle (V2V) communication is quite possible with wireless virtualization where different radio resources are allocated to V2V communications based on the flow classification, i.e., safety-related flow or non-safety flows, and the controller is responsible for managing the overall vehicular environment and V2X communications. The motivation behind this study is to implement a machine learning-enabled architecture to cater the sophisticated demands of modern vehicular Internet infrastructures. The inclination towards robust communications in 5G-enabled networks has made it somewhat tricky to manage network slicing efficiently. This paper also presents a proof of concept for leveraging machine learning-enabled resource classification and management through experimental evaluation of special-purpose testbed established in custom mininet setup. Furthermore, the results have been evaluated using Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN). While concluding the paper, it is shown that the LSTM has outperformed the rest of classification techniques with promising results.",,10.1109/ACCESS.2020.2964697,"Resource management , 5G mobile communication , Computer architecture , Wireless communication , Machine learning , Ad hoc networks , Communication system security ",,,,,,,,,,
30,When Machine Learning Meets Privacy in 6G: A Survey,"Yuanyuan Sun , Jiajia Liu , Jiadai Wang , Yurui Cao , Nei Kato ",IEEE Communications Surveys & Tutorials,2373-745X,22,4,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9146540,"The rapid-developing Artificial Intelligence (AI) technology, fast-growing network traffic, and emerging intelligent applications (e.g., autonomous driving, virtual reality, etc.) urgently require a new, faster, more reliable and flexible network form. At this time, researchers in both industry and academia have turned their attention to the sixth generation (6G) communication networks. In the 6G vision, various intelligent application scenarios that utilize Machine Learning (ML) technology (the most important branch of AI) will bring rich heterogeneous connections, as well as massive information storage and operations. When ML meets 6G, new opportunities will emerge along with numerous privacy challenges. On one hand, a secure ML structure, or the correct application of ML, can protect privacy in 6G. On the other hand, ML may be attacked or abused, resulting in privacy violation. It is worth noting that the alliance between 6G and ML may also be a double-edged sword in many cases, rather than absolutely infringe or protect privacy. Therefore, based on lots of existing meaningful works, this paper aims to provide a comprehensive survey of ML and privacy in 6G, with a view to further promoting the development of 6G and privacy protection technologies.",,10.1109/COMST.2020.3011561,"6G mobile communication , Data privacy , Big Data , MIMO communication , Tutorials , Machine learning ",,,,,,,,,,
31,Artificial Intelligence to Manage Network Traffic of 5G Wireless Networks,"Yu Fu , Sen Wang , Cheng-Xiang Wang , Xuemin Hong , Stephen McLaughlin ",IEEE Network,1558-156X,32,6,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8553655,"The deployment of 5G wireless communication systems is projected to begin in 2020. With new scenarios, new technologies, and new network architectures, the traffic management for 5G networks will present significant technical challenges. In recent years, AI technologies, especially ML technologies, have demonstrated significant success in many application domains, suggesting their potential to help solve the problem of 5G traffic management. In this article, we investigate the new characteristics of 5G wireless network traffic and discuss challenges they present for 5G traffic management. Potential solutions and research directions for the management of 5G traffic, including distributed and lightweight ML algorithms and a novel AI assistant content retrieval algorithm framework, are discussed.",,10.1109/MNET.2018.1800115,"5G mobile communication , Telecommunication traffic , Networked control systems , Traffic control , Supervised learning , Wireless networks , Network architecture , Artificial intelligence , Content management ",,,,,,,,,,
32,Deep Learning Based Pilot Allocation Scheme (DL-PAS) for 5G Massive MIMO System,"Kwihoon Kim , Joohyung Lee , Junkyun Choi ",IEEE Communications Letters,2373-7891,22,4,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8283585,"This letter proposes a deep learning-based pilot assignment scheme (DL-PAS) for a massive multiple-input multiple-output (massive MIMO) system that utilizes a large number of antennas for multiple users. The proposed DL-PAS improves the performance in cellular networks with severe pilot contamination by learning the relationship between pilot assignment and the users' location pattern. In this letter, we design a novel supervised learning method, where input features and output labels are users' locations in all cells and pilot assignments, respectively. Specifically, pretrained optimal pilot assignments with given users' locations are provided through an exhaustive search method as the training data. Then, the proposed DL-PAS provides a near-optimal pilot assignment from the produced inferred function by analyzing the training data. We implement the proposed scheme using a commercial deep multilayer perceptron system. Simulation-based experiments show that the proposed scheme achieves almost 99.38% theoretical upper-bound performance with low complexity, requiring only 0.92-ms computational time.",,10.1109/LCOMM.2018.2803054,"Machine learning , MIMO communication , Base stations , Antennas , Contamination , Interference , Resource management ",,,,,,,,,,
33,Machine Learning for Intelligent Authentication in 5G and Beyond Wireless Networks,"He Fang , Xianbin Wang , Stefano Tomasin ",IEEE Wireless Communications,1558-0687,26,5,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8883130,"The 5G and beyond wireless networks are critical to support diverse vertical applications by connecting heterogeneous devices and machines, which directly increase vulnerability for various spoofing attacks. Conventional cryptographic and physical layer authentication techniques are facing some challenges in complex dynamic wireless environments, including significant security overhead, low reliability, as well as difficulties in pre-designing a precise authentication model, providing continuous protection, and learning time-varying attributes. In this article, we envision new authentication approaches based on machine learning techniques by opportunistically leveraging physical layer attributes, and introduce intelligence to authentication for more efficient security provisioning. Machine learning paradigms for intelligent authentication design are presented, namely for parametric/non-parametric and supervised/ unsupervised/reinforcement learning algorithms. In a nutshell, the machine-learning-based intelligent authentication approaches utilize specific features in the multi-dimensional domain for achieving cost-effective, more reliable, model-free, continuous, and situation-aware device validation under unknown network conditions and unpredictable dynamics.",,10.1109/MWC.001.1900054,"Authentication , 5G mobile communication , Machine learning , Physical layer security , Wireless networks , Learning systems ",,,,,,,,,,
34,Deep-Learning-Empowered Digital Forensics for Edge Consumer Electronics in 5G HetNets,"Feng Ding , Guopu Zhu , Mamoun Alazab , Xiangjun Li , Keping Yu ",IEEE Consumer Electronics Magazine,2162-2256,11,2,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9309361,"The upcoming 5G heterogeneous networks (HetNets) have attracted much attention worldwide. Large amounts of high-velocity data can be transported by using the bandwidth spectrum of HetNets, yielding both great benefits and several concerning issues. In particular, great harm to our community could occur if the main visual information channels, such as images and videos, are maliciously attacked and uploaded to the Internet, where they can be spread quickly. Therefore, we propose a novel framework as a digital forensics tool to protect end users. It is built based on deep learning and can realize the detection of attacks via classification. Compared with the conventional methods and justified by our experiments, the data collection efficiency, robustness, and detection performance of the proposed model are all refined. In addition, assisted by 5G HetNets, our proposed framework makes it possible to provide high-quality real-time forensics services on edge consumer devices such as cell phone and laptops, which brings colossal practical value. Some discussions are also carried out to outline potential future threats.",,10.1109/MCE.2020.3047606,"Forensics , Digital forensics , 5G mobile communication , Detectors , Deep learning , Consumer electronics ",,,,,,,,,,
35,Intelligent wireless communications enabled by cognitive radio and machine learning,"Xiangwei Zhou , Mingxuan Sun , Geoffrey Ye Li , Biing-Hwang Fred Juang ",China Communications,1673-5447,15,12,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594714,"The ability to intelligently utilize resources to meet the need of growing diversity in services and user behavior marks the future of wireless communication systems. Intelligent wireless communications aims at enabling the system to perceive and assess the available resources, to autonomously learn to adapt to the perceived wireless environment, and to reconfigure its operating mode to maximize the utility of the available resources. The perception capability and reconfigurability are the essential features of cognitive radio while modern machine learning techniques project great potential in system adaptation. In this paper, we discuss the development of the cognitive radio technology and machine learning techniques and emphasize their roles in improving spectrum and energy utility of wireless communication systems. We describe the state-of-the-art of relevant techniques, covering spectrum sensing and access approaches and powerful machine learning algorithms that enable spectrum and energy-efficient communications in dynamic wireless environments. We also present practical applications of these techniques and identify further research challenges in cognitive radio and machine learning as applied to the existing and future wireless communication systems.",,10.12676/j.cc.2018.12.002,"Sensors , Cognitive radio , Wireless sensor networks , Machine learning , Wideband , Energy efficiency ",,,,,,,,,,
36,A Machine Learning Approach to 5G Infrastructure Market Optimization,"Dario Bega , Marco Gramaglia , Albert Banchs , Vincenzo Sciancalepore , Xavier Costa-Pérez ",IEEE Transactions on Mobile Computing,2161-9875,19,3,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8632676,"It is now commonly agreed that future 5G Networks will build upon the network slicing concept. The ability to provide virtual, logically independent “slices” of the network will also have an impact on the models that will sustain the business ecosystem. Network slicing will open the door to new players: the infrastructure provider, which is the owner of the infrastructure, and the tenants, which may acquire a network slice from the infrastructure provider to deliver a specific service to their customers. In this new context, how to correctly handle resource allocation among tenants and how to maximize the monetization of the infrastructure become fundamental problems that need to be solved. In this paper, we address this issue by designing a network slice admission control algorithm that (i) autonomously learns the best acceptance policy while (ii) it ensures that the service guarantees provided to tenants are always satisfied. The contributions of this paper include: (i) an analytical model for the admissibility region of a network slicing-capable 5G Network, (ii) the analysis of the system (modeled as a Semi-Markov Decision Process) and the optimization of the infrastructure providers revenue, and (iii) the design of a machine learning algorithm that can be deployed in practical settings and achieves close to optimal performance.",,10.1109/TMC.2019.2896950,"5G mobile communication , Network slicing , Machine learning , Admission control , Biological system modeling , Machine learning algorithms , Optimization ",,,,,,,,,,
37,Deep Learning-Based DDoS-Attack Detection for Cyber–Physical System Over 5G Network,"Bilal Hussain , Qinghe Du , Bo Sun , Zhiqiang Han ",IEEE Transactions on Industrial Informatics,1941-0050,17,2,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9000893,"With the advent of 5G, cyber–physical systems (CPSs) employed in the vertical industries and critical infrastructures will depend on the cellular network more than ever; making their attack surface wider. Hence, guarding the network against cyberattacks is critical not only for its primary subscribers but to prevent it from being exploited as a proxy to attack CPSs. In this article, we propose a consolidated framework, by utilizing deep convolutional neural networks (CNNs) and real network data, to provide early detection for distributed denial-of-service (DDoS) attacks orchestrated by a botnet that controls malicious devices. These puppet devices individually perform silent call, signaling, SMS spamming, or a blend of these attacks targeting call, Internet, SMS, or a blend of these services, respectively, to cause a collective DDoS attack in a cell that can disrupt CPSs’ operations. Our results demonstrate that our framework can achieve higher than $91\%$ normal and underattack cell detection accuracy.",,10.1109/TII.2020.2974520,"Computer crime , 5G mobile communication , Computer architecture , Cellular networks , Microprocessors , Performance evaluation , Unsolicited electronic mail ",,,,,,,,,,
38,Machine Learning Techniques for 5G and Beyond,"Jasneet Kaur , M. Arif Khan , Mohsin Iftikhar , Muhammad Imran , Qazi Emad Ul Haq ",IEEE Access,2169-3536,9,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9321326,"Wireless communication systems play a very crucial role in modern society for entertainment, business, commercial, health and safety applications. These systems keep evolving from one generation to next generation and currently we are seeing deployment of fifth generation (5G) wireless systems around the world. Academics and industries are already discussing beyond 5G wireless systems which will be sixth generation (6G) of the evolution. One of the main and key components of 6G systems will be the use of Artificial Intelligence (AI) and Machine Learning (ML) for such wireless networks. Every component and building block of a wireless system that we currently are familiar with from our knowledge of wireless technologies up to 5G, such as physical, network and application layers, will involve one or another AI/ML techniques. This overview paper, presents an up-to-date review of future wireless system concepts such as 6G and role of ML techniques in these future wireless systems. In particular, we present a conceptual model for 6G and show the use and role of ML techniques in each layer of the model. We review some classical and contemporary ML techniques such as supervised and un-supervised learning, Reinforcement Learning (RL), Deep Learning (DL) and Federated Learning (FL) in the context of wireless communication systems. We conclude the paper with some future applications and research challenges in the area of ML and AI for 6G networks.",,10.1109/ACCESS.2021.3051557,"6G mobile communication , 5G mobile communication , Artificial intelligence , Wireless networks , Resource management , Data models , Solid modeling ",,,,,,,,,,
39,"EdgeAI: A vision for distributed, edge-native artificial intelligence in future 6G networks","L Lovén, T Leppänen, E Peltonen, J Partala, ...",The 1st 6G wireless …,,,,,2019,jultika.oulu.fi,http://jultika.oulu.fi/files/nbnfi-fe2019050314180.pdf,"Edge computing, a key part of the upcoming 5G mobile networks and future 6G technologies, promises to distribute cloud applications while providing more bandwidth and reducing latencies [1]. The promises are delivered by moving application-specific computations between the cloud, the data producing devices, and the network infrastructure components at the edges of wireless and fixed networks. In stark contrast, current artificial intelligence (AI) and in particular machine learning (ML) methods assume computations are conducted in a homogeneous cloud with ample computational and data storage resources available. Currently, AI’s cloud-centric architectural model requires transmitting data from the end-user devices to the cloud, consuming significant data transmission resources and introducing latencies.",,,,,,,,,,,,,
40,Deep Learning for Ultra-Reliable and Low-Latency Communications in 6G Networks,"Changyang She , Rui Dong , Zhouyou Gu , Zhanwei Hou , Yonghui Li , Wibowo Hardjawana , Chenyang Yang , Lingyang Song , Branka Vucetic ",IEEE Network,1558-156X,34,5,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9136591,"In future 6th generation networks, URLLC will lay the foundation for emerging mission-critical applications that have stringent requirements on end-to-end delay and reliability. Existing works on URLLC are mainly based on theoretical models and assumptions. The model-based solutions provide useful insights, but cannot be directly implemented in practice. In this article, we first summarize how to apply data-driven supervised deep learning and deep reinforcement learning in URLLC, and discuss some open problems of these methods. To address these open problems, we develop a multi-level architecture that enables device intelligence, edge intelligence, and cloud intelligence for URLLC. The basic idea is to merge theoretical models and realworld data in analyzing the latency and reliability and training deep neural networks (DNNs). Deep transfer learning is adopted in the architecture to fine-tune the pre-trained DNNs in non-stationary networks. Further considering that the computing capacity at each user and each mobile edge computing server is limited, federated learning is applied to improve the learning efficiency. Finally, we provide some experimental and simulation results and discuss some future directions.",,10.1109/MNET.011.1900630,"Ultra reliable low latency communication , Deep learning , Computer architecture , 6G mobile communication , Quality of service , Delays , Training ",,,,,,,,,,
41,Machine Learning at the Edge: A Data-Driven Architecture With Applications to 5G Cellular Networks,"Michele Polese , Rittwik Jana , Velin Kounev , Ke Zhang , Supratim Deb , Michele Zorzi ",IEEE Transactions on Mobile Computing,2161-9875,20,12,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9107476,"The fifth generation of cellular networks (5G) will rely on edge cloud deployments to satisfy the ultra-low latency demand of future applications. In this paper, we argue that such deployments can also be used to enable advanced data-driven and Machine Learning (ML) applications in mobile networks. We propose an edge-controller-based architecture for cellular networks and evaluate its performance with real data from hundreds of base stations of a major U.S. operator. In this regard, we will provide insights on how to dynamically cluster and associate base stations and controllers, according to the global mobility patterns of the users. Then, we will describe how the controllers can be used to run ML algorithms to predict the number of users in each base station, and a use case in which these predictions are exploited by a higher-layer application to route vehicular traffic according to network Key Performance Indicators (KPIs). We show that the prediction accuracy improves when based on machine learning algorithms that rely on the controllers’ view and, consequently, on the spatial correlation introduced by the user mobility, with respect to when the prediction is based only on the local data of each single base station.",,10.1109/TMC.2020.2999852,"5G mobile communication , Cellular networks , Base stations , Machine learning algorithms , Machine learning , Computer architecture , Clustering algorithms ",,,,,,,,,,
42,"Artificial Intelligence for 5G and Beyond 5G: Implementations, Algorithms, and Optimizations","Chuan Zhang , Yeong-Luh Ueng , Christoph Studer , Andreas Burg ",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,2156-3365,10,2,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9115947,"This Special Issue of the IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS) is dedicated to demonstrating the latest research progress on artificial intelligence for 5G and beyond 5G (B5G) with respect to implementations, algorithms, and optimizations.",,10.1109/JETCAS.2020.2999944,"Special issues and sections , Wireless communication , Artificial intelligence , Hardware , Wireless sensor networks , 5G mobile communication ",,,,,,,,,,
43,FPGA Realization of a Reversible Data Hiding Scheme for 5G MIMO-OFDM System by Chaotic Key Generation-Based Paillier Cryptography Along with LDPC and Its Side Channel Estimation Using Machine Learning Technique,"Shajin, Francis H. and Rajesh, P.","Journal of Circuits, Systems and Computers",0218-1266,31,5,2250093,2021,World Scientific Publishing Co.,https://doi.org/10.1142/S0218126622500931,"Multiple-Input and Multiple-Output (MIMO) technology is a significant and timely subject, which is highly motivated by the needs of 5G wireless communications. Data transmission performs MIMO, which is highly sensitive. There are several security issues while transmitting the data such as loss of data and code injection. Two efficient methods are Encryption and Data Hiding protection of data in wireless communication. This dissertation suggests FPGA Implementation of RDHS by Chaotic Key Generation-Based Paillier Cryptography with LDPC using machine learning technique. RDHS stands for Reversible Data Hiding Scheme. In a reversible method, the initial stage of preprocessing is to shrink the histogram of image before the process of encryption. Hence, the plaintext domain changing the encrypted images to data embedding cannot result from any pixel repletion. A little distortion data embedding may be taken as the original image may recover the directly decrypted image. Here, the performance metrics of throughput, area consumed, latency, delay, packet delivery, network life and overhead are calculated. The proposed Paillier homomorphic cryptosystem proposes higher network throughput as 99%, higher network life 98%, lower delay rate as 60%, packet delivery as 74%, overhead as 66%, latency as 55% and area consumed as 61% with the existing method such as McEliece, Elgamal and Elliptic curve cryptosystem in the security analysis of the proposed method providing decryption time 94% and encryption time 98% better than the existing method.
Multiple-Input and Multiple-Output (MIMO) technology is a significant and timely subject, which is highly motivated by the needs of 5G wireless communications. Data transmission performs MIMO, which is highly sensitive. There are several security issues while transmitting the data such as loss of data and code injection. Two efficient methods are Encryption and Data Hiding protection of data in wireless communication. This dissertation suggests FPGA Implementation of RDHS by Chaotic Key Generation-Based Paillier Cryptography with LDPC using machine learning technique. RDHS stands for Reversible Data Hiding Scheme. In a reversible method, the initial stage of preprocessing is to shrink the histogram of image before the process of encryption. Hence, the plaintext domain changing the encrypted images to data embedding cannot result from any pixel repletion. A little distortion data embedding may be taken as the original image may recover the directly decrypted image. Here, the performance metrics of throughput, area consumed, latency, delay, packet delivery, network life and overhead are calculated. The proposed Paillier homomorphic cryptosystem proposes higher network throughput as 99%, higher network life 98%, lower delay rate as 60%, packet delivery as 74%, overhead as 66%, latency as 55% and area consumed as 61% with the existing method such as McEliece, Elgamal and Elliptic curve cryptosystem in the security analysis of the proposed method providing decryption time 94% and encryption time 98% better than the existing method.",doi: 10.1142/S0218126622500931,10.1142/S0218126622500931,,,,,,,,,,,
44,… scheme for 5G MIMO-OFDM system by chaotic key generation-based paillier cryptography along with LDPC and its side channel estimation using machine learning …,"FH Shajin, P Rajesh","Journal of Circuits, Systems and Computers",,,,,2022,World Scientific,https://www.worldscientific.com/doi/abs/10.1142/S0218126622500931,… A machine learning attack on text encrypted … the machine learning-based attack is good accuracy. The goal is to estimate the side channel using machine learning technique and FPGA …,,,,,,,,,,,,,
45,An optimal multitier resource allocation of cloud RAN in 5G using machine learning,"AK Bashir, R Arul, S Basheer, G Raja, ...",Transactions on …,2161-3915,30,8,e3627,2019,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.3627?casa_token=v3fhRzYH6l4AAAAA:QNgDYJDg--xTSLIMXU8pTONw9d_jW2HQtSq2MqrrXv9-3f5VjdguddyjpsmIKfyIZHdUjmnLQTGRbHNI,"The networks are evolving drastically since last few years in order to meet user requirements. For example, the 5G is offering most of the available spectrum under one umbrella. In this work, we will address the resource allocation problem in fifth-generation (5G) networks, to be exact in the Cloud Radio Access Networks (C-RANs). The radio access network mechanisms involve multiple network topologies that are isolated based on the spectrum bands and it should be enhanced with numerous access technology in the deployment of 5G network. The C-RAN is one of the optimal technique to combine all the available spectral bands. However, existing C-RAN mechanisms lacks the intelligence perspective on choosing the spectral bands. Thus, C-RAN mechanism requires an advanced tool to identify network topology to allocate the network resources for substantial traffic volumes. Therefore, there is a need to propose a framework that handles spectral resources based on user requirements and network behavior. In this work, we introduced a new C-RAN architecture modified as multitier Heterogeneous Cloud Radio Access Networks in a 5G environment. This architecture handles spectral resources efficiently. Based on the simulation analysis, the proposed multitier H-CRAN architecture with improved control unit in network management perspective enables augmented granularity, end-to-end optimization, and guaranteed quality of service by 15 percentages over the existing system.",,10.1002/ett.3627,,,,,,,,,,,
46,A survey of 5G network systems: challenges and machine learning approaches,"H Fourati, R Maaloul, L Chaari",International Journal of Machine Learning …,1868-808X,12,,385–431,2021,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s13042-020-01178-4&casa_token=LhWVrpkr9dsAAAAA:dITj5LY0Prd2BRvABZDlfQk-UWvzfDARiLtnudClJODxh3qnfDt2vir2L1F2ko167ICqUjX_QmkfS9Zc0Q,"5G cellular networks are expected to be the key infrastructure to deliver the emerging services. These services bring new requirements and challenges that obstruct the desired goal of forthcoming networks. Mobile operators are rethinking their network design to provide more flexible, dynamic, cost-effective and intelligent solutions. This paper starts with describing the background of the 5G wireless networks then we give a deep insight into a set of 5G challenges and research opportunities for machine learning (ML) techniques to manage these challenges. The first part of the paper is devoted to overview the fifth-generation of cellular networks, explaining its requirements as well as its key technologies, their challenges and its forthcoming architecture. The second part is devoted to present a basic overview of ML techniques that are nowadays applied to cellular networks. The last part discusses the most important related works which propose ML solutions in order to overcome 5G challenges.  ",,10.1007/s13042-020-01178-4,"5G cellular network, 5G services, 5G key technologies,  5G architectures, 5G challenges, ML solutions, Intelligence",,,,,,,,,,
47,Deep learning at the mobile edge: Opportunities for 5G networks,"M McClellan, C Cervelló-Pastor, S Sallent",Applied Sciences,2076-3417,10,14,4735,2020,mdpi.com,https://www.mdpi.com/764806,"Mobile edge computing (MEC) within 5G networks brings the power of cloud computing, storage, and analysis closer to the end user. The increased speeds and reduced delay enable novel applications such as connected vehicles, large-scale IoT, video streaming, and industry robotics. Machine Learning (ML) is leveraged within mobile edge computing to predict changes in demand based on cultural events, natural disasters, or daily commute patterns, and it prepares the network by automatically scaling up network resources as needed. Together, mobile edge computing and ML enable seamless automation of network management to reduce operational costs and enhance user experience. In this paper, we discuss the state of the art for ML within mobile edge computing and the advances needed in automating adaptive resource allocation, mobility modeling, security, and energy efficiency for 5G networks.",,10.3390/app10144735,"5G,  edge network, deep learning, reinforcement learning, caching, task offloading, mobile computing, edge computing, mobile edge, computing, cloud computing, network function virtualization, slicing, 5G network standardization",,,,,,,,,,
48,Consideration On Automation of 5G Network Slicing with Machine Learning,"Ved P. Kafle , Yusuke Fukushima , Pedro Martinez-Julia , Takaya Miyazawa ",2018 ITU Kaleidoscope: Machine Learning for a 5G Future (ITU K),,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8597639,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.",,10.23919/ITU-WT.2018.8597639,"Machine learning , Automation , 5G mobile communication , Network slicing , Monitoring , Security , Resource management ",,,,,,,,,,
49,Ultra-reliable MU-MIMO detector based on deep learning for 5G/B5G-enabled IoT,"K He, Z Wang, D Li, F Zhu, L Fan",Physical Communication,1874-4907,43,,,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S1874490720302585?casa_token=DGfKJYBBkJ8AAAAA:hzGCvg50jNJ5JMwtCy8sTWbyVYTrBpkaLwqZqjR9sRfRmlo0pZkPw3rLEcNnuxPpvN0z6tQ9yXg,"In this paper, we propose an ultra-reliable multiuser multiple-input multiple-output (MU-MIMO) detector based on deep learning for the fifth-generation and beyond the fifth-generation (5G/B5G) enabled Internet of Things (IoT), where the system is operating in interfering environments correlated over the time or frequency domain. For this system, we employ an iterative detection framework of a conventional symbol-by-symbol detector and a deep convolutional neural network (DCNN), where the DCNN is used to suppress the interfering signals by capturing the characteristics through deep learning. The conventional detector in the framework can be either ZF-MLD or MMSE-MLD, where the conventional zero-forcing (ZF) or minimum mean square error (MMSE) is initially used and then the near-by signal candidates are searched through the maximum likelihood detection (MLD). Thus, the proposed MU-MIMO detector can suppress the influence of the correlated interferences with low computational complexity and finally improve the reliability of the practical MU-MIMO systems in the presence of correlated interferences. To further enhance the system detection performance, user scheduling is employed, where several user selection criteria are proposed to choose one best user among multiple ones. Simulation results are finally presented to show that an ultra-reliable detection performance can be achieved for the 5G/B5G-enabled IoT.",,10.1016/j.phycom.2020.101181,"Deep learning, Correlated Interference, Signal detection, Wireless communication, IoT, 5G",,,,,,,,,,
50,Blockchain-Based Data Security for Artificial Intelligence Applications in 6G Networks,"Weiwei Li , Zhou Su , Ruidong Li , Kuan Zhang , Yuntao Wang ",IEEE Network,1558-156X,34,6,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9277898,"The sixth generation (6G) networks are expected to provide a fully connected world with terrestrial wireless and satellite communications integration. The design concept of 6G networks is to leverage artificial intelligence (Ai) to promote the intelligent and agile development of network services. intelligent services inevitably involve the processing of large amounts of data, such as storage, computing, and analysis, such that the data may be vulnerable to tampering or contamination by attackers. in this article, we propose a blockchain-based data security scheme for Ai applications in 6G networks. Specifically, we first introduce the 6G architecture (i.e., a space-air-ground-underwater integrated network). Then we discuss two Ai-enabled applications, indoor positioning and autonomous vehicle, in the context of 6G. Through a case study of an indoor navigation system, we demonstrate the effectiveness of blockchain in data security. The integration of Ai and blockchain is developed to evaluate and optimize the quality of intelligent service. Finally, we discuss several open issues about data security in the upcoming 6G networks.",,10.1109/MNET.021.1900629,"6G mobile communication , Artificial intelligence , Autonomous vehicles , Servers , Satellite broadcasting , Blockchain , Wireless communication ",,,,,,,,,,
51,Deep Learning Based Massive MIMO Beamforming for 5G Mobile Network,"Taras Maksymyuk , Juraj Gazda , Oleh Yaremko , Denys Nevinskiy ",2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8525802,"The rapid increasing of the data volume in mobile networks forces operators to look into different options for capacity improvement. Thus, modern 5G networks became more complex in terms of deployment and management. Therefore, new approaches are needed to simplify network design and management by enabling self-organizing capabilities. In this paper, we propose a novel intelligent algorithm for performance optimization of the massive MIMO beamforming. The key novelty of the proposed algorithm is in the combination of three neural networks which cooperatively implement the deep adversarial reinforcement learning workflow. In the proposed system, one neural network is trained to generate realistic user mobility patterns, which are then used by second neural network to produce relevant antenna diagram. Meanwhile, third neural network estimates the efficiency of the generated antenna diagram returns corresponding reward to both networks. The advantage of the proposed approach is that it leans by itself and does not require large training datasets.",,10.1109/IDAACS-SWS.2018.8525802,"MIMO communication , Array signal processing , Antenna arrays , 5G mobile communication , Throughput , Spectral efficiency ",,,,,,,,,,
52,Optimal 5G network slicing using machine learning and deep learning concepts,"MH Abidi, H Alkhalefah, K Moiduddin, M Alazab, ...",Computer Standards & …,0920-5489,76,,103518,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S0920548921000131?casa_token=aJN0oGv0UrkAAAAA:OdkkvTEssV7SGalngJTqKGGaU7PdvV-tzQ4Zci1rPpp_m60tD7k1qatOjPb3fQIdqOK-XtnKw-o,"Network slicing is predetermined to hold up the diversity of emerging applications with enhanced performance and flexibility requirements in the way of splitting the physical network into numerous logical networks. Consequently, a tremendous data count has been generated with an enormous number of mobile phones due to these applications. This has made remarkable challenges and has a considerable influence on the network slicing performance. This work aims to design an efficient network slicing using a hybrid learning algorithm. Thus, we proposed a model, which involves three main phases: (a) Data collection, (b) Optimal weighted feature extraction (OWFE), and (c) Slicing classification. First, we collected the 5G network slicing dataset, which involves the attributes associated with various network devices like “user device type, duration, packet loss ratio, packet delay budget, bandwidth, delay rate, speed, jitter, and modulation type.” Next, we performed the OWFE, in which a weight function is multiplied with the attribute values to have high scale variation. We optimized this weight function by the hybridization of two meta-heuristic algorithms—glowworm swarm optimization and deer hunting optimization algorithm (DHOA)—and named the proposed model glowworm swarm-based DHOA (GS-DHOA). For the given attributes, we classified the exact network slices like “eMBB, mMTC, and URLLC” for each device by a hybrid classifier using deep belief and neural networks. The weight function of both networks is optimized by the GS-DHOA. The experiment results revealed that the proposed model could influence the provision of accurate 5G network slicing.",,10.1016/j.csi.2021.103518,"Network Slicing, Deep Learning, Neural Network, Deep Belief Network, Optimal Weighted Feature Extraction, Glowworm Swarm-Deer Hunting Optimization Algorithm",,,,,,,,,,
53,Machine learning‐based IDS for software‐defined 5G network,"J Li, Z Zhao, R Li",Iet Networks,2047-4954,7,2,53–60,2018,Wiley Online Library,https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-net.2017.0212,"As an inevitable trend of future fifth generation (5G) networks, software-defined architecture has many advantages in providing centralised control and flexible resource management. However, it is also confronted with various security challenges and potential threats with emerging services and technologies. As the focus of network security, intrusion detection systems (IDSs) are usually deployed separately without collaboration. They are also unable to detect novel attacks with limited intelligent abilities, which are hard to meet the needs of software-defined 5G. In this study, the authors propose an intelligent IDS taking the advances in software-defined technology and artificial intelligence based on software-defined 5G architecture. It flexibly integrates security function modules which are adaptively invoked under centralised management and control with a global view. It can also deal with unknown intrusions by using machine learning algorithms. Evaluation results prove that the intelligent IDS achieves better performance with lower overhead. It is also verified that the selected machine learning algorithms show better accuracy and reduced false alarm rate in flow-based classification.",,10.1049/iet-net.2017.0212,,,,,,,,,,,
54,An efficient deep learning model for intrusion classification and prediction in 5G and IoT networks,"Shahadate Rezvy , Yuan Luo , Miltos Petridis , Aboubaker Lasebae , Tahmina Zebin ",2019 53rd Annual Conference on Information Sciences and Systems (CISS),,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8693059,"A Network Intrusion Detection System is a critical component of every internet-connected system due to likely attacks from both external and internal sources. Such Security systems are used to detect network born attacks such as flooding, denial of service attacks, malware, and twin-evil intruders that are operating within the system. Neural networks have become an increasingly popular solution for network intrusion detection. Their capability of learning complex patterns and behaviors make them a suitable solution for differentiating between normal traffic and network attacks. In this paper, we have applied a deep autoencoded dense neural network algorithm for detecting intrusion or attacks in 5G and IoT network. We evaluated the algorithm with the benchmark Aegean Wi-Fi Intrusion dataset. Our results showed an excellent performance with an overall detection accuracy of 99.9% for Flooding, Impersonation and Injection type of attacks. We also presented a comparison with recent approaches used in literature which showed a substantial improvement in terms of accuracy and speed of detection with the proposed algorithm.",,10.1109/CISS.2019.8693059,"Deep learning , Neural networks , Intrusion detection , Feature extraction , Training , Internet of Things , Wireless networks ",,,,,,,,,,
55,Edge Cloud Server Deployment With Transmission Power Control Through Machine Learning for 6G Internet of Things,"Tiago Koketsu Rodrigues , Katsuya Suto , Nei Kato ",IEEE Transactions on Emerging Topics in Computing,2376-4562,9,4,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8946743,"Cloud computing is an important technology for bringing a big pool of elastic resources to client devices. Their main drawback has long been the long distance between users and servers, but this has been remedied by Edge Cloud Computing, where the cloud servers are located in the network edge. Edge Cloud Computing is regarded as essential for future networks and consequently, there is plenty of research on how to optimize its operation. However, the vast majority of them ignore the decision of where the edge servers should be deployed, despite how severely this can affect the performance of the system. Furthermore, future networks must also deal with massive amounts of clients and servers, such as the ones characteristic of the Internet of Things and 6G Networks. This demands solutions that are scalable. Given these two points, we propose a Machine Learning-based server deployment policy in 6G Internet of Things environments. Our solution is proven to approach optimality while being feasible. Furthermore, we also prove that our proposal leads to lower latency and higher resource efficiency than conventional Edge Cloud Computing server deployment solutions.",,10.1109/TETC.2019.2963091,"Cloud computing , Servers , 6G mobile communication , Base stations , Edge computing , Computational modeling , Machine learning ",,,,,,,,,,
56,Artificial intelligence‐enabled sensing technologies in the 5G/internet of things era: from virtual reality/augmented reality to the digital twin,"Z Zhang, F Wen, Z Sun, X Guo, T He, ...",Advanced Intelligent …,2640-4567,4,7,2100228,2022,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202100228,"With the development of 5G and Internet of Things (IoT), the era of big data-driven product design is booming. In addition, artificial intelligence (AI) is also emerging and evolving by recent breakthroughs in computing power and software architectures. In this regard, the digital twin, analyzing various sensor data with the help of AI algorithms, has become a cutting-edge technology that connects the physical and virtual worlds, in which the various sensors are highly desirable to collect environmental information. However, although existing sensor technologies, including cameras, microphones, inertial measurement units, etc., are widely used as sensing elements for various applications, high-power consumption and battery replacement of them is still a problem. Triboelectric nanogenerators (TENGs) as self-powered sensors supply a feasible platform for realizing self-sustainable and low-power systems. Herein, the recent progress on TENG-based intelligent systems, that is, wearable electronics, robot-related systems, and smart homes, followed by prospective future development enabled by sensor fusion technology, is focused on. Finally, how to apply artificial intelligence to the design of intelligent sensor systems for the 5G and IoT era is discussed.",,10.1002/aisy.202100228,,,,,,,,,,,
57,Distributed artificial intelligence-as-a-service (DAIaaS) for smarter IoE and 6G environments,"N Janbi, I Katib, A Albeshri, R Mehmood",Sensors,1424-8220,20,20,5796,2020,mdpi.com,https://www.mdpi.com/854906,"Artificial intelligence (AI) has taken us by storm, helping us to make decisions in everything we do, even in finding our “true love” and the “significant other”. While 5G promises us high-speed mobile internet, 6G pledges to support ubiquitous AI services through next-generation softwarization, heterogeneity, and configurability of networks. The work on 6G is in its infancy and requires the community to conceptualize and develop its design, implementation, deployment, and use cases. Towards this end, this paper proposes a framework for Distributed AI as a Service (DAIaaS) provisioning for Internet of Everything (IoE) and 6G environments. The AI service is “distributed” because the actual training and inference computations are divided into smaller, concurrent, computations suited to the level and capacity of resources available with cloud, fog, and edge layers. Multiple DAIaaS provisioning configurations for distributed training and inference are proposed to investigate the design choices and performance bottlenecks of DAIaaS. Specifically, we have developed three case studies (e.g., smart airport) with eight scenarios (e.g., federated learning) comprising nine applications and AI delivery models (smart surveillance, etc.) and 50 distinct sensor and software modules (e.g., object tracker). The evaluation of the case studies and the DAIaaS framework is reported in terms of end-to-end delay, network usage, energy consumption, and financial savings with recommendations to achieve higher performance. DAIaaS will facilitate standardization of distributed AI provisioning, allow developers to focus on the domain-specific details without worrying about distributed training and inference, and help systemize the mass-production of technologies for smarter environments.",,10.3390/s20205796," internet of everything (IoE), 6th generation, (6G) networks, artificial intelligence, Distributed AI as a Service (DAIaaS), fog computing, edge computing, cloud computing, smart airport, smart districts",,,,,,,,,,
58,Enhanced Deployment Strategy for the 5G Drone-BS Using Artificial Intelligence,"Fadi Al-Turjman , Joel Poncha Lemayian , Sinem Alturjman , Leonardo Mostarda ",IEEE Access,2169-3536,7,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8732936,"The use of drones to perform various task has recently gained a lot of attention. Drones have been used by traders to deliver goods to customers, scientists, and researchers to observe and search for endangered species, and by the military during critical operations. The flexibility of drones in remote controlling makes them ideal candidates to perform critical tasks with minimum time and cost. In this paper, we use drones to setup base stations that provide 5G cellular coverage over a given area in danger. The aim of this paper is to determine the optimum number of drones and their optimum location, such that each point in the selected area is covered with the least cost while considering communication relevant parameters such as data rate, latency, and throughput. The problem is mathematically modeled by forming linear optimization equations. For fast optimized solutions, genetic algorithm (GA) and simulated annealing (SA) algorithms are provisionally employed to solve the problem, and the results are accordingly compared. Using these two meta-heuristic methods, quick and relatively inexpensive feedback can be provided to designers and service providers in 5G next generation networks.",,10.1109/ACCESS.2019.2921729,"Drones , 5G mobile communication , Genetic algorithms , Base stations , Simulated annealing , Quality of service ",,,,,,,,,,
59,Optimizing Computation Offloading in Satellite-UAV-Served 6G IoT: A Deep Learning Approach,"Bomin Mao , Fengxiao Tang , Yuichi Kawamoto , Nei Kato ",IEEE Network,1558-156X,35,4,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9520341,"Satellite networks can provide Internet of Things (IoT) devices in remote areas with seamless coverage and downlink multicast transmissions. However, the large transmission latency, serious path loss, as well as the energy and resource constraints of IoT terminals challenge the stringent service requirements for throughput and latency in the 6G era. To address these problems, technologies including space-air-ground integrated networks (SAGINs), machine learning, edge computing, and energy harvesting are highly expected in 6G IoT. In this article, we consider the unmanned aerial vehicles (UAVs) and satellites to offer wireless-powered IoT devices edge computing and cloud computing services, respectively. To accelerate the communications, Terahertz frequency bands are utilized for communications between UAVs and IoT devices. Since the tasks generated by terrestrial IoT devices can be conducted locally, offloaded to the UAV-based edge servers or remote cloud servers through satellites, we focus on the computation offloading problem and consider deep learning techniques to optimize the task success rate considering the energy dynamics and channel conditions. A deep-learning-based offloading policy optimization strategy is given where the long short-term memory model is considered to address the dynamics of energy harvesting performance. Through the theoretical explanation and performance analysis, we discover the importance of emerging technologies including SAGIN, energy harvesting, and artificial intelligence techniques for 6G IoT.",,10.1109/MNET.011.2100097,"6G mobile communication , Deep learning , Cloud computing , Satellites , Internet of Things , Energy harvesting , Servers ",,,,,,,,,,
60,Fusion of blockchain and artificial intelligence for secure drone networking underlying 5G communications,"R Gupta, A Kumari, S Tanwar",Transactions on Emerging …,1941-0050,32,1,e4176,2021,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.4176?casa_token=I8Ntg5qXmAwAAAAA:FonOkHRvALYjMNxLMddnLUMJYPrROd_36SFakDnpiMHSnJY_l__Z6xsHjkt1jqX9YvuWgQyed7Y1CH7E,"The use of aerial base stations, AI cloud, and satellite storage can help manage location, traffic, and specific application-based services for vehicular social networks. However, sharing of such data makes the vehicular network vulnerable to data and privacy leakage. In this regard, this article proposes an efficient and secure data sharing scheme using community segmentation and a blockchain-based framework for vehicular social networks. The proposed work considers similarity matrices that employ the dynamics of structural similarity, modularity matrix, and data compatibility. These similarity matrices are then passed through stacked autoencoders that are trained to extract encoded embedding. A density-based clustering approach is then employed to find the community segments from the information distances between the encoded embeddings. A blockchain network based on the Hyperledger Fabric platform is also adopted to ensure data sharing security. Extensive experiments have been carried out to evaluate the proposed data-sharing framework in terms of the sum of squared error, sharing degree, time cost, computational complexity, throughput, and CPU utilization for proving its efficacy and applicability. The results show that the CSB framework achieves a higher degree of SD, lower computational complexity, and higher throughput.",,10.1109/TII.2022.3188963,,,,,,,,,,,
61,Machine learning for 5G and beyond: From model-based to data-driven mobile wireless networks,"Tianyu Wang , Shaowei Wang , Zhi-Hua Zhou ",China Communications,1673-5447,16,1,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8633313,"During the past few decades, mobile wireless communications have experienced four generations of technological revolution, namely from 1G to 4G, and the deployment of the latest 5G networks is expected to take place in 2019. One fundamental question is how we can push forward the development of mobile wireless communications while it has become an extremely complex and sophisticated system. We believe that the answer lies in the huge volumes of data produced by the network itself, and machine learning may become a key to exploit such information. In this paper, we elaborate why the conventional model-based paradigm, which has been widely proved useful in pre-5G networks, can be less efficient or even less practical in the future 5G and beyond mobile networks. Then, we explain how the data-driven paradigm, using state-of-the-art machine learning techniques, can become a promising solution. At last, we provide a typical use case of the data-driven paradigm, i.e., proactive load balancing, in which online learning is utilized to adjust cell configurations in advance to avoid burst congestion caused by rapid traffic changes.",,10.12676/j.cc.2019.01.015,"5G mobile communication , Data models , Wireless networks , Machine learning , Computational modeling , Broadband antennas ",,,,,,,,,,
62,"Survey on Machine Learning for Intelligent End-to-End Communication Toward 6G: From Network Access, Routing to Traffic Control and Streaming Adaption","Fengxiao Tang , Bomin Mao , Yuichi Kawamoto , Nei Kato ",IEEE Communications Surveys & Tutorials,2373-745X,23,3,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9403380,"The end-to-end quality of service (QoS) and quality of experience (QoE) guarantee is quite important for network optimization. The current 5G and conceived 6G network in the future with ultra high density, bandwidth, mobility and large scale brings urgent requirement of high efficient end-to-end optimization methods. The conventional network optimization methods without learning and intelligent decision ability are hard to handle the high complexity and dynamic scenarios of 6G. Recently, machine learning based QoS and QoE aware network optimization algorithms emerge as a hot research area and attract much attention, which is widely acknowledged as the potential solution for end-to-end optimization in 6G. However, there are still many critical issues of employing machine learning in networks, especially in 6G. In this paper, we give a comprehensive survey on the recent machine learning based network optimization methods to guarantee the end-to-end QoS and QoE. To easy to follow, we introduce the investigated works following the end-to-end transmission flow from network access, routing to network congestion control and adaptive steaming control. Then we discuss some open issues and potential future research directions.",,10.1109/COMST.2021.3073009,"Quality of service , Heuristic algorithms , 6G mobile communication , Quality of experience , Routing , Machine learning algorithms , Reinforcement learning ",,,,,,,,,,
63,A Heuristic Offloading Method for Deep Learning Edge Services in 5G Networks,"Xiaolong Xu , Daoming Li , Zhonghui Dai , Shancang Li , Xuening Chen ",IEEE Access,2169-3536,7,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8721102,"With the continuous development of the Internet of Things (IoT) and communications technology, especially under the epoch of 5G, mobile tasks with big scales of data have a strong demand in deep learning such as virtual speech recognition and video classification. Considering the limited computing resource and battery consumption of mobile devices (MDs), these tasks are often offloaded to the remote infrastructure, like cloud platforms, which leads to the unavoidable offloading transmission delay. Edge computing (EC) is a novel computing paradigm, capable of offloading the computation tasks to the edge of networks, which reduces the transmission delay between the MDs and cloud. Therefore, combining deep learning and EC is expected to be a solution for these tasks. However, how to decide the offloading destination [cloud or deep learning-enabled edge computing nodes (ECNs)] for computation offloading is still a challenge. In this paper, a heuristic offloading method, named HOM, is proposed to minimize the total transmission delay. To be more specific, an offloading framework for deep learning edge services is built upon centralized unit (CU)-distributed unit (DU) architecture. Then, we acquire the appropriate offloading strategy by the origin-destination ECN distance estimation and heuristic searching of the destination virtual machines for accommodating the offloaded computation tasks. Finally, the effectiveness of the scheme is verified by detailed experimental evaluations.",,10.1109/ACCESS.2019.2918585,"Task analysis , Deep learning , 5G mobile communication , Delays , Cloud computing , Internet of Things , Edge computing ",,,,,,,,,,
64,A Survey of Online Data-Driven Proactive 5G Network Optimisation Using Machine Learning,"Bo Ma , Weisi Guo , Jie Zhang ",IEEE Access,2169-3536,8,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9003183,"In the fifth-generation (5G) mobile networks, proactive network optimisation plays an important role in meeting the exponential traffic growth, more stringent service requirements, and to reduce capital and operational expenditure. Proactive network optimisation is widely acknowledged as one of the most promising ways to transform the 5G network based on big data analysis and cloud-fog-edge computing, but there are many challenges. Proactive algorithms will require accurate forecasting of highly contextualised traffic demand and quantifying the uncertainty to drive decision making with performance guarantees. Context in Cyber-Physical-Social Systems (CPSS) is often challenging to uncover, unfolds over time, and even more difficult to quantify and integrate into decision making. The first part of the review focuses on mining and inferring CPSS context from heterogeneous data sources, such as online user-generated-content. It will examine the state-of-the-art methods currently employed to infer location, social behaviour, and traffic demand through a cloud-edge computing framework; combining them to form the input to proactive algorithms. The second part of the review focuses on exploiting and integrating the demand knowledge for a range of proactive optimisation techniques, including the key aspects of load balancing, mobile edge caching, and interference management. In both parts, appropriate state-of-the-art machine learning techniques (including probabilistic uncertainty cascades in proactive optimisation), complexity-performance trade-offs, and demonstrative examples are presented to inspire readers. This survey couples the potential of online big data analytics, cloud-edge computing, statistical machine learning, and proactive network optimisation in a common cross-layer wireless framework. The wider impact of this survey includes better cross-fertilising the academic fields of data analytics, mobile edge computing, AI, CPSS, and wireless communications, as well as informing the industry of the promising potentials in this area.",,10.1109/ACCESS.2020.2975004,"Optimization , 5G mobile communication , Cloud computing , Machine learning , Forecasting , Big Data , 3GPP ",,,,,,,,,,
65,Artificial Intelligence for Elastic Management and Orchestration of 5G Networks,"David M. Gutierrez-Estevez , Marco Gramaglia , Antonio De Domenico , Ghina Dandachi , Sina Khatibi , Dimitris Tsolkas , Irina Balan , Andres Garcia-Saavedra , Uri Elzur , Yue Wang ",IEEE Wireless Communications,1558-0687,26,5,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8809570,"The emergence of 5G enables a broad set of diversified and heterogeneous services with complex and potentially conflicting demands. For networks to be able to satisfy those needs, a flexible, adaptable, and programmable architecture based on network slicing is being proposed. A softwarization and cloudification of the communications networks is required, where network functions (NFs) are being transformed from programs running on dedicated hardware platforms to programs running over a shared pool of computational and communication resources. This architectural framework allows the introduction of resource elasticity as a key means to make an efficient use of the computational resources of 5G systems, but adds challenges related to resource sharing and efficiency. In this article, we propose Artificial Intelligence (AI) as a built-in architectural feature that allows the exploitation of the resource elasticity of a 5G network. Building on the work of the recently formed Experiential Network Intelligence (ENI) industry specification group of the European Telecommunications Standards Institute (ETSI) to embed an AI engine in the network, we describe a novel taxonomy for learning mechanisms that target exploiting the elasticity of the network as well as three different resource elastic use cases leveraging AI. This work describes the basis of a use case recently approved at ETSI ENI.",,10.1109/MWC.2019.1800498,"Elasticity , Artificial intelligence , 5G mobile communication , Computer architecture , Resource management , Cloud computing , Network slicing ",,,,,,,,,,
66,Enhanced Machine Learning Techniques for Early HARQ Feedback Prediction in 5G,"Nils Strodthoff , Barış Göktepe , Thomas Schierl , Cornelius Hellge , Wojciech Samek ",IEEE Journal on Selected Areas in Communications,1558-0008,37,11,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8792202,"We investigate Early Hybrid Automatic Repeat reQuest (E-HARQ) feedback schemes enhanced by machine learning techniques as a path towards ultra-reliable and low-latency communication (URLLC). To this end, we propose machine learning methods to predict the outcome of the decoding process ahead of the end of the transmission. We discuss different input features and classification algorithms ranging from traditional methods to newly developed supervised autoencoders. These methods are evaluated based on their prospects of complying with the URLLC requirements of effective block error rates below 10-5 at small latency overheads. We provide realistic performance estimates in a system model incorporating scheduling effects to demonstrate the feasibility of E-HARQ across different signal-to-noise ratios, subcode lengths, channel conditions and system loads, and show the benefit over regular HARQ and existing E-HARQ schemes without machine learning.",,10.1109/JSAC.2019.2934001,"Machine learning , Decoding , Probabilistic logic , Machine learning algorithms , Error analysis , Delays , Receivers ",,,,,,,,,,
67,Toward Smart and Cooperative Edge Caching for 5G Networks: A Deep Learning Based Approach,"Haitian Pang , Jiangchuan Liu , Xiaoyi Fan , Lifeng Sun ",2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS),1548-615X,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8624176,"The emerging 5G mobile networking promises ultrahigh network bandwidth and ultra-low communication latency (<;1ms), benefiting a wide range of applications, including live video streaming, online gaming, virtual and augmented reality, and Vehicle-to-X, to name but a few. The backbone Internet, however, does not keep up, particularly in latency (>100ms), due to its store-and-forward design and the physical barrier from signal propagation speed, not to mention congestion that frequently happens. Caching is known to be effective to bridge the speed gap, which has become a critical component in the 5G deployment as well. Besides storage, 5G base stations (BSs) will also be powered with strong computing modules, offering mobile edge computing (MEC) capability. This paper explores the potentials of edge computing towards improving the cache performance, and we envision a learning-based framework that facilitates smart caching beyond simple frequency- and time-based replace strategies and cooperation among base stations. Within this framework, we develop DeepCache, a deep-learning-based solution to understand the request patterns in individual base stations and accordingly make intelligent cache decisions. Using mobile video, one of the most popular applications with high traffic demand, as a case, we further develop a cooperation strategy for nearby base stations to collectively serve user requests. Experimental results on real-world dataset show that using the collaborative DeepCache algorithm, the overall transmission delay is reduced by 14%~22%, with a backhaul data traffic saving of 15%~23%.",,10.1109/IWQoS.2018.8624176,"5G mobile communication , Servers , Streaming media , Training , Deep learning , Base stations , Bandwidth ",,,,,,,,,,
68,Dynamic management of a deep learning-based anomaly detection system for 5G networks,"L Fernández Maimó, A Huertas Celdrán, ...",Journal of Ambient …,1868-5145,8,10,3083 – 3097,2019,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s12652-018-0813-4&casa_token=5nYM-4mKQ7QAAAAA:r9nngOJxLkUF54yugnf24VRFC3JfdglP_c7FRMJhVnUwrzxvHXmWenBwBSPl3w7w06NYn2xeHVRxCULREg,"Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.  ",,10.1007/s12652-018-0813-4,"Deep learning,  Anomaly detection,  Virtualization,  5G mobile networks",,,,,,,,,,
69,Intelligent network data analytics function in 5G cellular networks using machine learning,"Salih Sevgican , Meriç Turan , Kerim Gökarslan , H. Birkan Yilmaz , Tuna Tugcu ",Journal of Communications and Networks,1976-5541,22,3,,2020,KICS,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9143579,"5G cellular networks come with many new features compared to the legacy cellular networks, such as network data analytics function (NWDAF), which enables the network operators to either implement their own machine learning (ML) based data analytics methodologies or integrate third-party solutions to their networks. In this paper, the structure and the protocols of NWDAF that are defined in the 3rd Generation Partnership Project (3GPP) standard documents are first described. Then, cell-based synthetic data set for 5G networks based on the fields defined by the 3GPP specifications is generated. Further, some anomalies are added to this data set (e.g., suddenly increasing traffic in a particular cell), and then these anomalies within each cell, subscriber category, and user equipment are classified. Afterward, three ML models, namely, linear regression, long-short term memory, and recursive neural networks are implemented to study behaviour information estimation (e.g., anomalies in the network traffic) and network load prediction capabilities of NWDAF. For the prediction of network load, three different models are used to minimize the mean absolute error, which is calculated by subtracting the actual generated data from the model prediction value. For the classification of anomalies, two ML models are used to increase the area under the receiver operating characteristics curve, namely, logistic regression and extreme gradient boosting. According to the simulation results, neural network algorithms outperform linear regression in network load prediction, whereas the tree-based gradient boosting algorithm outperforms logistic regression in anomaly detection. These estimations are expected to increase the performance of the 5G network through NWDAF.",,10.1109/JCN.2020.000019,"5G mobile communication , Cellular networks , 3GPP , Data analysis , Machine learning , Load modeling , Data models ",,,,,,,,,,
70,Deep Learning for Security Problems in 5G Heterogeneous Networks,"Zhihan Lv , Amit Kumar Singh , Jinhua Li ",IEEE Network,1558-156X,35,2,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9318424,"With increasingly complex network structure, requirements for heterogeneous 5G are also growing. The aim of this study is to meet the network security performance under the existing high-capacity and highly reliable transmission. In this context, deep learning technology is adopted to solve the security problem of the 5G heterogeneous network. First, the security problems existing in 5G heterogeneous networks are presented, mainly from two aspects of the physical layer security problems and application prospects of deep learning in communication technology. Then the combination of deep learning and 5G heterogeneous networks is analyzed. The combination of deep learning technology, modulation information recognition, and beam formation is introduced. The application of deep learning in communications technology is analyzed, and the modulation information recognition and beamforming based on deep learning are introduced. Finally, the challenges of solving security problems in 5G heterogeneous networks by deep learning are explored. The results show that the deep learning model can solve the modulation recognition problem well, and the modulation mode of the convolutional neural network can well identify the modulation signals involved in the experiment. Therefore, deep learning has a good advantage in solving modulation recognition. In addition, compared to the traditional algorithm, the unsupervised beamforming algorithm based on deep learning proposed in this research can effectively reduce the computational complexity under different numbers of transmitting antennas, which verifies the superiority of the unsupervised beamforming algorithm based on deep learning proposed in this research. Therefore, the present work provides a good idea for solving the security problem of 5G heterogeneous networks.",,10.1109/MNET.011.2000229,"Security , Heterogeneous networks , 5G mobile communication , Deep learning , Modulation , Communication networks , Communication systems ",,,,,,,,,,
71,Secure5G: A Deep Learning Framework Towards a Secure Network Slicing in 5G and Beyond,"Anurag Thantharate , Rahul Paropkari , Vijay Walunj , Cory Beard , Poonam Kankariya ",2020 10th Annual Computing and Communication Workshop and Conference (CCWC),,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9031158,"Network Slicing will play a vital role in enabling a multitude of 5G applications, use cases, and services. Network slicing functions will provide an end-to-end isolation between slices with an ability to customize each slice based on the service demands (bandwidth, coverage, security, latency, reliability, etc.). Maintaining isolation of resources, traffic flow, and network functions between the slices is critical in protecting the network infrastructure system from Distributed Denial of Service (DDoS) attack. The 5G network demands and new feature sets to support ever-growing and complex business requirements have made existing approaches to network security inadequate. In this paper, we have developed a Neural Network based `Secure5G' Network Slicing model to proactively detect and eliminate threats based on incoming connections before they infest the 5G core network. `Secure5G' is a resilient model that quarantines the threats ensuring end-to-end security from device(s) to the core network, and to any of the external networks. Our designed model will enable the network operators to sell network slicing as-a-service to serve diverse services efficiently over a single infrastructure with high security and reliability.",,10.1109/CCWC47524.2020.9031158,"5G mobile communication , Network slicing , Computer crime , Computer architecture , 3GPP , Servers ",,,,,,,,,,
72,"Towards artificial intelligence enabled 6G: State of the art, challenges, and opportunities","S Zhang, D Zhu",Computer Networks,1389-1286,183,,107556,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S138912862031207X?casa_token=mpRKqDYl9dIAAAAA:SsgNJTNguppSPZ_6SqI266bCBLDX-XkDeV0tzPkc_tsFn0cSDGZNA6Gsn_8tEkcxmXAazy16LSM,"6G is expected to support the unprecedented Internet of everything scenarios with extremely diverse and challenging requirements. To fulfill such diverse requirements efficiently, 6G is envisioned to be space-aerial-terrestrial-ocean integrated three-dimension networks with different types of slices enabled by new technologies and paradigms to make the system more intelligent and flexible. As 6G networks are increasingly complex, heterogeneous and dynamic, it is very challenging to achieve efficient resource utilization, seamless user experience, automatic management and orchestration. With the advancement of big data processing technology, computing power and the availability of rich data, it is natural to tackle complex 6G network issues by leveraging artificial intelligence (AI). In this paper, we make a comprehensive survey about AI-empowered networks evolving towards 6G. We first present the vision of AI-enabled 6G system, the driving forces of introducing AI into 6G and the state of the art in machine learning. Then applying machine learning techniques to major 6G network issues including advanced radio interface, intelligent traffic control, security protection, management and orchestration, and network optimization is extensively discussed. Moreover, the latest progress of major standardization initiatives and industry research programs on applying machine learning to mobile networks evolving towards 6G are reviewed. Finally, we identify important open issues to inspire further studies towards an intelligent, efficient and secure 6G system.",,10.1016/j.comnet.2020.107556,"6G, Artificial intelligence, Radio technology, Traffic control, Management and orchestration, Network security, Network optimization, Network architecture",,,,,,,,,,
73,Redefining Wireless Communication for 6G: Signal Processing Meets Deep Learning With Deep Unfolding,"Anu Jagannath , Jithin Jagannath , Tommaso Melodia ",IEEE Transactions on Artificial Intelligence,2691-4581,2,6,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9524496,"The year 2019 witnessed the rollout of the 5G standard, which promises to offer significant data rate improvement over 4G. While 5G is still in its infancy, there has been an increased shift in the research community for communication technologies beyond 5G. The recent emergence of machine learning approaches for enhancing wireless communications and empowering them with much-desired intelligence holds immense potential for redefining wireless communication for 6G. The evolving communication systems will be bottlenecked in terms of latency, throughput, and reliability by the underlying signal processing at the physical layer. In this position letter, we motivate the need to redesign iterative signal processing algorithms by leveraging deep unfolding techniques to fulfill the physical layer requirements for 6G networks. To this end, we begin by presenting the service requirements and the key challenges posed by the envisioned 6G communication architecture. We outline the deficiencies of the traditional algorithmic principles and data-hungry deep learning (DL) approaches in the context of 6G networks. Specifically, deep unfolded signal processing is presented by sketching the interplay between domain knowledge and DL. The deep unfolded approaches reviewed in this letter are positioned explicitly in the context of the requirements imposed by the next generation of cellular networks. Finally, this letter motivates open research challenges to truly realize hardware-efficient edge intelligence for future 6G networks. Impact Statement—In this letter, we discuss why the infusion of domain knowledge into machine learning frameworks holds the key to future embedded intelligent communication systems. Applying traditional signal processing and deep learning approaches independently entails significant computational and memory constraints. This becomes challenging in the context of future communication networks, such as 6G with significant communication demands where dense deployments of embedded Internet of Things (IoT) devices are envisioned. Hence, we put forth deep unfolded approaches as the potential enabling technology for 6G artificial intelligence (AI) radio to mitigate the computational and memory demands as well as to fulfill the future 6G latency, reliability, and throughput requirements. To this end, we present a general deep unfolding methodology that can be applied to iterative signal processing algorithms. Thereafter, we survey some initial steps taken in this direction and more importantly discuss the potential it has in overcoming challenges in the context of 6G requirements. This letter concludes by providing future research directions in this promising realm.",,10.1109/TAI.2021.3108129,"6G mobile communication , Physical layer , 5G mobile communication , Artificial intelligence , Wireless communication , Deep learning , Signal processing algorithms , Wireless networks ",,,,,,,,,,
74,Decoding Optimization for 5G LDPC Codes by Machine Learning,"Xiaoning Wu , Ming Jiang , Chunming Zhao ",IEEE Access,2169-3536,6,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8458121,"In this paper, we propose a generalized minimum-sum decoding algorithm using a linear approximation (LAMS) for protograph-based low-density parity-check (PB-LDPC) codes with quasi-cyclic (QC) structures. The linear approximation introduces some factors in each decoding iteration, which linearly adjust the check node updating and channel output. These factors are optimized iteratively using machine learning, where the optimization can be efficiently solved by a small and shallow neural network with training data produced by the LAMS decoder. The neural network is built according to the parity check matrix of a PB-LDPC code with a QC structure which can greatly reduce the size of the neural network. Since, we optimize the factors once per decoding iteration, the optimization is not limited by the number of the iterations. Then, we give the optimized results of the factors in the LAMS decoder and perform decoding simulations for PB-LDPC codes in fifth generation mobile networks (5G). In the simulations, the LAMS algorithm shows noticeable improvement over the normalized and the offset minimum-sum algorithms and even better performance than the belief propagation algorithm in some high signal-to-noise ratio regions.",,10.1109/ACCESS.2018.2869374,"Iterative decoding , Decoding , Approximation algorithms , Neural networks , Optimization , Machine learning ",,,,,,,,,,
75,PIRATE: A Blockchain-Based Secure Framework of Distributed Machine Learning in 5G Networks,"Sicong Zhou , Huawei Huang , Wuhui Chen , Pan Zhou , Zibin Zheng , Song Guo ",IEEE Network,1558-156X,34,6,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9210138,"In fifth-generation (5G) networks and beyond, communication latency and network bandwidth will be no longer be bottlenecks to mobile users. Thus, almost every mobile device can participate in distributed learning. That is, the availability issue of distributed learning can be eliminated. However, model safety will become a challenge. This is because the distributed learning system is prone to suffering from byzantine attacks during the stages of updating model parameters and aggregating gradients among multiple learning participants. Therefore, to provide the byzantine-resilience for distributed learning in the 5G era, this article proposes a secure computing framework based on the sharding technique of blockchain, namely PiRATE. To prove the feasibility of the proposed PiRATE, we implemented a prototype. A case study shows how the proposed PiRATE contributes to distributed learning. Finally, we also envision some open issues and challenges based on the proposed byzantine- resilient learning framework.",,10.1109/MNET.001.1900658,"Protocols , Machine learning , Computational modeling , Training , Peer-to-peer computing , 5G mobile communication ",,,,,,,,,,
76,Deep Learning-Based Resource Allocation for 5G Broadband TV Service,"Peng Yu , Fanqin Zhou , Xiang Zhang , Xuesong Qiu , Michel Kadoch , Mohamed Cheriet ",IEEE Transactions on Broadcasting,1557-9611,66,4,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8989816,"The vision of next-generation TV is to support media services to achieve sharing of cross-domain experience, and the eMBB scenario of the 5G network is one of its important driving forces. Considering the bandwidth and resource requirements of different services, such as unicast and multicast services of multimedia TV broadcasting, rationally allocating resources while providing high-quality services and realizing green energy savings of base stations is one of the challenges. This paper is aimed at the resource allocation for TV multimedia service in the 5G wireless cloud network (C-RAN) scenario, which can support unicast services for cellular users and multicast services for broadcast services simultaneously, and it proposes the corresponding slice resources allocation architecture based on the concept of a self-organizing network. The management architecture first builds the functions and processes of the corresponding autonomous resource management. Based on the multidimensional data, an effective deep learning model named LSTM (long short-term memory) is used to construct the dynamic traffic model of the multicast service in space-time, which provides a basis for further network resource allocation. Based on the prediction results and the condition of satisfying the changing requirements of users, the corresponding optimization model is constructed with the goal of minimizing the energy usage of the RRHs (remote radio heads) and taking the QoS constraints of the users into account. A deep reinforcement learning (DRL) framework combined with a convex optimization method are then used to complete the users' bandwidth and power resource allocation. The experimental results show that the proposed method can not only predict the multicast service requirement accurately but also effectively improve the energy efficiency of the network under targeted QoS requirements along with time variations.",,10.1109/TBC.2020.2968730,"5G mobile communication , TV , Resource management , Predictive models , Digital multimedia broadcasting , Unicast , Wireless communication ",,,,,,,,,,
77,Deep Learning for Radio Resource Allocation With Diverse Quality-of-Service Requirements in 5G,"Rui Dong , Changyang She , Wibowo Hardjawana , Yonghui Li , Branka Vucetic ",IEEE Transactions on Wireless Communications,1558-2248,20,4,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9286851,"To accommodate diverse Quality-of-Service (QoS) requirements in 5th generation cellular networks, base stations need real-time optimization of radio resources in time-varying network conditions. This brings high computing overheads and long processing delays. In this work, we develop a deep learning framework to approximate the optimal resource allocation policy that minimizes the total power consumption of a base station by optimizing bandwidth and transmit power allocation. We find that a fully-connected neural network (NN) cannot fully guarantee the QoS requirements due to the approximation errors and quantization errors of the numbers of subcarriers. To tackle this problem, we propose a cascaded structure of NNs, where the first NN approximates the optimal bandwidth allocation, and the second NN outputs the transmit power required to satisfy the QoS requirement with given bandwidth allocation. Considering that the distribution of wireless channels and the types of services in the wireless networks are non-stationary, we apply deep transfer learning to update NNs in non-stationary wireless networks. Simulation results validate that the cascaded NNs outperform the fully connected NN in terms of QoS guarantee. In addition, deep transfer learning can reduce the number of training samples required to train the NNs remarkably.",,10.1109/TWC.2020.3041319,"Artificial neural networks , Quality of service , Resource management , Optimization , Ultra reliable low latency communication , Wireless networks , Delays ",,,,,,,,,,
78,Machine Learning Threatens 5G Security,"Jani Suomalainen , Arto Juhola , Shahriar Shahabuddin , Aarne Mämmelä , Ijaz Ahmad ",IEEE Access,2169-3536,8,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9229146,"Machine learning (ML) is expected to solve many challenges in the fifth generation (5G) of mobile networks. However, ML will also open the network to several serious cybersecurity vulnerabilities. Most of the learning in ML happens through data gathered from the environment. Un-scrutinized data will have serious consequences on machines absorbing the data to produce actionable intelligence for the network. Scrutinizing the data, on the other hand, opens privacy challenges. Unfortunately, most of the ML systems are borrowed from other disciplines that provide excellent results in small closed environments. The resulting deployment of such ML systems in 5G can inadvertently open the network to serious security challenges such as unfair use of resources, denial of service, as well as leakage of private and confidential information. Therefore, in this article we dig into the weaknesses of the most prominent ML systems that are currently vigorously researched for deployment in 5G. We further classify and survey solutions for avoiding such pitfalls of ML in 5G systems.",,10.1109/ACCESS.2020.3031966,"5G mobile communication , Data models , Machine learning , Wireless networks , Computer security , Data privacy ",,,,,,,,,,
79,Blockchain and Artificial Intelligence for Dynamic Resource Sharing in 6G and Beyond,"Shisheng Hu , Ying-Chang Liang , Zehui Xiong , Dusit Niyato ",IEEE Wireless Communications,1558-0687,28,4,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9382024,"Wireless resources, such as spectrum, computation, infrastructures and so on, are critical in 6G and beyond. Dynamic Resource Sharing (DRS), which improves the resource utilization compared to the static and fixed resource allocation, thus needs to be further exploited. Blockchain and AI are two promising techniques for DRS in 6G and beyond. In this article, a blockchain and AI-empow-ered DRS architecture is proposed, where block-chain is used to achieve the functionalities in DRS with improved distribution, security and automation, and AI is implemented to improve the performance of pattern recognition and decision-making in DRS. Finally, a case study where dynamic spectrum sharing is implemented within the proposed architecture, and deep reinforcement learning is used and shown to optimize the profit ratio of the users.",,10.1109/MWC.001.2000409,"Blockchains , Artificial intelligence , Resource management , 6G mobile communication , Dynamic scheduling , Smart contracts , Pattern recognition ",,,,,,,,,,
80,"Artificial-Intelligence-Enabled Air Interface for 6G: Solutions, Challenges, and Standardization Impacts","Shuangfeng Han , Tian Xie , Chih-Lin I , Li Chai , Zhiming Liu , Yifei Yuan , Chunfeng Cui ",IEEE Communications Magazine,1558-1896,58,10,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9247527,"As 3GPP has completed Release 16 specifications and worldwide 5G commercialization is speeding up, global interest in 6G is starting to grow. An interesting and important question is: will the rapid progress in artificial intelligence (AI) eventually alleviate the tremendous efforts required for future standardization of 6G and beyond? In this article, the potential impacts of AI on the air interface design and standardization are investigated. The AI-enabled network architecture is first discussed. The higher layer, physical layer, and cross-layer design empowered by AI capability are further presented. Based on these designs, the future 6G and beyond are expected to enter into an AI era. For potential new use cases and more challenging requirements, the network is capable of automatic updating the air interface protocols, which may substantially reduce the standardization efforts and costs of wireless communication networks.",,10.1109/MCOM.001.2000218,"Artificial intelligence , 6G mobile communication , Protocols , 5G mobile communication , Communication channels , Wireless communication ",,,,,,,,,,
81,Distributed Artificial Intelligence Solution for D2D Communication in 5G Networks,"Iacovos Ioannou , Vasos Vassiliou , Christophoros Christophorou , Andreas Pitsillides ",IEEE Systems Journal,2373-7816,14,3,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9080591,"Device-to-device (D2D) communication, a core technology component of the evolving fifth-generation (5G) architecture, promises improvements in energy efficiency, spectral efficiency, overall system capacity, and higher data rates. These improvements in network performance spearheaded a vast amount of research in D2D, which identified significant challenges that need to be addressed before realizing their full potential in 5G networks, and beyond. Toward this end, this article proposes the use of a distributed intelligent approach to control the generation of D2D networks. More precisely, the proposed approach uses Belief Desire Intention (BDI) intelligent agents with extended capabilities (BDIx) to manage each D2D node independently and autonomously, without the help of the base station. To illustrate the above, this article proposes the DAIS algorithm for the decision of transmission mode in D2D, which maximizes the data rate and minimizes the power consumption in the network, while taking into consideration the computational load. Simulations show the applicability of BDI agents in solving D2D challenges.",,10.1109/JSYST.2020.2979044,"Device-to-device communication , Interference , 5G mobile communication , Relays , Intelligent agents , Cellular networks , Artificial intelligence ",,,,,,,,,,
82,Machine learning prediction approach to enhance congestion control in 5G IoT environment,"IA Najm, AK Hamoud, J Lloret, I Bosch",Electronics,2079-9292,8,6,607,2019,mdpi.com,https://www.mdpi.com/471384,"The 5G network is a next-generation wireless form of communication and the latest mobile technology. In practice, 5G utilizes the Internet of Things (IoT) to work in high-traffic networks with multiple nodes/sensors in an attempt to transmit their packets to a destination simultaneously, which is a characteristic of IoT applications. Due to this, 5G offers vast bandwidth, low delay, and extremely high data transfer speed. Thus, 5G presents opportunities and motivations for utilizing next-generation protocols, especially the stream control transmission protocol (SCTP). However, the congestion control mechanisms of the conventional SCTP negatively influence overall performance. Moreover, existing mechanisms contribute to reduce 5G and IoT performance. Thus, a new machine learning model based on a decision tree (DT) algorithm is proposed in this study to predict optimal enhancement of congestion control in the wireless sensors of 5G IoT networks. The model was implemented on a training dataset to determine the optimal parametric setting in a 5G environment. The dataset was used to train the machine learning model and enable the prediction of optimal alternatives that can enhance the performance of the congestion control approach. The DT approach can be used for other functions, especially prediction and classification. DT algorithms provide graphs that can be used by any user to understand the prediction approach. The DT C4.5 provided promising results, with more than 92% precision and recall.",,10.3390/electronics8060607," machine learning, decision tree algorithm IoT, WSN, C4.5,  congestion control,  5G network",,,,,,,,,,
83,When 5G meets deep learning: a systematic review,"GL Santos, PT Endo, D Sadok, J Kelner",Algorithms,1999-4893,13,9,208,2020,mdpi.com,https://www.mdpi.com/807044,"This last decade, the amount of data exchanged on the Internet increased by over a staggering factor of 100, and is expected to exceed well over the 500 exabytes by 2020. This phenomenon is mainly due to the evolution of high-speed broadband Internet and, more specifically, the popularization and wide spread use of smartphones and associated accessible data plans. Although 4G with its long-term evolution (LTE) technology is seen as a mature technology, there is continual improvement to its radio technology and architecture such as in the scope of the LTE Advanced standard, a major enhancement of LTE. However, for the long run, the next generation of telecommunication (5G) is considered and is gaining considerable momentum from both industry and researchers. In addition, with the deployment of the Internet of Things (IoT) applications, smart cities, vehicular networks, e-health systems, and Industry 4.0, a new plethora of 5G services has emerged with very diverging and technologically challenging design requirements. These include high mobile data volume per area, high number of devices connected per area, high data rates, longer battery life for low-power devices, and reduced end-to-end latency. Several technologies are being developed to meet these new requirements, and each of these technologies brings its own design issues and challenges. In this context, deep learning models could be seen as one of the main tools that can be used to process monitoring data and automate decisions. As these models are able to extract relevant features from raw data (images, texts, and other types of unstructured data), the integration between 5G and DL looks promising and one that requires exploring. As main contribution, this paper presents a systematic review about how DL is being applied to solve some 5G issues. Differently from the current literature, we examine data from the last decade and the works that address diverse 5G specific problems, such as physical medium state estimation, network traffic prediction, user device location prediction, self network management, among others. We also discuss the main research challenges when using deep learning models in 5G scenarios and identify several issues that deserve further consideration.",,10.3390/a13090208,"the next generation of telecommunication (5G), deep learning  reinforcement learning, systematic review, cellular networks ",,,,,,,,,,
84,Machine learning-based network sub-slicing framework in a sustainable 5g environment,"SK Singh, MM Salim, J Cha, Y Pan, JH Park",Sustainability,2071-1050,12,15,6250,2020,mdpi.com,https://www.mdpi.com/787976,"Nowadays, 5G network infrastructures are being developed for various industrial IoT (Internet of Things) applications worldwide, emerging with the IoT. As such, it is possible to deploy power-optimized technology in a way that promotes the long-term sustainability of networks. Network slicing is a fundamental technology that is implemented to handle load balancing issues within a multi-tenant network system. Separate network slices are formed to process applications having different requirements, such as low latency, high reliability, and high spectral efficiency. Modern IoT applications have dynamic needs, and various systems prioritize assorted types of network resources accordingly. In this paper, we present a new framework for the optimum performance of device applications with optimized network slice resources. Specifically, we propose a Machine Learning-based Network Sub-slicing Framework in a Sustainable 5G Environment in order to optimize network load balancing problems, where each logical slice is divided into a virtualized sub-slice of resources. Each sub-slice provides the application system with different prioritized resources as necessary. One sub-slice focuses on spectral efficiency, whereas the other focuses on providing low latency with reduced power consumption. We identify different connected device application requirements through feature selection using the Support Vector Machine (SVM) algorithm. The K-means algorithm is used to create clusters of sub-slices for the similar grouping of types of application services such as application-based, platform-based, and infrastructure-based services. Latency, load balancing, heterogeneity, and power efficiency are the four primary key considerations for the proposed framework. We evaluate and present a comparative analysis of the proposed framework, which outperforms existing studies based on experimental evaluation.",,10.3390/su12156250,"machine learning, network sub-slicing, sustainable 5G environment, IoT",,,,,,,,,,
85,Deep learning architectures for accurate millimeter wave positioning in 5G,"J Gante, G Falcao, L Sousa",Neural Processing Letters,1573-773X,51,1,487,2020,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11063-019-10073-1&casa_token=zx9_Pr5HnUMAAAAA:tv181gofo0ShfU2BqwkO0AY2Ue3b0PO-y2-ahLMFr6gco0zm7OZdyD7IgsgTUi_cLqdwBoqsaDwafxzgbg,"The introduction of 5G’s millimeter wave transmissions brings a new paradigm to wireless communications. Whereas physical obstacles were mostly associated with signal attenuation, their presence now adds complex, non-linear phenomena, including reflections and scattering. The result is a multipath propagation environment, shaped by the obstacles encountered, indicating a strong presence of hidden spatial information within the received signal. To untangle said information into a mobile device position, this paper proposes the usage of neural networks over beamformed fingerprints, enabling a single-anchor positioning approach. Depending on the mobile device target application, positioning can also be enhanced with tracking techniques, which leverage short-term historical data. The main contributions of this paper are to discuss and evaluate typical neural network architectures suitable to the beamformed fingerprint positioning problem, including convolutional neural networks, hierarchy-based techniques, and sequence learning approaches. Using short sequences with temporal convolutional networks, simulation results show that stable average estimation errors of down to 1.78 m are obtained on realistic outdoor scenarios, containing mostly non-line-of-sight positions. These results establish a new state-of-the-art accuracy value for non-line-of-sight millimeter wave outdoor positioning, making the proposed methods very competitive and promising alternatives in the field.",,10.1007/s11063-019-10073-1,"5G, Deep learning, Millimeter wave, Outdoor positioning, Temporal convolutional networks",,,,,,,,,,
86,Deep Learning and Blockchain-Empowered Security Framework for Intelligent 5G-Enabled IoT,"Shailendra Rathore , Jong Hyuk Park , Hangbae Chang ",IEEE Access,2169-3536,9,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9420742,"Recently, many IoT applications, such as smart transportation, healthcare, and virtual and augmented reality experiences, have emerged with fifth-generation (5G) technology to enhance the Quality of Service (QoS) and user experience. The revolution of 5G-enabled IoT supports distinct attributes, including lower latency, higher system capacity, high data rate, and energy saving. However, such revolution also delivers considerable increment in data generation that further leads to a major requirement of intelligent and effective data analytic operation across the network. Furthermore, data growth gives rise to data security and privacy concerns, such as breach and loss of sensitive data. The conventional data analytic and security methods do not meet the requirement of 5G-enabled IoT including its unique characteristic of low latency and high throughput. In this paper, we propose a Deep Learning (DL) and blockchain-empowered security framework for intelligent 5G-enabled IoT that leverages DL competency for intelligent data analysis operation and blockchain for data security. The framework's hierarchical architecture wherein DL and blockchain operations emerge across the four layers of cloud, fog, edge, and user is presented. The framework is simulated and analyzed, employing various standard measures of latency, accuracy, and security to demonstrate its validity in practical applications.",,10.1109/ACCESS.2021.3077069,"Security , Internet of Things , Data analysis , Blockchain , Reliability , 5G mobile communication , Quality of service ",,,,,,,,,,
87,"Blockchain and artificial intelligence for 5G‐enabled Internet of Things: Challenges, opportunities, and solutions","A Dhar Dwivedi, R Singh, K Kaushik, ...",Transactions on …,2161-3915,,,e4329,2021,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.4329?casa_token=KSIV-NvJeqwAAAAA:Eumv6c4zfIEjjhpweE40DveIuQnrAdUAlUq1xaqoNUE1naELzb-J3AoXepQ76UoFoyG_jnRJsDhx28Yg,"Internet of Things (IoT) has revolutionized the digital world by connecting billions of electronic devices over the internet. IoT devices play an essential role in the modern era when conventional devices become more autonomous and smart. On the one hand, high-speed data transfer is a major issue where the 5G-enabled environment plays an important role. On the other hand, these IoT devices transfer the data by using protocols based on centralized architecture and may cause several security issues for the data. Merging artificial intelligence to 5G wireless systems solves several issues such as autonomous robots, self-driving vehicles, virtual reality, and engender security problems. Building trust among the network users without trusting third party authorities is the system's primary concern. Blockchain emerged as a key technology based on a distributed ledger to maintain the network's event logs. Blockchain provides a secure, decentralized, and trustless environment for IoT devices. However, integrating IoT and blockchain also has several challenges; for example, major challenge is low throughput. Currently, the ethereum blockchain network can process approximately 12 to 15 transactions per second, while IoT devices require relatively higher throughput. Therefore, blockchains are incapable of providing functionality for a 5G-enabled IoT based network. The limiting factor of throughput in the blockchain is their network. The slow propagation of transactions and blocks in the P2P network does not allow miners and verifiers to fastly mine and verify new blocks, respectively. Therefore, network scalability is the major issue of IoT based blockchains. In this work, we solved the network scalability issue using blockchain distributed network while to increase the throughput of blockchain, this article uses the Raft consensus algorithm. Another most important issue with IoT networks is privacy. Unfortunately, the blockchain distributed ledgers are public and sensitive information is available on the network for everyone are private, but in such cases, third party editing is not possible without revealing the original contents. To solve privacy issues, we used zkLedger as a solution that is based on zero knowledge-based cryptography.",,10.1002/ett.4329,,,,,,,,,,,
88,On the application of machine learning to the design of UAV-based 5G radio access networks,"V Kouhdaragh, F Verde, G Gelli, J Abouei",Electronics,2079-9292,9,4,689,2020,mdpi.com,https://www.mdpi.com/699178,"A groundbreaking design of radio access networks (RANs) is needed to fulfill 5G traffic requirements. To this aim, a cost-effective and flexible strategy consists of complementing terrestrial RANs with unmanned aerial vehicles (UAVs). However, several problems must be solved in order to effectively deploy such UAV-based RANs (U-RANs). Indeed, due to the high complexity and heterogeneity of these networks, model-based design approaches, often relying on restrictive assumptions and constraints, exhibit severe limitation in real-world scenarios. Moreover, design of a set of appropriate protocols for such U-RANs is a highly sophisticated task. In this context, machine learning (ML) emerges as a useful tool to obtain practical and effective solutions. In this paper, we discuss why, how, and which types of ML methods are useful for designing U-RANs, by focusing in particular on supervised and reinforcement learning strategies.",,10.3390/electronics9040689,"5G and beyond systems, machine learning, radio access networks, reinforcement learning, supervised learning, unmanned aerial vehicles (UAVs)",,,,,,,,,,
89,Patient-Centric HetNets Powered by Machine Learning and Big Data Analytics for 6G Networks,"Mohammed S. Hadi , Ahmed Q. Lawey , Taisir E. H. El-Gorashi , Jaafar M. H. Elmirghani ",IEEE Access,2169-3536,8,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9086596,"Having a cognitive and self-optimizing network that proactively adapts not only to channel conditions, but also according to its users' needs can be one of the highest forthcoming priorities of future 6G Heterogeneous Networks (HetNets). In this paper, we introduce an interdisciplinary approach linking the concepts of e-healthcare, priority, big data analytics (BDA) and radio resource optimization in a multi-tier 5G network. We employ three machine learning (ML) algorithms, namely, naïve Bayesian (NB) classifier, logistic regression (LR), and decision tree (DT), working as an ensemble system to analyze historical medical records of stroke out-patients (OPs) and readings from body-attached internet-of-things (IoT) sensors to predict the likelihood of an imminent stroke. We convert the stroke likelihood into a risk factor functioning as a priority in a mixed integer linear programming (MILP) optimization model. Hence, the task is to optimally allocate physical resource blocks (PRBs) to HetNet users while prioritizing OPs by granting them high gain PRBs according to the severity of their medical state. Thus, empowering the OPs to send their critical data to their healthcare provider with minimized delay. To that end, two optimization approaches are proposed, a weighted sum rate maximization (WSRMax) approach and a proportional fairness (PF) approach. The proposed approaches increased the OPs' average signal to interference plus noise (SINR) by 57% and 95%, respectively. The WSRMax approach increased the system's total SINR to a level higher than that of the PF approach, nevertheless, the PF approach yielded higher SINRs for the OPs, better fairness and a lower margin of error.",,10.1109/ACCESS.2020.2992555,"Optimization , Machine learning , Big Data , Stroke (medical condition) , Resource management , Support vector machines , Interference ",,,,,,,,,,
90,Towards an FPGA-Accelerated programmable data path for edge-to-core communications in 5G networks,"R Ricart-Sanchez, P Malagon, P Salva-Garcia, ...",Journal of Network and …,1084-8045,124,,80-93,2018,Elsevier,https://www.sciencedirect.com/science/article/pii/S1084804518302923?casa_token=a15Ct0SsEVIAAAAA:VWYWJyXmA3gDh8hWjOH7jMAgraLChPJluwWwkj23Q2im3Y2qBw0utG0zZg_LPsN8Pl9ENlX5fZQ,"Data path, FPGA 5G, Mobile edge computing, Reconfigurable hardware, Network architecture and design}, abstract = {The Fifth-Generation (5G) networks, as the emerging next generation mobile networks, are adopting softwarization and virtualization technologies as the cornerstones for the network operators to gain significant competitive advantages by reducing both capital and operational expenditure, enabling agile and flexible service creation and deployment, among others. Meanwhile, a virtualized and softwarized 5G network would suffer from downgraded system performance due to this unprecedented paradigm shift towards software-based networking. Addressing one of the top challenges in this context, this paper focuses on improving the performance of the data plane from the edge to the core network segment (backhaul) in a 5G multi-tenant network by leveraging and exploring the programmability introduced by software-based networking. A fully functional prototype has been designed and implemented utilizing a Field Programmable Gate Arrays (FPGAs) acceleration-based platform, and the prototyped system has been empirically tested and evaluated to demonstrate the superior performance enhancements. The proposed solution can effectively support 5G networks in delivering mission-critical or time-sensitive applications such as ultra-high definition video use cases as experimentally validated and shown in this paper, by fulfilling the strict Quality of Service (QoS) requirements imposed to the data plane.",,10.1016/j.jnca.2018.09.012,"Data path, FPGA 5G, Mobile edge computing, Reconfigurable hardware, Network architecture and design",,,,,,,,,,
91,A Survey on Deep Learning for Ultra-Reliable and Low-Latency Communications Challenges on 6G Wireless Systems,"Adeeb Salh , Lukman Audah , Nor Shahida Mohd Shah , Abdulraqeb Alhammadi , Qazwan Abdullah , Yun Hee Kim , Samir Ahmed Al-Gailani , Shipun A. Hamzah , Bashar Ali F. Esmail , Akram A. Almohammedi ",IEEE Access,2169-3536,9,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9389782,"The sixth generation (6G) wireless communication network presents itself as a promising technique that can be utilized to provide a fully data-driven network evaluating and optimizing the end-to-end behavior and big volumes of a real-time network within a data rate of Tb/s. In addition, 6G adopts an average of 1000+ massive number of connections per person in one decade (2030 virtually instantaneously). The data-driven network is a novel service paradigm that offers a new application for the future of 6G wireless communication and network architecture. It enables ultra-reliable and low latency communication (URLLC) enhancing information transmission up to around 1 Tb/s data rate while achieving a 0.1 millisecond transmission latency. The main limitation of this technique is the computational power available for distributing with big data and greatly designed artificial neural networks. The work carried out in this paper aims to highlight improvements to the multi-level architecture by enabling artificial intelligence (AI) in URLLC providing a new technique in designing wireless networks. This is done through the application of learning, predicting, and decision-making to manage the stream of individuals trained by big data. The secondary aim of this research paper is to improve a multi-level architecture. This enables user level for device intelligence, cell level for edge intelligence, and cloud intelligence for URLLC. The improvement mainly depends on using the training process in unsupervised learning by developing data-driven resource management. In addition, improving a multi-level architecture for URLLC through deep learning (DL) would facilitate the creation of a data-driven AI system, 6G networks for intelligent devices, and technologies based on an effective learning capability. These investigational problems are essential in addressing the requirements in the creation of future smart networks. Moreover, this work provides further ideas on several research gaps between DL and 6G that are up-to-date unknown.",,10.1109/ACCESS.2021.3069707,"6G mobile communication , Ultra reliable low latency communication , Artificial intelligence , Computer architecture , Wireless networks , Bandwidth , Reliability ",,,,,,,,,,
92,"The role of artificial intelligence driven 5G networks in COVID-19 outbreak: Opportunities, challenges, and future outlook","AI Abubakar, KG Omeke, M Ozturk, ...",Frontiers in …,2673-530X,1,,575065,2020,frontiersin.org,https://www.frontiersin.org/articles/10.3389/frcmn.2020.575065/full,"There is no doubt that the world is currently experiencing a global pandemic that is reshaping our daily lives as well as the way business activities are being conducted. With the emphasis on social distancing as an effective means of curbing the rapid spread of the infection, many individuals, institutions, and industries have had to rely on telecommunications as a means of ensuring service continuity in order to prevent complete shutdown of their operations. This has put enormous pressure on both fixed and mobile networks. Though fifth generation mobile networks (5G) is at its infancy in terms of deployment, it possesses a broad category of services including enhanced mobile broadband (eMBB), ultra-reliable low-latency communications (URLLC), and massive machine-type communications (mMTC), that can help in tackling pandemic-related challenges. Therefore, in this paper, we identify the challenges facing existing networks due to the surge in traffic demand as a result of the COVID-19 pandemic and emphasize the role of 5G empowered by artificial intelligence in tackling these problems. In addition, we also provide a brief insight on the use of artificial intelligence driven 5G networks in predicting future pandemic outbreaks, and the development a pandemic-resilient society in case of future outbreaks.",,10.3389/frcmn.2020.575065,"COVID-19, coronavirus, pandemic, 5G networks, self-organizing networks, artificial intelligence, machine learning",,,,,,,,,,
93,5G standards for the Industry 4.0 enabled communication systems using artificial intelligence: perspective of smart healthcare system,"B Alhayani, AS Kwekha-Rashid, HB Mahajan, ...",Applied …,2190-5517,,,1807–18171,2022,Springer,https://link.springer.com/article/10.1007/s13204-021-02152-4,"The emergence of the Industry 4.0 revolution to upgrade the Internet of Things (IoT) standards provides the prominence outcomes for the future wireless communication systems called 5G. The development of 5G green communication systems suffers from the various challenges to fulfill the requirement of higher user capacity, network speed, minimum cost, and reduced resource consumption. The use of 5G standards for Industry 4.0 applications will increase data rate performance and connected device's reliability. Since the arrival of novel Covid-19 disease, there is a higher demand for smart healthcare systems worldwide. However, designing the 5G communication systems has the research challenges like optimum resource utilization, mobility management, cost-efficiency, interference management, spectral efficiency, etc. The rapid development of Artificial Intelligence (AI) across the different formats brings performance enhancement compared to conventional techniques. Therefore, introducing the AI into 5G standards will optimize the performances further considering the various end-user applications. We first present the survey of the terms like 5G standard, Industry 4.0, and some recent works for future wireless communications. The purpose is to explore the current research problems using the 5G technology. We further propose the novel architecture for smart healthcare systems using the 5G and Industry 4.0 standards. We design and implement that proposed model using the Network Simulator (NS2) to investigate the current 5G methods. The simulation results show that current 5G methods for resource management and interference management suffer from the challenges like performance trade-offs.  ",,10.1007/s13204-021-02152-4,"Artifcial intelligence, Future wireless communications, Internet of Things, Industry 4.0, Interference management, Resource optimization",,,,,,,,,,
94,Machine learning based anomaly detection for 5g networks,"J Lam, R Abbas",arXiv preprint arXiv:2003.03474,,,,,2020,arxiv.org,https://arxiv.org/abs/2003.03474,"Protecting the networks of tomorrow is set to be a challenging domain due to increasing cyber security threats and widening attack surfaces created by the Internet of Things (IoT), increased network heterogeneity, increased use of virtualisation technologies and distributed architectures. This paper proposes SDS (Software Defined Security) as a means to provide an automated, flexible and scalable network defence system. SDS will harness current advances in machine learning to design a CNN (Convolutional Neural Network) using NAS (Neural Architecture Search) to detect anomalous network traffic. SDS can be applied to an intrusion detection system to create a more proactive and end-to-end defence for a 5G network. To test this assumption, normal and anomalous network flows from a simulated environment have been collected and analyzed with a CNN. The results from this method are promising as the model has identified benign traffic with a 100% accuracy rate and anomalous traffic with a 96.4% detection rate. This demonstrates the effectiveness of network flow analysis for a variety of common malicious attacks and also provides a viable option for detection of encrypted malicious network traffic.",,10.48550/arXiv.2003.03474,"5G Security, IoT Security, Automated Intrusion Detection Systems, Convolutional Neural Networks, Artificial Intelligence, Software Defined Security",,,,,,,,,,
95,A survey on artificial intelligence techniques in cognitive radio networks,"R Ganesh Babu, V Amudha",Emerging Technologies in Data Mining and …,ISBN: 978-981-13-1950-1,,,99–110,2019,Springer,https://link.springer.com/chapter/10.1007/978-981-13-1951-8_10,"… Cognitive radio (CR) is the solution for the current spectral underutilized problems, Context … will become Cognitive Radio by imparting intelligence to SDR using Artificial Intelligence …",,10.1007/978-981-13-1951-8_10,"Cognitive radio, Artificial intelligence techniques, Artificial neural networks (ANN) ,Genetic algorithms (GA)",,,,,,,,,,
96,GPF: A GPU-based Design to Achieve~ 100 μs Scheduling for 5G NR,"Y Huang, S Li, YT Hou, W Lou",Proceedings of the 24th Annual …,ISBN: 978-1-4503-5903-0,,,207–222,2018,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3241539.3241552,"5G New Radio (NR) is designed to operate under a broad range of frequency bands and support new applications with ultra-low latency. To support its diverse operating conditions, a set of different OFDM numerologies has been defined in the standards body. Under this numerology, it is necessary to perform scheduling with a time resolution of ∼100 μs. This requirement poses a new challenge that does not exist in LTE and cannot be supported by any existing LTE schedulers. In this paper, we present the design of GPF -- a GPU-based proportional fair (PF) scheduler that can meet the ∼100 μs time requirement. The key ideas include decomposing the scheduling problem into a large number of small and independent sub-problems and selecting a subset of sub-problems from the most promising search space to fit into a GPU. By implementing GPF on an off-the-shelf Nvidia Quadro P6000 GPU, we show that GPF is able to achieve near-optimal performance while meeting the ∼100 $\mathrmμs time requirement. GPF represents the first successful design of a GPU-based PF scheduler that can meet the new time requirement in NR.",,10.1145/3241539.3241552,"5G NR, optimization, real-time, GPU, resource scheduling",,,,,,,,,,
97,Role of machine learning and deep learning in securing 5G-driven industrial IoT applications,"P Sharma, S Jain, S Gupta, V Chamola",Ad Hoc Networks,1570-8705,123,,102685,2021,Elsevier,https://www.sciencedirect.com/science/article/pii/S1570870521001906?casa_token=w0K8Ori1dSkAAAAA:wjVw9JHMtb7hCMTzesFLZOaYA2bbre3uF8Ny1UCrg89gNC85J9ZjYFYQSxDxcHeVUHi_A-NE_eA,"The Internet of Things (IoT) connects millions of computing devices and has set a stage for future technology where industrial use cases like smart cities and smart houses will operate with minimal human intervention. IoT’s cross-domain amalgamations with emergent technologies like 5G and blockchain affects human life. Hence, increase in reliance over IoT necessitates focus on its privacy and security concerns. Implementing security through encryption, authentication, access control and communication security is the need of the hour. These needs can be best catered with the use of machine learning (ML) and deep learning (DL) that can help in realizing secure intelligent systems. In this work, the authors present a comprehensive review for securing Industrial-IoT (I-IoT) devices to contribute to the development of security methods for I-IoT deployed over 5G and blockchain. The survey provides a general analysis of the state-of-the-art security implementation and further assesses the product life cycle of IoT devices. The authors present numerous virtues as well as faults in the machine learning and deep learning algorithms deployed over the fog architecture in context with the security solutions. The potential security algorithms can help overcome many challenges in the IoT security and pave way for implementation with emerging technologies like 5G, blockchain, edge computing, fog computing and their use cases for creating smart environments.",,10.1016/j.adhoc.2021.102685,"ndustrial internet of things, Security, Machine learning, Deep learning, Artificial intelligence, Block chain, Smart city",,,,,,,,,,
98,Dethroning GPS: Low-Power Accurate 5G Positioning Systems Using Machine Learning,"Joao Gante , Leonel Sousa , Gabriel Falcao ",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,2156-3365,10,2,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9080126,"Over the last years positioning systems have become increasingly pervasive, covering most of the planet's surface. Although they are accurate enough for a large number of uses, their precision, power consumption, and hardware requirements establish the limits for their adoption in mobile devices. In this paper, the energy consumption of a proposed deep learning-based millimeter wave positioning method is assessed, being subsequently compared to the state-of-the-art on accurate outdoor positioning systems. Requiring as low as 0.4 mJ per position fix, when compared to the most recent assisted-GPS implementations the proposed method has energy efficiency gains of 47× and 85× for continuous and sporadic position fixes (respectively), while also having slightly lower estimation errors. Therefore, the proposed method significantly reduces the energy required for precise positioning in the presence of millimeter wave networks, enabling the design of more efficient and accurate positioning-enabled mobile devices.",,10.1109/JETCAS.2020.2991024,"Mobile handsets , Global navigation satellite system , Satellites , Circuits and systems , Energy consumption , Global Positioning System , Millimeter wave technology ",,,,,,,,,,
99,FPGA for 5G: Re-configurable Hardware for Next Generation Communication,"Vinay Chamola , Sambit Patra , Neeraj Kumar , Mohsen Guizani ",IEEE Wireless Communications,1558-0687,27,3,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9076114,"Next generation communication relies on standardized protocols, heterogeneous architectures and advanced technologies that are envisioned to bring ubiquitous and seamless connectivity. This evolution of communication will not only improve the performance of the existing networks, but will also enable various applications in other fields while integrating different heterogeneous systems. This massive scaling of mobile communication requires higher bandwidth to operate. 5G promises a robust solution by offering ultra-low latency and high bandwidth for data transmission. To provide individuals and companies with a real-time, social, and all connected experience, an end-to-end coordinated architecture which is agile and intelligent has to be designed at each stage. As FPGA has the potential to be resource/power efficient, it can be used for building up constituents of 5G infrastructure. It can accelerate network performance without making a large investment in new hardware. Dynamic reconfigurability and in-field programming features of FPGAs compared to fixed function ASICs help in developing better wireless systems. This article presents various application areas of FPGAs for the upcoming 5G network planning.",,10.1109/MWC.001.1900359,"Field programmable gate arrays , 5G mobile communication , Virtualization , Bandwidth , Hardware , Cloud computing , Macrocell networks ",,,,,,,,,,
100,Deep Learning at the Physical Layer: System Challenges and Applications to 5G and Beyond,"Francesco Restuccia , Tommaso Melodia ",IEEE Communications Magazine,1558-1896,58,10,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9247524,"The unprecedented requirements of IoT have made fine-grained optimization of spectrum resources an urgent necessity. Thus, designing techniques able to extract knowledge from the spectrum in real time and select the optimal spectrum access strategy accordingly has become more important than ever. Moreover, 5G networks will require complex management schemes to deal with problems such as adaptive beam management and rate selection. Although deep learning (DL) has been successful in modeling complex phenomena, commercially available wireless devices are still very far from actually adopting learning-based techniques to optimize their spectrum usage. In this article, we first discuss the need for real-time DL at the physical layer, and then summarize the current state of the art and existing limitations. We conclude the article by discussing an agenda of research challenges and how DL can be applied to address crucial problems in 5G and beyond networks.",,10.1109/MCOM.001.2000243,"Real-time systems , Wireless communication , Hardware , Neural networks , Feature extraction , Modulation , Deep learning ",,,,,,,,,,
101,"Towards Energy Efficient 5G Networks Using Machine Learning: Taxonomy, Research Challenges, and Future Research Directions","Amna Mughees , Mohammad Tahir , Muhammad Aman Sheikh , Abdul Ahad ",IEEE Access,2169-3536,8,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9218920,"As the world pushes toward the use of greener technology and minimizes energy waste, energy efficiency in the wireless network has become more critical than ever. The next-generation networks, such as 5G, are being designed to improve energy efficiency and thus constitute a critical aspect of research and network design. The 5G network is expected to deliver a wide range of services that includes enhanced mobile broadband, massive machine-type communication and ultra-reliability, and low latency. To realize such a diverse set of requirement, 5G network has evolved as a multi-layer network that uses various technological advances to offer an extensive range of wireless services. Several technologies, such as software-defined networking, network function virtualization, edge computing, cloud computing, and small cells, are being integrated into the 5G networks to fulfill the need for diverse requirements. Such a complex network design is going to result in increased power consumption; therefore, energy efficiency becomes of utmost importance. To assist in the task of achieving energy efficiency in the network machine learning technique could play a significant role and hence gained significant interest from the research community. In this paper, we review the state-of-art application of machine learning techniques in the 5G network to enable energy efficiency at the access, edge, and core network. Based on the review, we present a taxonomy of machine learning applications in 5G networks for improving energy efficiency. We discuss several issues that can be solved using machine learning regarding energy efficiency in 5G networks. Finally, we discuss various challenges that need to be addressed to realize the full potential of machine learning to improve energy efficiency in the 5G networks. The survey presents a broad range of ideas related to machine learning in 5G that addresses the issue of energy efficiency in virtualization, resource optimization, power allocation, and incorporating enabling technologies of 5G can enhance energy efficiency.",,10.1109/ACCESS.2020.3029903,"5G mobile communication , Machine learning , Energy consumption , Massive MIMO , Virtualization , Hardware , Resource management ",,,,,,,,,,
102,"Artificial Intelligence for 5G Wireless Systems: Opportunities, Challenges, and Future Research Direction","Youness Arjoune , Saleh Faruque ",2020 10th Annual Computing and Communication Workshop and Conference (CCWC),,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9031117,"The advent of the wireless communications systems augurs new cutting-edge technologies, including self-driving vehicles, unmanned aerial systems, autonomous robots, the Internet-of-Things, and virtual reality. These technologies require high data rates, ultra-low latency, and high reliability, all of which are promised by the fifth generation of wireless communication systems (5G). Many research groups state that 5G cannot meet its demands without artificial intelligence (A.I.) integration as 5G wireless networks are expected to generate unprecedented traffic giving wireless research designers access to big data that can help in predicting the demands and adjust cell designs to meet the users' requirements. Subsequently, many researchers applied A.I. in many aspects of 5G wireless communication design including radio resource allocation, network management, cyber-security. In this paper, we provide an in-depth review of A.I. for 5G wireless communication systems. in this respect, the aim of this paper is to survey A.I. in 5G wireless communication and networking by discussing many case studies, discuss the challenges, and shed new light on future research directions for leveraging A.I. in 5G wireless communications.",,10.1109/CCWC47524.2020.9031117,"5G mobile communication , Machine learning , Wireless communication , Modulation , Decoding , MIMO communication , Signal to noise ratio ",,,,,,,,,,
103,Prediction-Based Conditional Handover for 5G mm-Wave Networks: A Deep-Learning Approach,"Changsung Lee , Hyoungjun Cho , Sooeun Song , Jong-Moon Chung ",IEEE Vehicular Technology Magazine,1556-6080,15,1,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8959359,"Conditional handover (CHO) is one of several promising mobility enhancements in 5G networks. By making preparation decisions earlier than in LTE HO, CHO can provide an improved HO success rate. This article, analyzes the strengths and weaknesses of CHO by comparing CHO to 5G baseline HO. Since millimeter-wave communications are vulnerable to blockages, sudden changes in signal reception power can mislead CHO into making undesired early preparations in 5G networks. To enhance the robustness of CHO, current studies propose using an increased number of preparations, resulting in considerable signaling overhead. This article offers a novel prediction-based CHO (PCHO) scheme that uses deep-learning technology to overcome the weaknesses of CHO and make more intelligent preparation decisions. Based on the changes in the signal patterns of the base stations, PCHO uses former blockage information to predict the best next base station to which to conduct HO. Performance evaluation demonstrates that PCHO can improve the early preparation success rate while reducing signaling overhead compared to current CHO schemes.",,10.1109/MVT.2019.2959065,"5G mobile communication , Handover , Long Term Evolution , Reliability , Base stations , 3GPP ",,,,,,,,,,
104,Deep Learning-Aided 5G Channel Estimation,A. L. Ha and T. Van Chien and T. H. Nguyen and W. Choi and V. D. Nguyen,2021 15th International Conference on Ubiquitous Information Management and Communication (IMCOM),     VO  - ,,,1-7,2021,IEEE,https://ieeexplore.ieee.org/abstract/document/9377351/?casa_token=50w_5CPQJbcAAAAA:xe7xIlL0cGtdR7kB5haAZAdPl4bJdUgKmzNRhPxP_Uh3S1J2OaSBT0qhC9n99uIRaC65tUD5-YNhHg,"Deep learning has demonstrated the important roles in improving the system performance and reducing computational complexity for 5G-and-beyond networks. In this paper, we propose a new channel estimation method with the assistance of deep learning in order to support the least squares estimation, which is a low-cost method but having relatively high channel estimation errors. This goal is achieved by utilizing a MIMO (multiple-input multiple-output) system with a multi-path channel profile used for simulations in the 5G networks under the severity of Doppler effects. Numerical results demonstrate the superiority of the proposed deep learning-assisted channel estimation method over the other channel estimation methods in previous works in terms of mean square errors.",,10.1109/IMCOM51814.2021.9377351,Acquired Immunodeficiency Syndrome;Learning,,,1-7,2021,IEEE,https://ieeexplore.ieee.org/abstract/document/9377351/?casa_token=50w_5CPQJbcAAAAA:xe7xIlL0cGtdR7kB5haAZAdPl4bJdUgKmzNRhPxP_Uh3S1J2OaSBT0qhC9n99uIRaC65tUD5-YNhHg,"Deep learning has demonstrated the important roles in improving the system performance and reducing computational complexity for 5G-and-beyond networks. In this paper, we propose a new channel estimation method with the assistance of deep learning in order to support the least squares estimation, which is a low-cost method but having relatively high channel estimation errors. This goal is achieved by utilizing a MIMO (multiple-input multiple-output) system with a multi-path channel profile used for simulations in the 5G networks under the severity of Doppler effects. Numerical results demonstrate the superiority of the proposed deep learning-assisted channel estimation method over the other channel estimation methods in previous works in terms of mean square errors.",,10.1109/IMCOM51814.2021.9377351,Acquired Immunodeficiency Syndrome;Learning
105,Adversarial machine learning for 5G communications security,"YE Sagduyu, T Erpek, Y Shi",… Theory and Machine Learning for …,ISBN:9781119723943,,,270–288,2021,Wiley Online Library,https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119723950.ch14,"Machine learning provides automated means to capture complex dynamics of wireless spectrum and support better understanding of spectrum resources and their efficient utilization. As communication systems become smarter with cognitive radio capabilities empowered by machine learning to perform critical tasks such as spectrum awareness and spectrum sharing, they also become susceptible to new vulnerabilities due to the attacks that target the machine learning applications. This chapter identifies the emerging attack surface of adversarial machine learning and corresponding attacks launched against wireless communications in the context of 5G systems. The focus is on attacks against (i) spectrum sharing of 5G communications with incumbent users such as in the Citizens Broadband Radio Service (CBRS) band and (ii) physical layer authentication of 5G User Equipment (UE) to support network slicing. For the first attack, the adversary transmits during data transmission or spectrum sensing periods to manipulate the signal‐level inputs to the deep learning classifier that is deployed at the Environmental Sensing Capability (ESC) to support the 5G system. For the second attack, the adversary spoofs wireless signals with the generative adversarial network (GAN) to infiltrate the physical layer authentication mechanism based on a deep learning classifier that is deployed at the 5G base station. Results indicate major vulnerabilities of 5G systems to adversarial machine learning. To sustain the 5G system operations in the presence of adversaries, a defense mechanism is presented to increase the uncertainty of the adversary in training the surrogate model used for launching its subsequent attacks.",,10.1002/9781119723950.ch14,"Adversarial machine learning, deep learning, 5G, spectrum sharing, authentication, network slicing, GAN, jamming, spoofing",,,,,,,,,,
106,"Artificial Intelligence and Machine Learning in 5G Network Security: Opportunities, advantages, and future research trends","N Haider, MZ Baig, M Imran",arXiv preprint arXiv:2007.04490,,,,1 – 7,2020,arxiv.org,https://arxiv.org/abs/2007.04490,"Recent technological and architectural advancements in 5G networks have proven their worth as the deployment has started over the world. Key performance elevating factor from access to core network are softwareization, cloudification and virtualization of key enabling network functions. Along with the rapid evolution comes the risks, threats and vulnerabilities in the system for those who plan to exploit it. Therefore, ensuring fool proof end-to-end (E2E) security becomes a vital concern. Artificial intelligence (AI) and machine learning (ML) can play vital role in design, modelling and automation of efficient security protocols against diverse and wide range of threats. AI and ML has already proven their effectiveness in different fields for classification, identification and automation with higher accuracy. As 5G networks’ primary selling point has been higher data rates and speed, it will be difficult to tackle wide range of threats from different points using typical/traditional protective measures. Therefore, AI and ML can play central role in protecting highly data-driven softwareized and virtualized network components. This article presents AI and ML driven applications for 5G network security, their implications and possible research directions. Also, an overview of key data collection points in 5G architecture for threat classification and anomaly detection are discussed.",,10.48550/arXiv.2007.04490 ,"5G Security, Artificial Intelligence, Machine Learning, Attacks and Threats, Threat classification.",,,,,,,,,,
107,Deep-Learning-Based Intelligent Intervehicle Distance Control for 6G-Enabled Cooperative Autonomous Driving,"Xiaosha Chen , Supeng Leng , Jianhua He , Longyu Zhou ",IEEE Internet of Things Journal,2372-2541,8,20,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9310315,"Research on the sixth-generation cellular networks (6G) is gaining huge momentum to achieve ubiquitous wireless connectivity. Connected autonomous vehicles (CAVs) is a critical vertical application for 6G, holding great potentials of improving road safety, road and energy efficiency. However, the stringent service requirements of CAV applications on reliability, latency, and high speed communications will present big challenges to 6G networks. New channel access algorithms and intelligent control schemes for connected vehicles are needed for 6G-supported CAV. In this article, we investigated 6G-supported cooperative driving, which is an advanced driving mode through information sharing and driving coordination. First, we quantify the delay upper bounds of 6G vehicle-to-vehicle (V2V) communications with hybrid communication and channel access technologies. A deep learning neural network is developed and trained for the fast computation of the delay bounds in real-time operations. Then, an intelligent strategy is designed to control the intervehicle distance for cooperative autonomous driving. Furthermore, we propose a Markov chain-based algorithm to predict the parameters of the system states, and also a safe distance mapping method to enable smooth vehicular speed changes. The proposed algorithms are implemented in the AirSim autonomous driving platform. Simulation results show that the proposed algorithms are effective and robust with safe and stable cooperative autonomous driving, which greatly improve the road safety, capacity, and efficiency.",,10.1109/JIOT.2020.3048050,"6G mobile communication , Delays , Autonomous vehicles , Upper bound , Safety , Real-time systems , Prediction algorithms ",,,,,,,,,,
108,Throughput Prediction Using Machine Learning in LTE and 5G Networks,"Dimitar Minovski , Niclas Ögren , Karan Mitra , Christer Åhlund ",IEEE Transactions on Mobile Computing,2161-9875,22,3,,2023,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9495144,"The emergence of novel cellular network technologies, within 5G, are envisioned as key enablers of a new set of use-cases, including industrial automation, intelligent transportation, and tactile internet. The critical nature of the traffic requirements ranges from ultra-reliable communications, massive connectivity, and enhanced mobile broadband. Thus, the growing research on cellular network monitoring and prediction aims for ensuring a satisfied user-base and fulfillment of service level agreements. The scope of this study is to develop an approach for predicting the cellular link throughput of end-users, with a goal to benchmark the performance of network slices. First, we report and analyze a measurement study involving real-life cases, such as driving in urban, sub-urban, and rural areas, as well as tests in large crowded areas. Second, we develop machine learning models using lower-layer metrics, describing the radio environment, to predict the available throughput. The models are initially validated on the LTE network and then applied to a non-standalone 5G network. Finally, we suggest scaling the proposed model into the future standalone 5G network. We have achieved 93 and 84 percent $R^2$R2 accuracy, with 0.06 and 0.17 mean squared error, in predicting the end-user's throughput in LTE and non-standalone 5G network, respectively.",,10.1109/TMC.2021.3099397,"Throughput , Long Term Evolution , 5G mobile communication , Quality of service , Measurement , Predictive models , Interference ",,,,,,,,,,
109,"From 5G to 6G technology: meets energy, internet-of-things and machine learning: a survey","MN Mahdi, AR Ahmad, QS Qassim, H Natiq, ...",Applied Sciences,2076-3417,11,17,8117,2021,mdpi.com,https://www.mdpi.com/2076-3417/11/17/8117,"Due to the rapid development of the fifth-generation (5G) applications, and increased demand for even faster communication networks, we expected to witness the birth of a new 6G technology within the next ten years. Many references suggested that the 6G wireless network standard may arrive around 2030. Therefore, this paper presents a critical analysis of 5G wireless networks’, significant technological limitations and reviews the anticipated challenges of the 6G communication networks. In this work, we have considered the applications of three of the highly demanding domains, namely: energy, Internet-of-Things (IoT) and machine learning. To this end, we present our vision on how the 6G communication networks should look like to support the applications of these domains. This work presents a thorough review of 370 papers on the application of energy, IoT and machine learning in 5G and 6G from three major libraries: Web of Science, ACM Digital Library, and IEEE Explore. The main contribution of this work is to provide a more comprehensive perspective, challenges, requirements, and context for potential work in the 6G communication standard.",,10.3390/app11178117,"5G 6G energy, deep learning, machine learning, Internet-of-Things, IoT, security",,,,,,,,,,
110,A Survey of Machine Learning Applications to Handover Management in 5G and Beyond,"Michael S. Mollel , Attai Ibrahim Abubakar , Metin Ozturk , Shubi Felix Kaijage , Michael Kisangiri , Sajjad Hussain , Muhammad Ali Imran , Qammer H. Abbasi ",IEEE Access,2169-3536,9,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9381854,"Handover (HO) is one of the key aspects of next-generation (NG) cellular communication networks that need to be properly managed since it poses multiple threats to quality-of-service (QoS) such as the reduction in the average throughput as well as service interruptions. With the introduction of new enablers for fifth-generation (5G) networks, such as millimetre wave (mm-wave) communications, network densification, Internet of things (IoT), etc., HO management is provisioned to be more challenging as the number of base stations (BSs) per unit area, and the number of connections has been dramatically rising. Considering the stringent requirements that have been newly released in the standards of 5G networks, the level of the challenge is multiplied. To this end, intelligent HO management schemes have been proposed and tested in the literature, paving the way for tackling these challenges more efficiently and effectively. In this survey, we aim at revealing the current status of cellular networks and discussing mobility and HO management in 5G alongside the general characteristics of 5G networks. We provide an extensive tutorial on HO management in 5G networks accompanied by a discussion on machine learning (ML) applications to HO management. A novel taxonomy in terms of the source of data to be utilized in training ML algorithms is produced, where two broad categories are considered; namely, visual data and network data. The state-of-the-art on ML-aided HO management in cellular networks under each category is extensively reviewed with the most recent studies, and the challenges, as well as future research directions, are detailed.",,10.1109/ACCESS.2021.3067503,"5G mobile communication , Visualization , Cellular networks , 6G mobile communication , Bandwidth , Taxonomy , Quality of service ",,,,,,,,,,
111,Customized Slicing for 6G: Enforcing Artificial Intelligence on Resource Management,"Wanqing Guan , Haijun Zhang , Victor C. M. Leung ",IEEE Network,1558-156X,35,5,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9382026,"Next generation wireless networks are expected to support diverse vertical industries and offer countless emerging use cases. To satisfy stringent requirements of diversified services, network slicing is developed, which enables service-oriented resource allocation by tailoring the infrastructure network into multiple logical networks. However, there are still some challenges in cross-domain multi-dimensional resource management for end-to-end (E2E) slices under the dynamic and uncertain environment. Trading off the revenue and cost of resource allocation while guaranteeing service quality is significant to tenants. Therefore, this article introduces a hierarchical resource management framework, utilizing deep reinforcement learning in admission control of resource requests from different tenants and resource adjustment within admitted slices for each tenant. In particular, we first discuss the challenges in customized resource management of 6G. Second, the motivation and background are presented to explain why artificial intelligence (AI) is applied in resource customization of multi-tenant slicing. Third, E2E resource management is decomposed into two problems, multi-dimensional resource allocation decision based on slice-level feedback, and real-time slice adaption aimed at avoiding service quality degradation. Simulation results demonstrate the effectiveness of AI-based customized slicing. Finally, several significant challenges that need to be addressed in practical implementation are investigated.",,10.1109/MNET.011.2000644,"Resource management , 6G mobile communication , Network slicing , Dynamic scheduling , Decision making , Real-time systems ",,,,,,,,,,
112,UAVs joint optimization problems and machine learning to improve the 5G and Beyond communication,"Z Ullah, F Al-Turjman, U Moatasim, L Mostarda, ...",Computer Networks,1389-1286,182,,107478,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S1389128620311518?casa_token=M03r0XT3-qgAAAAA:vbFzkM_Bs9IS9IQeLrFFrHaVEuxwHb1GbcIZroXjC438IsaJjEN8cmGrHUmmiqmIEYAkzOBKr00,"Recently, unmanned aerial vehicles (UAVs) have gained notable interest in various applications such as wireless coverage, aerial surveillance, precision agriculture, construction, power lines monitoring and blood delivery, etc. The UAVs implicit attributes e.g., rapid deployment, quick mobility, increase in flight duration, improvements in payload capacities, etc. , place it as an effective candidate for many applications in 5G and Beyond communications. The UAVs-assisted next-generation communications are determined to be highly influenced by various techniques and technologies like artificial intelligence (AI), machine learning (ML), deep reinforcement learning (DRL), mobile edge computing (MEC), and software-defined networks (SDN). In this article, we develop a review to investigate the UAVs joint optimization problems to enhance system efficiency. We classify the joint optimization problems based on the number of parameters used in proposed optimization problems. Moreover, we explore the impact of AI, ML, DRL, MEC, and SDN over UAVs joint optimization problems and present future research challenges and directions.",,10.1016/j.comnet.2020.107478.,"5G and B5G communication, UAVs, Artificial intelligence, Machine learning, Mobile edge computing, Software-defined networks, Internet of Things, mmWave communication, Smart city, Joint optimization",,,,,,,,,,
113,Machine learning-based 5G-and-beyond channel estimation for MIMO-OFDM communication systems,"HA Le, T Van Chien, TH Nguyen, H Choo, VD Nguyen",Sensors,1424-8220,21,14,4861,2021,mdpi.com,https://www.mdpi.com/1191314,"Channel estimation plays a critical role in the system performance of wireless networks. In addition, deep learning has demonstrated significant improvements in enhancing the communication reliability and reducing the computational complexity of 5G-and-beyond networks. Even though least squares (LS) estimation is popularly used to obtain channel estimates due to its low cost without any prior statistical information regarding the channel, this method has relatively high estimation error. This paper proposes a new channel estimation architecture with the assistance of deep learning in order to improve the channel estimation obtained by the LS approach. Our goal is achieved by utilizing a MIMO (multiple-input multiple-output) system with a multi-path channel profile for simulations in 5G-and-beyond networks under the level of mobility expressed by the Doppler effects. The system model is constructed for an arbitrary number of transceiver antennas, while the machine learning module is generalized in the sense that an arbitrary neural network architecture can be exploited. Numerical results demonstrate the superiority of the proposed deep learning-based channel estimation framework over the other traditional channel estimation methods popularly used in previous works. In addition, bidirectional long short-term memory offers the best channel estimation quality and the lowest bit error ratio among the considered artificial neural network architectures.",,10.3390/s21144861,"machine learning, channel estimation, MIMO-OFDM, frequency selective channels",,,,,,,,,,
114,Machine learning in beyond 5G/6G networks—State-of-the-art and future trends,"VP Rekkas, S Sotiroudis, P Sarigiannidis, S Wan, ...",Electronics,2079-9292,10,22,1–28,2021,mdpi.com,https://www.mdpi.com/1357474,"Artificial Intelligence (AI) and especially Machine Learning (ML) can play a very important role in realizing and optimizing 6G network applications. In this paper, we present a brief summary of ML methods, as well as an up-to-date review of ML approaches in 6G wireless communication systems. These methods include supervised, unsupervised and reinforcement techniques. Additionally, we discuss open issues in the field of ML for 6G networks and wireless communications in general, as well as some potential future trends to motivate further research into this area.",,10.3390/electronics10222786,"6G, wireless communications, artificial intelligence, machine learning",,,,,,,,,,
115,Nine Challenges in Artificial Intelligence and Wireless Communications for 6G,"Wen Tong , Geoffrey Ye Li ",IEEE Wireless Communications,1558-0687,29,4,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9770094,"In recent years, artificial intelligence (AI) techniques, especially machine learning (ML), have been successfully applied in various areas, leading to a widespread belief that AI will collectively play an important role in future wireless communications. To accomplish the aspiration, we present nine challenges to be addressed by the interdisciplinary areas of AI/ML and wireless communications, with particular focus on the sixth generation (6G) wireless networks. Specifically, this article classifies the nine challenges into computation in AI, distributed neural networks and learning, and semantic communications.",,10.1109/MWC.006.2100543,"Artificial intelligence , Biological neural networks , Neural networks , 6G mobile communication , Deep learning , Wireless networks , Data models ",,,,,,,,,,
116,Leveraging machine-learning for D2D communications in 5G/beyond 5G networks,"S Hashima, BM ElHalawany, K Hatano, K Wu, ...",Electronics,2079-9292,10,2,1–16,2021,mdpi.com,https://www.mdpi.com/960106,"Device-to-device (D2D) communication is a promising paradigm for the fifth generation (5G) and beyond 5G (B5G) networks. Although D2D communication provides several benefits, including limited interference, energy efficiency, reduced delay, and network overhead, it faces a lot of technical challenges such as network architecture, and neighbor discovery, etc. The complexity of configuring D2D links and managing their interference, especially when using millimeter-wave (mmWave), inspire researchers to leverage different machine-learning (ML) techniques to address these problems towards boosting the performance of D2D networks. In this paper, a comprehensive survey about recent research activities on D2D networks will be explored with putting more emphasis on utilizing mmWave and ML methods. After exploring existing D2D research directions accompanied with their existing conventional solutions, we will show how different ML techniques can be applied to enhance the D2D networks performance over using conventional ways. Then, still open research directions in ML applications on D2D networks will be investigated including their essential needs. A case study of applying multi-armed bandit (MAB) as an efficient online ML tool to enhance the performance of neighbor discovery and selection (NDS) in mmWave D2D networks will be presented. This case study will put emphasis on the high potency of using ML solutions over using the conventional non-ML based methods for highly improving the average throughput performance of mmWave NDS.",,10.3390/electronics10020169 ,"D2D communication, mmWave, machine-learning applications, 5G, B5G",,,,,,,,,,
117,The Role of Deep Learning in NOMA for 5G and Beyond Communications,"Moh. Khalid Hasan , Md. Shahjalal , Md. Mainul Islam , Md. Morshed Alam , Md. Faisal Ahmed , Yeong Min Jang ",2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9065219,"In the coming future, it is obvious that the wireless networks will be congested with massive amounts of data traffic with the increasing number of users. Current multiple access techniques will certainly not have the capability to efficiently serve in the massively congested scenarios. In recent times, nonorthogonal multiple access (NOMA) has been recognized as an immensely potential technique for 5G and beyond communications that can increase spectral efficiency to a greater extent serving a vast number of users. However, several circumscriptions are observed in NOMA such as the requirement of a perfect channel state information in the transmitter and high computational complexity in the receiver. The use of deep learning (DL) techniques is a great resolution to deal with the challenges. This paper discusses the applications of the DL methods in NOMA for 5G and beyond communications. Firstly, the deep neural networks that are employed in NOMA are listed up and discussed. After that, their functions are studied specifically focusing on how to improve the NOMA performance. Finally, the possible future challenges and research issues are identified at the end of the paper.",,10.1109/ICAIIC48513.2020.9065219,"NOMA , Resource management , 5G mobile communication , Neural networks , Training , Machine learning , Silicon carbide ",,,,,,,,,,
118,Implementing deep learning techniques in 5G IoT networks for 3D indoor positioning: DELTA (DeEp Learning-Based Co-operaTive Architecture),"B El Boudani, L Kanaris, A Kokkinis, M Kyriacou, ...",Sensors,1424-8220,20,19,5495,2020,mdpi.com,https://www.mdpi.com/838928,"In the near future, the fifth-generation wireless technology is expected to be rolled out, offering low latency, high bandwidth and multiple antennas deployed in a single access point. This ecosystem will help further enhance various location-based scenarios such as assets tracking in smart factories, precise smart management of hydroponic indoor vertical farms and indoor way-finding in smart hospitals. Such a system will also integrate existing technologies like the Internet of Things (IoT), WiFi and other network infrastructures. In this respect, 5G precise indoor localization using heterogeneous IoT technologies (Zigbee, Raspberry Pi, Arduino, BLE, etc.) is a challenging research area. In this work, an experimental 5G testbed has been designed integrating C-RAN and IoT networks. This testbed is used to improve both vertical and horizontal localization (3D Localization) in a 5G IoT environment. To achieve this, we propose the DEep Learning-based co-operaTive Architecture (DELTA) machine learning model implemented on a 3D multi-layered fingerprint radiomap. The DELTA begins by estimating the 2D location. Then, the output is recursively used to predict the 3D location of a mobile station. This approach is going to benefit use cases such as 3D indoor navigation in multi-floor smart factories or in large complex buildings. Finally, we have observed that the proposed model has outperformed traditional algorithms such as Support Vector Machine (SVM) and K-Nearest Neighbor (KNN).",,10.3390/s20195495,"5G IoT, indoor positioning, deep learning, tracking, localization, navigation, positioning accuracy, single access point positioning, Internet of Things",,,,,,,,,,
119,"A Review of Deep Learning in 5G Research: Channel Coding, Massive MIMO, Multiple Access, Resource Allocation, and Network Security","Amanda Ly , Yu-Dong Yao ",IEEE Open Journal of the Communications Society,2644-125X,2,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9353849,"The current development of 5G technology is flourishing with widespread deployment across the world at a rapid pace. However, there is still a demand concerning 5G research for service and performance improvement. Research tasks include but are not limited to quality-of-service (QoS), energy efficiency, massive connectivity, reliable communications, and security. Due to the advancement of deep learning, numerous such research has utilized this technique. This article provides a comprehensive review of 5G communications research using deep learning. Specifically, we address the issues of low-density parity-check (LDPC) coding, massive multiple-input multiple-output (MIMO), non-orthogonal multiple access (NOMA), resource allocation, and security.",,10.1109/OJCOMS.2021.3058353,"Deep learning , NOMA , 5G mobile communication , Wireless networks , Massive MIMO , Quality of service , Parity check codes ",,,,,,,,,,
120,5G Signal Identification Using Deep Learning,"Mohsen H. Alhazmi , Mofadal Alymani , Hatim Alhazmi , Alhussain Almarhabi , Abdullah Samarkandi , Yu-Dong Yao ",2020 29th Wireless and Optical Communications Conference (WOCC),2379-1268,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9114912,"Spectrum awareness, including identifying different types of signals, is very important in a cellular system environment. In this paper, a neural network is utilized to identify 5G signals among different cellular communications signals, including Long-Term Evolution (LTE) and Universal Mobile Telecommunication Service (UMTS). We explore the use of deep learning in wireless communications systems. We consider the effects of training dataset size, features extracted, and channel fading in our study. Experiment results demonstrate the effectiveness of deep learning neural networks in identifying cellular system signals, including UMTS, LTE, and 5G.",,10.1109/WOCC48579.2020.9114912,"Deep learning , Wireless communication , Fading channels , Training , 5G mobile communication , Neural networks , 3G mobile communication ",,,,,,,,,,
121,A tutorial on ultra-reliable and low-latency communications in 6G: Integrating domain knowledge into deep learning,"C She, C Sun, Z Gu, Y Li, C Yang, HV Poor, ...",arXiv preprint arXiv …,1558-2256,109,3,204–246,2020,arxiv.org,https://arxiv.org/abs/2009.06010," As one of the key communication scenarios in the 5th and also the 6th generation (6G) of mobile communication networks, ultra-reliable and low-latency communications (URLLC) will be central for the development of various emerging mission-critical applications. State-of-the-art mobile communication systems do not fulfill the end-to-end delay an d overall reliability requirements of URLLC. In particular, a holistic framework that takes into account latency, reliability, availability, scalability, and decision making under uncertainty is lacking. Driven by recent breakthroughs in deep neural networks, deep learning algorithms have been considered as promising ways of developing enabling technologies for URLLC in future 6G networks. This tutorial illustrates how domain knowledge (models, analytical tools, and optimization frameworks) of communications and networking can be integrated into different kinds of deep learning algorithms for URLLC. We first provide some background of URLLC and review promising network architectures and deep learning frameworks for 6G. To better illustrate how to improve learning algorithms wit h domain knowledge, we revisit model-based analytical tools and cross-layer optimization frameworks for URLLC. Following that, we examine the potential of applying supervised/unsupervised deep learning and deep reinforcement learning in URLLC and summarize related open problems. Finally, we provide simulation and experimental results to validate the effectiveness of different learning algorithms and discuss future directions.",,10.1109/JPROC.2021.3053601,"Ultra-reliable and low-latency communications, 6G, cross-layer optimization, supervised deep learning, unsupervised deep learning, deep reinforcement learning.",,,,,,,,,,
122,RF Fingerprinting and Deep Learning Assisted UE Positioning in 5G,"M. Majid Butt , Anil Rao , Daejung Yoon ",2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring),1090-3038,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9128640,"In this work, we investigate user equipment (UE) positioning assisted by deep learning (DL) in 5G and beyond networks. As compared to state of the art positioning algorithms used in today’s networks, radio signal fingerprinting and machine learning (ML) assisted positioning requires smaller additional feedback overhead; and the positioning estimates are made directly inside the radio access network (RAN), thereby assisting in radio resource management. The conventional positioning algorithms will be used as back-up for the environments with high variability in conditions; but ML-assisted positioning serves as more efficient and simpler technique to provide better or similar positioning accuracy. In this regard, we study ML-assisted positioning methods and evaluate their performance using system level simulations for an outdoor scenario in Lincoln park Chicago. The study is based on the use of raytracing tools, a 3GPP 5G NR compliant system level simulator and DL framework to estimate positioning accuracy of the UE. The use of raytracing tool and system level simulator helps avoid expensive drive test measurements in practical scenarios. Our proposed mechanism is a first step towards more proactive mobility management in future networks. We evaluate and compare performance of various DL models and show mean positioning error in the range of 1-1.5m for the best DL configuration with appropriate system feature-modeling.",,10.1109/VTC2020-Spring48590.2020.9128640,"Training , 3GPP , Position measurement , Artificial neural networks , Deep learning , 5G mobile communication , Data models ",,,,,,,,,,
123,Assessment of deep learning methodology for self-organizing 5g networks,"MZ Asghar, M Abbas, K Zeeshan, P Kotilainen, ...",Applied Sciences,2076-3417,9,15,2975,2019,mdpi.com,https://www.mdpi.com/502838,"In this paper, we present an auto-encoder-based machine learning framework for self organizing networks (SON). Traditional machine learning approaches, for example, K Nearest Neighbor, lack the ability to be precisely predictive. Therefore, they can not be extended for sequential data in the true sense because they require a batch of data to be trained on. In this work, we explore artificial neural network-based approaches like the autoencoders (AE) and propose a framework. The proposed framework provides an advantage over traditional machine learning approaches in terms of accuracy and the capability to be extended with other methods. The paper provides an assessment of the application of autoencoders (AE) for cell outage detection. First, we briefly introduce deep learning (DL) and also shed light on why it is a promising technique to make self organizing networks intelligent, cognitive, and intuitive so that they behave as fully self-configured, self-optimized, and self-healed cellular networks. The concept of SON is then explained with applications of intrusion detection and mobility load balancing. Our empirical study presents a framework for cell outage detection based on an autoencoder using simulated data obtained from a SON simulator. Finally, we provide a comparative analysis of the proposed framework with the existing frameworks.",,10.3390/app9152975,"deep learning (DL), self-organizing networks (SON); 5G, autoencoder (AE), mobility load balancing (MLB), cell outage detection, intrusion detection",,,,,,,,,,
124,Deep Learning at the Edge for Channel Estimation in Beyond-5G Massive MIMO,"Mauro Belgiovine , Kunal Sankhe , Carlos Bocanegra , Debashri Roy , Kaushik R. Chowdhury ",IEEE Wireless Communications,1558-0687,28,2,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9430899,"Massive multiple-input multiple-output (mMIMO) is a critical component in upcoming 5G wireless deployment as an enabler for high data rate communications. mMIMO is effective when each corresponding antenna pair of the respective transmitter-receiver arrays experiences an independent channel. While increasing the number of antenna elements increases the achievable data rate, at the same time computing the channel state information (CSI) becomes prohibitively expensive. In this article, we propose to use deep learning via a multi-layer perceptron architecture that exceeds the performance of traditional CSI processing methods like least square (LS) and linear minimum mean square error (LMMSE) estimation, thus leading to a beyond fifth generation (B5G) networking paradigm wherein machine learning fully drives networking optimization. By computing the CSI of all pairwise channels simultaneously via our deep learning approach, our method scales with large antenna arrays as opposed to traditional estimation methods. The key insight here is to design the learning architecture such that it is implementable on massively parallel architectures, such as GPU or FPGA. We validate our approach by simulating a 32-element array base station and a user equipment with a 4-element array operating on millimeter-wave frequency band. Results reveal an improvement up to five and two orders of magnitude in BER with respect to fastest LS estimation and optimal LMMSE, respectively, substantially improving the end-to-end system performance and providing higher spatial diversity for lower SNR regions, achieving up to 4 dB gain in received power signal compared to performance obtained through LMMSE estimation.",,10.1109/MWC.001.2000322,"MIMO , Edge computing , 5G mobile communication , Channel estimation , Deep learning ",,,,,,,,,,
125,Trustworthy Deep Learning in 6G-Enabled Mass Autonomy: From Concept to Quality-of-Trust Key Performance Indicators,"Chen Li , Weisi Guo , Schyler Chengyao Sun , Saba Al-Rubaye , Antonios Tsourdos ",IEEE Vehicular Technology Magazine,1556-6080,15,4,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9210233,"Mass autonomy promises to revolutionize a wide range of engineering, service, and mobility industries. Coordinating complex communication among hyperdense autonomous agents requires new artificial intelligence (AI)-enabled orchestration of wireless communication services beyond 5G and 6G mobile networks. In particular, safety and mission-critical tasks will legally require both transparent AI decision processes and quantifiable quality-of-trust (QoT) metrics for a range of human end users (consumer, engineer, and legal). We outline the concept of trustworthy autonomy for 6G, including essential elements such as how explainable AI (XAI) can generate the qualitative and quantitative modalities of trust. We also provide XAI test protocols for integration with radio resource management and associated key performance indicators (KPIs) for trust. The research directions proposed will enable researchers to start testing existing AI optimization algorithms and develop new ones with the view that trust and transparency should be built in from the design through the testing phase.",,10.1109/MVT.2020.3017181,"Artificial intelligence , Cognition , Bit rate , Analytical models , Quality of service , Solid modeling , 5G mobile communication , 6G mobile communication , Decision making , Trust management ",,,,,,,,,,
126,Automation of 5G Network Slice Control Functions with Machine Learning,"Ved P. Kafle , Pedro Martinez-Julia , Takaya Miyazawa ",IEEE Communications Standards Magazine,2471-2833,3,3,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8928169,"5G communication networks will be complex due to the emergence of an unprecedented huge number of new types of connected devices and services. Moreover, the on-demand creation of virtual network slices, each suitable for a different application, is posing challenges to the efficient management of network resources, while optimally satisfying the quality of service requirements in time-varying workloads and network conditions. This article, which is tutorial in nature, introduces 5G network slices (from the point of view of the non-wireless part of the network) and elaborates the necessity of automation of network functions related to the design, construction, deployment, operation, control, and management of network slices. It revisits machine learning techniques applicable to the automation of network functions. It then presents a machine-learning-based framework for the operation and control of network slices by continuously monitoring workload, performance, and resource utilization, and dynamically adjusting the resources allocated to network slices. Preliminary results of workload prediction accuracy obtained from the analysis of real-life data collected from a web server are also reported.",,10.1109/MCOMSTD.001.1900010,"Machine learning , 5G mobile communication , Automation , Monitoring , Resource management , Quality of service , Network security , Communication networks ",,,,,,,,,,
127,Fast Initial Access with Deep Learning for Beam Prediction in 5G mmWave Networks,"Tarun S. Cousik , Vijay K. Shah , Jeffrey H. Reed , Tugba Erpek , Yalin E. Sagduyu ",MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM),2155-7578,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9653011,"We present DeepIA, a deep learning solution for a fast, reliable and secure initial access (IA) in directional networks such as the mmWave networks in 5G systems. By utilizing only a subset of beams during the IA process, DeepIA removes the need for an exhaustive beam search thereby reducing the beam sweep time in IA. A deep neural network (DNN) is trained to learn the complex mapping from the received signal strengths (RSSs) collected with a reduced number of beams to the optimal spatial beam of the receiver (among a larger set of beams). In test time, DeepIA measures the RSSs only from a small number of beams and runs the DNN to predict the best beam for IA. We show that DeepIA reduces the IA time by sweeping fewer beams and significantly outperforms the conventional IA&#x0027;s beam prediction accuracy in both line of sight (LoS) and non-line of sight (NLoS) mmWave channel conditions.",,10.1109/MILCOM52596.2021.9653011,"Deep learning , Military communication , 5G mobile communication , Simulation , Conferences , Neural networks , Receivers ",,,,,,,,,,
128,Usage of Network Simulators in Machine-Learning-Assisted 5G/6G Networks,"Francesc Wilhelmi , Marc Carrascosa , Cristina Cano , Anders Jonsson , Vishnu Ram , Boris Bellalta ",IEEE Wireless Communications,1558-0687,28,1,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9363049,"Without any doubt, Machine Learning (ML) will be an important driver of future communications due to its foreseen performance when applied to complex problems. However, the application of ML to networking systems raises concerns among network operators and other stakeholders, especially regarding trustworthiness and reliability. In this article, we devise the role of network simulators for bridging the gap between ML and communications systems. In particular, we present an architectural integration of simulators in ML-aware networks for training, testing, and validating ML models before being applied to the operative network. Moreover, we provide insights into the main challenges resulting from this integration, and then give hints discussing how they can be overcome. Finally, we illustrate the integration of network simulators into ML-assisted communications through a proof-of-concept testbed implementation of a residential WiFi network.",,10.1109/MWC.001.2000206,"Training data , Communication systems , Machine learning , Stakeholders , Reliability , Wireless fidelity , 5G mobile communication , 6G mobile communication ",,,,,,,,,,
129,Towards 6G IoT: tracing mobile sensor nodes with deep learning clustering in UAV networks,"Y Spyridis, T Lagkas, P Sarigiannidis, V Argyriou, ...",Sensors, 1424-8220,21,11,3936,2021,mdpi.com,https://www.mdpi.com/1141088,"Unmanned aerial vehicles (UAVs) in the role of flying anchor nodes have been proposed to assist the localisation of terrestrial Internet of Things (IoT) sensors and provide relay services in the context of the upcoming 6G networks. This paper considered the objective of tracing a mobile IoT device of unknown location, using a group of UAVs that were equipped with received signal strength indicator (RSSI) sensors. The UAVs employed measurements of the target’s radio frequency (RF) signal power to approach the target as quickly as possible. A deep learning model performed clustering in the UAV network at regular intervals, based on a graph convolutional network (GCN) architecture, which utilised information about the RSSI and the UAV positions. The number of clusters was determined dynamically at each instant using a heuristic method, and the partitions were determined by optimising an RSSI loss function. The proposed algorithm retained the clusters that approached the RF source more effectively, removing the rest of the UAVs, which returned to the base. Simulation experiments demonstrated the improvement of this method compared to a previous deterministic approach, in terms of the time required to reach the target and the total distance covered by the UAVs.",,10.3390/s21113936,"unmanned aerial vehicles, deep learning, IoT, 6G, graph convolutional network, sensor tracking, RSSI",,,,,,,,,,
130,Deep Learning Modalities for Biometric Alteration Detection in 5G Networks-Based Secure Smart Cities,"Ahmed Sedik , Lo’Ai Tawalbeh , Mohamed Hammad , Ahmed A. Abd El-Latif , Ghada M. El-Banby , Ashraf A. M. Khalaf , Fathi E. Abd El-Samie , Abdullah M. Iliyasu ",IEEE Access,2169-3536,9,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9450808,"Smart cities and their applications have become attractive research fields birthing numerous technologies. Fifth generation (5G) networks are important components of smart cities, where intelligent access control is deployed for identity authentication, online banking, and cyber security. To assure secure transactions and to protect user's identities against cybersecurity threats, strong authentication techniques should be used. The prevalence of biometrics, such as fingerprints, in authentication and identification makes the need to safeguard them important across different areas of smart applications. Our study presents a system to detect alterations to biometric modalities to discriminate pristine, adulterated, and fake biometrics in 5G-based smart cities. Specifically, we use deep learning models based on convolutional neural networks (CNN) and a hybrid model that combines CNN with convolutional long-short term memory (ConvLSTM) to compute a three-tier probability that a biometric has been tempered. Simulation-based experiments indicate that the alteration detection accuracy matches those recorded in advanced methods with superior performance in terms of detecting central rotation alteration to fingerprints. This makes the proposed system a veritable solution for different biometric authentication applications in secure smart cities.",,10.1109/ACCESS.2021.3088341,"Biometrics (access control) , Feature extraction , Authentication , Iris recognition , Fingerprint recognition , Smart cities , Veins ",,,,,,,,,,
131,Testbed for 5G Connected Artificial Intelligence on Virtualized Networks,"Cleverson Veloso Nahum , Lucas De Nóvoa Martins Pinto , Virgínia Brioso Tavares , Pedro Batista , Silvia Lins , Neiva Linder , Aldebaro Klautau ",IEEE Access,2169-3536,8,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9290141,"The fifth-generation (5G) cellular networks incorporate a large variety of technologies in order to address very distinct use cases. Assessing these technologies and investigating future alternatives is complicated when one relies only on simulators. 5G testbeds are an important alternative to simulators and many have been recently described, emphasizing aspects such as cloud functionalities, management and orchestration. This work presents a 5G mobile network testbed with a virtualized and orchestrated structure using containers, which focuses on integration to artificial intelligence (AI) applications. The presented testbed uses open-source technologies to deploy and orchestrate the virtual network functions (VNFs) to flexibly create various mobile network scenarios, with distinct fronthaul and backhaul topologies. Distinctive features of the testbed are its relatively low cost and the support to using AI for optimizing the network performance. The paper explains how to deploy the testbed structure and reproduce the presented results with the provided code. AI-based radio access network (RAN) slicing and VNF placement are used as examples of the testbed capabilities.",,10.1109/ACCESS.2020.3043876,"5G mobile communication , Network topology , Containers , Topology , Artificial intelligence , Open source software , Radio access networks ",,,,,,,,,,
132,5G Positioning - A Machine Learning Approach,M. Malmström and I. Skog and S. M. Razavi and Y. Zhao and F. Gunnarsson,"2019 16th Workshop on Positioning, Navigation and Communications (WPNC)",2164-9758     VO  - ,,,1-6,2019,IEEE,https://ieeexplore.ieee.org/abstract/document/8970186/,"In urban environments, cellular network-based positioning of user equipment (ue) is a challenging task, especially in frequently occurring non-line-of-sight (nlos) conditions. This paper investigates the use of two machine learning methods - neural networks and random forests - to estimate the position of ue in nlos using best received reference signal beam power measurements. We evaluated the suggested positioning methods using data collected from a fifth-generation cellular network (5g) testbed provided by Ericsson. A statistical test to detect nlos conditions with a probability of detection that is close to 90% is suggested. We show that knowledge of the antenna are crucial for accurate position estimation. In addition, our results show that even with a limited set of training data and one 5g transmission point, it is possible to position ue within 10 meters with 80% accuracy.",,10.1109/WPNC47567.2019.8970186,Learning,,,,,,,,,,
133,Integrating Artificial Intelligence Internet of Things and 5G for Next-Generation Smartgrid: A Survey of Trends Challenges and Prospect,"Ebenezer Esenogho , Karim Djouani , Anish M. Kurien ",IEEE Access,2169-3536,10,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9672084,"Smartgrid is a paradigm that was introduced into the conventional electricity network to enhance the way generation, transmission, and distribution networks interrelate. It involves the use of Information and Communication Technology (ICT) and other solution in fault and intrusion detection, mere monitoring of energy generation, transmission, and distribution. However, on one hand, the actual and earlier smartgrid, do not integrate more advanced features such as automatic decision making, security, scalability, self-healing and awareness, real-time monitoring, cross-layer compatibility, etc. On the other hand, the emergence of the digitalization of the communication infrastructure to support the economic sector which among them are energy generation and distribution grid with Artificial Intelligence (AI) and large-scale Machine to Machine (M2M) communication. With the future Massive Internet of Things (MIoT) as one of the pillars of 5G/6G network factory, it is the enabler to support the next generation smart grid by providing the needed platform that integrates, in addition to the communication infrastructure, the AI and IoT support, providing a multitenant system. This paper aim at presenting a comprehensive review of next smart grid research trends and technological background, discuss a futuristic next-generation smart grid driven by artificial intelligence (AI) and leverage by IoT and 5G. In addition, it discusses the challenges of next-generation smart-grids as it relate to the integration of AI, IoT and 5G for better smart grid architecture. Also, proffers possible solutions to some of the challenges and standards to support this novel trend. A corresponding future work will dwell on the implementation of the discussed integration of AI, IoT and 5G for next-generation smart grid, using Matlab, NS2/NS3, Open-daylight and Mininet as soft tools and compare with related literature.",,10.1109/ACCESS.2022.3140595,"Smart grids , Artificial intelligence , Next generation networking , 5G mobile communication , Internet of Things , Distribution networks , Market research ",,,,,,,,,,
134,Machine Learning Approach for Automatic Configuration and Management of 5G Platforms,"Talha Ahmed Khan , Asif Mehmood , Javier Jose Diaz Rivera , Wang-Cheol Song ",2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS),2576-8565,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8893119,"The automatic control over the network platforms is an esteem requirement of the operators. Recently, 5G with its aim to attain the internet of everything, it challenged researchers for the achievement of automatic control over the network platform. Furthermore, the 5G system consists of multi-domain network applications and platforms which make it complex to control and configure the network. Hence, the focus of this research is to enable easy configuration through high-level instructions and the use of machine learning for automatic control over the network infrastructure. The overall system consists of Intent-Base application that includes a machine learning model and it configures and controls the M-CORD-based network slicing test-bed.",,10.23919/APNOMS.2019.8893119,,,,,,,,,,,
135,Examining Machine Learning for 5G and Beyond Through an Adversarial Lens,"Muhammad Usama , Inaam Ilahi , Junaid Qadir , Rupendra Nath Mitra , Mahesh K. Marina ",IEEE Internet Computing,1941-0131,25,2,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9314254,"Spurred by the recent advances in deep learning to harness rich information hidden in large volumes of data and to tackle problems that are hard to model/solve (e.g., resource allocation problems), there is currently tremendous excitement in the mobile networks domain around the transformative potential of data-driven artificial intelligence/machine learning (AI/ML) based network automation, control and analytics for 5G and beyond. In this article, we present a cautionary perspective on the use of AI/ML in the 5G context by highlighting the adversarial dimension spanning multiple types of ML (supervised/unsupervised/reinforcement learning) and support this through three case studies. We also discuss approaches to mitigate this adversarial ML risk, offer guidelines for evaluating the robustness of ML models, and call attention to issues surrounding ML oriented research in 5G more generally.",,10.1109/MIC.2021.3049190,"5G mobile communication , Modulation , Computational modeling , Security , Context modeling , Cloud computing , Signal to noise ratio , Resource management , Learning (artificial intelligence) , Analytical models , Deep learning ",,,,,,,,,,
136,A Deep Learning Method for Predictive Channel Assignment in Beyond 5G Networks,"Sadman Sakib , Tahrat Tazrin , Mostafa M. Fouda , Zubair Md. Fadlullah , Nidal Nasser ",IEEE Network,1558-156X,35,1,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9237456,"In Beyond Fifth Generation (B5G) networks, Internet of Things (IoT) and massive Machine Type Communication (mMTC) traffic are anticipated to be offloaded by multi-hop, Device-to-Device (D2D)-enabled relay networks. The relays offer an energy and spectral-efficient solution to the rising problem of spectrum scarcity and overloading of cellular base stations. Moving beyond the conventional paradigm of the relay nodes employing channels on a specific band at a time, in this article, we aim to investigate how to simultaneously leverage multiple bands at a relay node to improve spectral efficiency. We address the challenge associated with dynamic channel conditions in the multi-band relay networks, and envision a deep learning-based predictive channel selection method to solve the problem. A 1-D (one-dimensional) Convolutional Neural Network (CNN) model is employed to predict the suitable channels across multiple bands with the best Signal-to-Interference-plus-Noise Ratio (SINR). The packets received from the source or previous relay node are scheduled to be transmitted to subsequent relay node/destination based on the best modulation and coding rates to transmit over the predicted band. Our envisioned approach, based on shallow and deep-CNN models, proposes two proactive channel assignment strategies, namely controlled and smart prediction. Our proposal is evaluated with several, comparable machine/deep learning methods. Experimental results, based on datasets, demonstrate encouraging performance of our proposed lightweight deep learning-based proactive channel selection in multi-band relay systems.",,10.1109/MNET.011.2000301,"Predictive models , Deep learning , Channel allocation , Data models , Signal to noise ratio , Relay networks (telecommunication) ",,,,,,,,,,
137,On recommendation-aware content caching for 6G: An artificial intelligence and optimization empowered paradigm,"Y Fu, KN Doan, TQS Quek",Digital Communications and Networks,2352-8648,6,3,304–311,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S2352864820301802,"Recommendation-aware Content Caching (RCC) at the edge enables a significant reduction of the network latency and the backhaul load, thereby invigorating ubiquitous latency-sensitive innovative services. However, the effectiveness of RCC strategies is highly dependent on explicit information as regards subscribers’ content request patterns, the sophisticated caching placement policy, and the personalized recommendation tactics. In this article, we investigate how the potentials of Artificial Intelligence (AI) and optimization techniques can be harnessed to address those core issues and facilitate the full implementation of RCC for the upcoming intelligent 6G era. Towards this end, we first elaborate on the hierarchical RCC network architecture. Then, the devised AI and optimization empowered paradigm is introduced, whereas AI and optimization techniques are leveraged to predict the users’ content preferences in real-time situations with the assistance of their historical behavior data and determine the cache pushing and recommendation decision, respectively. Through extensive case studies, we validate the effectiveness of AI-based predictors in estimating users’ content preference and the superiority of optimized RCC policies over the conventional benchmarks. At last, we shed light on the opportunities and challenges in the future.",,10.1016/j.dcan.2020.06.005.,"Artificial intelligence, Content caching, Optimization techniques, Recommendation, 6G",,,,,,,,,,
138,Deep learning based modulation classification for 5G and beyond wireless systems,"Clement, J. Christopher and Indira, N. and Vijayakumar, P. and Nandakumar, R.",Peer-to-Peer Networking and Applications,1936-6450,14,1,319-332,2021,,https://doi.org/10.1007/s12083-020-01003-3,"The 5G and beyond wireless networks will be more dynamic and heterogeneous, which needs to work on multistrand waveforms. One of the most significant challenges in such a dynamic network, especially non cooperated cases, is the identification of particular modulation type, which the transmitter uses at the given time to decode the data successfully. This research proposes a modulation classification algorithm using the combination architectures of modified convolutional neural network. The proposed deep learning architecture is developed by combining the convolutional neural network, dense network, and long short-term memory network (LSTM), which is named as convolutional LSTM dense neural network (CLDNN). Moreover, the mean cumulative sum metric (MCS) is introduced in the pooling layer for improved classification accuracy. Dimensionality reduction through Principal Component Analysis is also applied to minimize the training time, so that the proposed architecture can be adopted for its practical usage. The simulation results prove that the presented CLDNN outperforms an ordinary CNN, while taking less training time.",,10.1007/s12083-020-01003-3,"Convolutional neural network , Dense network , LSTM , Modulation classification , Learning ,   ,   ,   ,  ",,,,,,,,,,
139,"GPU-Based, LDPC Decoding for 5G and Beyond",C. Tarver and M. Tonnemacher and H. Chen and J. Zhang and J. R. Cavallaro,IEEE Open Journal of Circuits and Systems,2644-1225     VO  - 2,2,,278-290,2021,IEEE,https://ieeexplore.ieee.org/abstract/document/9336349/,"In 5G New Radio (NR), low-density parity-check (LDPC) codes are included as the error correction codes (ECC) for the data channel. While LDPC codes enable a low, near Shannon capacity, bit error rate (BER), they also become a computational bottleneck in the physical layer processing. Moreover, 5G LDPC has new challenges not seen in previous LDPC implementations, such as Wi-Fi. The LDPC specification in 5G includes many reconfigurations to support a variety of rates, block sizes, and use cases. 5G also creates targets for supporting high-throughput and low-latency applications. For this new, flexible standard, traditional hardware-based solutions in FGPA and ASIC may struggle to support all cases and may be cost-prohibitive at scale. Software solutions can trivially support all possible reconfigurations but struggle with performance. This article demonstrates the high-throughput and low-latency capabilities of graphics processing units (GPUs) for LDPC decoding as an alternative to FPGA and ASIC decoders, effectively providing the high performance needed while maintaining the benefits of a software-based solution. In particular, we highlight how by varying the parallelization strategy for mapping GPU kernels to blocks, we can use the many GPU cores to compute one codeword quickly to target low-latency, or we can use the cores to work on many codewords simultaneously to target high throughput applications. This flexibility is particularly useful for virtualized radio access networks (vRAN), a next-generation technology that is expected to become more prominent in the coming years. In vRAN, the hardware computational resources will become decoupled from the specific computational functions in the RAN through virtualization, allowing for benefits such as load-balancing, improved scalability, and reduced costs. To highlight and investigate how the GPU can accelerate tasks such as LDPC decoding when containerizing vRAN functionality, we integrate our decoder into the Open Air Interface (OAI) NR software stack. With our GPU-based decoder, we measure a best case-latency of  $87~\mu \text{s}$  and a best-case throughput of nearly 4 Gbps using the Titan RTX GPU.",,10.1109/OJCAS.2020.3042448,,,,,,,,,,,
140,Radar Aided 6G Beam Prediction: Deep Learning Algorithms and Real-World Demonstration,"Umut Demirhan , Ahmed Alkhateeb ",2022 IEEE Wireless Communications and Networking Conference (WCNC),1525-3511,,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9771564,"Adjusting the narrow beams at millimeter wave (mmWave) and terahertz (THz) MIMO communication systems is associated with high beam training overhead, which makes it hard for these systems to support highly-mobile applications. This overhead can potentially be reduced or eliminated if sufficient awareness about the transmitter/receiver locations and the surrounding environment is available. In this paper, efficient deep learning solutions that leverage radar sensory data are developed to guide the mmWave beam prediction and significantly reduce the beam training overhead. Our solutions integrate radar signal processing approaches to extract the relevant features for the learning models, and hence optimize their complexity and inference time. The proposed machine learning based radar-aided beam prediction solutions are evaluated using a large-scale real-world mmWave radar/communication dataset and their capabilities were demonstrated in a realistic vehicular communication scenario. In addition to completely eliminating the radar/communication calibration overhead, the proposed algorithms are able to achieve around 90% top-5 beam prediction accuracy while saving 93% of the beam training overhead. This highlights a promising direction for addressing the training overhead challenge in mmWave/THz communication systems.",,10.1109/WCNC51071.2022.9771564,"Training , Deep learning , Machine learning algorithms , Signal processing algorithms , Millimeter wave radar , Prediction algorithms , Feature extraction ",,,,,,,,,,
141,Survey on Artificial Intelligence Techniques in 5G Networks,"Refaee, Ali and Koucheryavy, Andrey",Telecom IT,,8,,1-10,2020,,https://www.researchgate.net/publication/342174425_SURVEY_ON_ARTIFICIAL_INTELLIGENCE_TECHNIQUES_IN_5G_NETWORKS,"Research subject. Fifth-generation telecommunication networks are currently the determining direction of telecommunications development as a whole. At the same time, the complexity of the processes of functioning of fifth-generation telecommunication networks increases by an order of magnitude compared to existing networks. All this requires the use of new technologies, including artificial intelligence, to ensure the stable functioning of telecommunication networks. Method. System analysis. Core results. The scientific tasks for the fifth generation communication networks, in which the use of artificial intelligence seems appropriate, including machine and deep learning, are identified. Practical significance. The results of the work can be useful in the education process in the field of networks and telecommunication systems, as well as for setting new scientific tasks for PhD students.",,10.31854/2307-1303-2020-8-1-1-10,"5G , Artificial intelligence , Machine learning , Deep learning , Intelligence ,   ,   ,   ,  ",,,,,,,,,,
142,A Machine Learning Approach for 5G SINR Prediction,"Ullah, Ruzat and Marwat, Safdar N. and Ahmad, Arbab M. and Ahmed, Salman and Hafeez, Abdul and Kamal, Tariq and Tufail, Muhammad",Electronics,2079-9292,9,10,,2020,,https://www.mdpi.com/2079-9292/9/10/1660,"Artificial Intelligence (AI) and Machine Learning (ML) are envisaged to play key roles in 5G networks. Efficient radio resource management is of paramount importance for network operators. With the advent of newer technologies, infrastructure, and plans, spending significant radio resources on estimating channel conditions in mobile networks poses a challenge. Automating the process of predicting channel conditions can efficiently utilize resources. To this point, we propose an ML-based technique, i.e., an Artificial Neural Network (ANN) for predicting SINR (Signal-to-Interference-and-Noise-Ratio) in order to mitigate the radio resource usage in mobile networks. Radio resource scheduling is generally achieved on the basis of estimated channel conditions, i.e., SINR with the help of Sounding Reference Signals (SRS). The proposed Non-Linear Auto Regressive External/Exogenous (NARX)-based ANN aims to minimize the rate of sending SRS and achieves an accuracy of R = 0.87. This can lead to vacating up to 4% of the spectrum, improving bandwidth efficiency and decreasing uplink power consumption.",,10.3390/electronics9101660,"5G , Machine learning , Artificial intelligence , Sounding reference signals , Signal-to-Interference-and-Noise-Ratio , Learning ,   ,   ,  ",,,,,,,,,,
143,Micro-Safe: Microservices- and Deep Learning-Based Safety-as-a-Service Architecture for 6G-Enabled Intelligent Transportation System,"Chandana Roy , Ruelia Saha , Sudip Misra , Kapal Dev ",IEEE Transactions on Intelligent Transportation Systems,1558-0016,23,7,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9548710,"In this paper, we propose a microservices and deep learning-based scheme, termed as Micro-Safe, for provisioning Safety-as-a-Service (Safe-aaS) in a 6G environment. A Safe-aaS infrastructure provides customized safety-related decisions dynamically to the registered end-users. As the decisions are time-sensitive in nature, the generation of these decisions should incur minimum latency and high accuracy. Further, scalability and extension of the coverage of the entire Safe-aaS platform are also necessary. Considering road transportation as the application scenario, we propose Safe-aaS, which is a microservices- and deep learning-based platform for provisioning ultra-low latency safety services to the end-users in a 6G scenario. We design the proposed solution in two stages. In the first stage, we develop the microservices-enabled application layer to improve the scalability and adaptability of the traditional Safe-aaS platform. Moreover, we apply the state space model to represent the decision parameters requested and the decision delivered to the end-users. During the second stage, we use deep learning models to improve the accuracy in the decisions delivered to the end-users. Additionally, we apply an assortment of activation functions to analyze and compare the accuracy of the decisions generated in the proposed scheme. Extensive simulation of our proposed scheme, Micro-Safe, demonstrates that latency is improved by 26.1 – 31.2%, energy consumption is reduced by 22.1 – 29.9%, throughput is increased by 26.1 – 31.7%, compared to the existing schemes.",,10.1109/TITS.2021.3110725,"6G mobile communication , Safety , Deep learning , Scalability , Delays , Cloud computing , Throughput ",,,,,,,,,,
144,Reconfigurable Architecture of UFMC Transmitter for 5G and Its FPGA Prototype,"Vikas Kumar , Mithun Mukherjee , Jaime Lloret ",IEEE Systems Journal,2373-7816,14,1,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8771165,"A universal-filtered multicarrier (UFMC) system that is a generalization of filtered orthogonal frequency-division multiplexing (OFDM) and filter-bank-based multicarrier is being considered as a potential candidate for fifth-generation due to its robustness against intercarrier interference as in cyclic-prefix-based OFDM systems. However, real-time hardware realization of multicarrier systems is limited by a large number of arithmetic units for inverse fast Fourier transform and pulse-shaping filters. In this paper, we aim to propose a low-complexity and reconfigurable architecture for a baseband UFMC transmitter. To the best of our knowledge, the proposed architecture is the first reconfigurable architecture that has the flexibility to choose the number of subcarriers in a subband without any change in hardware resources. In addition, the proposed architecture selects the filter from a group of filters with a single selection line. Moreover, we use a commercially available field-programmable gate array device for real-time testing and analyzing the baseband UFMC signal. From the extensive experiments, we study the occupied bandwidth, main-lobe power, and sidelobe power of the baseband signal with different filters in real-time scenarios. Finally, we measure the quantization error in baseband signal generation for the proposed UFMC transmitter architecture and find comparable with the error bound.",,10.1109/JSYST.2019.2923549,"Transmitters , Baseband , Computer architecture , Real-time systems , 5G mobile communication , Field programmable gate arrays , Hardware ",,,,,,,,,,
145,vFPGAmanager: A Virtualization Framework for Orchestrated FPGA Accelerator Sharing in 5G Cloud Environments,"Sébastien Pinneterre , Spyros Chiotakis , Michele Paolino , Daniel Raho ",2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),2155-5052,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8436930,"Network operators are actively pushing towards the new 5G era and a crucial part to accomplish this is the Network Functions Virtualization (NFV). FPGAs and their hardware accelerators are a promising solution for NFV and 5G cloud environments because of their fast turnaround time and great speedup potential through application parallelism mapping on the reconfigurable fabric. Recently, consolidation reached a plateau in this field with lightweight virtualization techniques, that require a high overcommitment of FPGA accelerator resources to cope with numerous demands of guests. Although FPGAs can play an important role for the future 5G networks their capability to manage and control them from the upper layers of the software stack is inadequate. The lack of such support coupled with cloud integration and programmability issues can repel potential providers from utilizing FPGAs at their data centers. This paper presents the communication mechanism of the vFPGAmanager, an FPGA virtualization framework which can be orchestrated, monitored and enables accelerators overcommitment with direct guest access. These are key features to allow potential adopters of FPGA technology to include them in the next generation of NFV systems. The communication mechanism architecture is detailed and then benchmarked to show that even under heavy load on the system it demonstrates a minimal overhead to orchestrate and monitor the FPGA as a resource.",,10.1109/BMSB.2018.8436930,"Field programmable gate arrays , Virtualization , Context , Cloud computing , Registers , Acceleration ",,,,,,,,,,
146,A Waveform Parameter Assignment Framework for 6G With the Role of Machine Learning,"Ahmet Yazar , Hüseyin Arslan ",IEEE Open Journal of Vehicular Technology,2644-1330,1,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9086145,"5G enables a wide variety of wireless communications applications and use cases. There are different requirements associated with the applications, use cases, channel structure, network and user. To meet all of the requirements, several new configurable parameters are defined in 5G New Radio (NR). It is possible that 6G will have even higher number of configurable parameters based on new potential conditions. In line with this trend, configurable waveform parameters are also varied and this variation will increase in 6G considering the potential future necessities. In this paper, association of users and possible configurable waveform parameters in a cell is discussed for 6G communication systems. An assignment framework of configurable waveform parameters with different types of resource allocation optimization mechanisms is proposed. Most of all, the role and usage of machine learning (ML) in this framework is described. A case study with a simulation based dataset generation methodology is also presented.",,10.1109/OJVT.2020.2992502,"6G mobile communication , 5G mobile communication , Optimization , Resource management , Machine learning , Lattices ",,,,,,,,,,
147,5G Joint Artificial Intelligence Technology in the Innovation and Reform of University English Education,"Sun, Xia and Zhang, Yuanpeng",Wireless Communications and Mobile Computing,1530-8669,2021,,4892064,2021,Hindawi,https://doi.org/10.1155/2021/4892064,"This paper considers the issue of human subjectivity in the system of &#x201C;5G&#x2009;+&#x2009;AI+Education&#x201D; from the perspective of, on the one hand, the real need for the problems that gradually emerge in the new round of development and application of artificial intelligence, and a philosophical reflection on the application of artificial intelligence in specific fields, on the other hand. It is also a further examination of the issue of human subjectivity in the new context. On the other hand, it is also a further examination of the issue of human subjectivity in the new context of the times, which can also provide students with an immersive learning environment, and AI artificial intelligence and hologram technology can enhance students&#x2019; motivation. This paper shows the specific steps and implementation measures of &#x201C;5G&#x201D; technology into online oral teaching and provides a case study design to explore the new online oral teaching model, summarizing the advantages and proposing solutions to the shortcomings. The system visualizes each step of gesture recognition to facilitate students&#x2019; understanding. Students can experience the process of gesture recognition according to the guidance of the interactive interface, and then, the complex and abstract gesture recognition process is explained with a figurative example, which is conducive to primary and secondary school students&#x2019; deeper understanding and improved logical thinking. This will help primary and secondary school students to have a deeper understanding and improve their logical thinking skills. Finally, a comparison experiment is designed to verify the effectiveness of using this system to learn AI knowledge compared with traditional learning methods. The experimental results are analyzed to prove that using this system to learn AI knowledge is effective and helps improve users&#x2019; interest in learning and hands-on ability.",,10.1155/2021/4892064,"5G , Artificial intelligence , Education , Gesture recognition , Intelligence ,   ,   ,   ,  ",,,,,,,,,,
148,An FPGA-Oriented Baseband Modulator Architecture for 4G/5G Communication Scenarios,"Lopes Ferreira, Mário and Canas Ferreira, João",Electronics,2079-9292,8,1,,2019,,https://www.mdpi.com/384148,"The next evolution in cellular communications will not only improve upon the performance of previous generations, but also represent an unparalleled expansion in the number of services and use cases. One of the foundations for this evolution is the design of highly flexible, versatile, and resource-/power-efficient hardware components. This paper proposes and evaluates an FPGA-oriented baseband processing architecture suitable for communication scenarios such as non-contiguous carrier aggregation, centralized Cloud Radio Access Network (C-RAN) processing, and 4G/5G waveform coexistence. Our system is upgradeable, resource-efficient, cost-effective, and provides support for three 5G waveform candidates. Exploring Dynamic Partial Reconfiguration (DPR), the proposed architecture expands the design space exploration beyond the available hardware resources on the Zynq xc7z020 through hardware virtualization. Additionally, Dynamic Frequency Scaling (DFS) allows for run-time adjustment of processing throughput and reduces power consumption up to 88%. The resource overhead for DPR and DFS is residual, and the reconfiguration latency is two orders of magnitude below the control plane latency requirements proposed for 5G communications.",,10.3390/electronics8010002,"Reconfigurable hardware , FPGA , Dynamic partial reconfiguration , Baseband processing , Carrier aggregation , 4G/5G coexistence , Cloud-RAN ,   ,  ",,,,,,,,,,
149,Architecture of FPGA based channel coding for 5G wireless network using high-level synthesis,,,,,,,,,,,,,,,,,,,,,,,
150,"Machine learning for 5G security: Architecture, recent advances, and challenges","Afaq, Amir and Haider, Noman and Baig, Muhammad Zeeshan and Khan, Komal S. and Imran, Muhammad and Razzak, Imran",Ad Hoc Networks,1570-8705,123,,102667,2021,,https://www.sciencedirect.com/science/article/pii/S1570870521001785,"The granularization of crucial network functions implementation using software-centric, and virtualized approaches in 5G networks have brought forth unprecedented security challenges in general and privacy concerns. Moreover, these software components’ premature deployment and compromised supply chain put the individual network components at risk and have a ripple effect for the rest of the network. Some of the novel threats to 5G assets include tampering in identity and access management, supply-chain poisoning, masquerade and bot attacks, loop-holes in source codes. Machine learning (ML) in this context can help to provide heavily dynamic and robust security mechanisms for the software-centric architecture of 5G Networks. ML models’ development and implementation also rely on programmable environments; hence, they can play a vital role in designing, modelling, and automating efficient security protocols. This article presents the threat landscape across 5G networks and discusses the feasibility and architecture of different ML-based models to counter these threats. Also, we present the architecture for automated threat intelligence using cooperative and coordinated ML to secure 5G assets and infrastructure. We also present the summary of closely related existing works along with future research challenges.",,https://doi.org/10.1016/j.adhoc.2021.102667,"5G network security , Threat landscape , Vulnerabilities , Threat intelligence , Machine learning , Federated learning , Threat classification , Learning ,  ",,,,,,,,,,
151,Machine Learning for Intelligent-Reflecting-Surface-Based Wireless Communication towards 6G: A Review,"Sejan, Mohammad A. and Rahman, Md H. and Shin, Beom-Sik and Oh, Ji-Hye and You, Young-Hwan and Song, Hyoung-Kyu",Sensors,1424-8220,22,14,,2022,,https://www.mdpi.com/1424-8220/22/14/5405,"An intelligent reflecting surface (IRS) is a programmable device that can be used to control electromagnetic waves propagation by changing the electric and magnetic properties of its surface. Therefore, IRS is considered a smart technology for the sixth generation (6G) of communication networks. In addition, machine learning (ML) techniques are now widely adopted in wireless communication as the computation power of devices has increased. As it is an emerging topic, we provide a comprehensive overview of the state-of-the-art on ML, especially on deep learning (DL)-based IRS-enhanced communication. We focus on their operating principles, channel estimation (CE), and the applications of machine learning to IRS-enhanced wireless networks. In addition, we systematically survey existing designs for IRS-enhanced wireless networks. Furthermore, we identify major issues and research opportunities associated with the integration of IRS and other emerging technologies for applications to next-generation wireless communication.",,10.3390/s22145405,"Intelligent reflecting surfaces (IRSs) , Machine learning , Multiple input multiple output , Wireless networks , Intelligence ,   ,   ,   ,  ",,,,,,,,,,
152,Security concerns on machine learning solutions for 6G networks in mmWave beam prediction,"Catak, Ferhat Ozgur and Kuzlu, Murat and Catak, Evren and Cali, Umit and Unal, Devrim",Physical Communication,1874-4907,52,,101626,2022,,https://www.sciencedirect.com/science/article/pii/S1874490722000155,"6G – sixth generation – is the latest cellular technology currently under development for wireless communication systems. In recent years, machine learning (ML) algorithms have been applied widely in various fields, such as healthcare, transportation, energy, autonomous cars, and many more. Those algorithms have also been used in communication technologies to improve the system performance in terms of frequency spectrum usage, latency, and security. With the rapid developments of ML techniques, especially deep learning (DL), it is critical to consider the security concern when applying the algorithms. While ML algorithms offer significant advantages for 6G networks, security concerns on artificial intelligence (AI) models are typically ignored by the scientific community so far. However, security is also a vital part of AI algorithms because attackers can poison the AI model itself. This paper proposes a mitigation method for adversarial attacks against proposed 6G ML models for the millimeter-wave (mmWave) beam prediction using adversarial training. The main idea behind generating adversarial attacks against ML models is to produce faulty results by manipulating trained DL models for 6G applications for mmWave beam prediction. We also present a proposed adversarial learning mitigation method’s performance for 6G security in mmWave beam prediction application a fast gradient sign method attack. The results show that the defended model under attack’s mean square errors (i.e., the prediction accuracy) are very close to the undefended model without attack.",,https://doi.org/10.1016/j.phycom.2022.101626,"Machine learning , AI , Millimeter-wave (mmWave) , Beamforming , Adversarial machine learning , 6G , Deep learning , Learning ,  ",,,,,,,,,,
153,A Programmable and FPGA-accelerated GTP Offloading Engine for Mobile Edge Computing in 5G Networks,"Chung-An Shen , Ding-Yuan Lee , Chung-An Ku , Meng-Wei Lin , Kuo-Cheng Lu , Shao-Yu Tan ",IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8845143,"This poster presents a programmable and FPGA-accelerated packet processing engine, performing the encapsulation and decapsulation of GPRS Tunneling Protocol (GTP) packets, for Mobile Edge Computing in 5G Networks. The proposed engine is designed by using the P4 language and is implemented to the FPGA platform by using the Xilinx SDNet tool. The implemented system is practically realized on the Xilinx FPGA platform and the detailed architecture is presented. The experimental results show that targeted throughput of 10Gbps per port is achieved given the packet-processing latency of 5μs.",,10.1109/INFCOMW.2019.8845143,,,,,,,,,,,
154,Efficient and reliable hybrid deep learning-enabled model for congestion control in 5G/6G networks,"Khan, Sulaiman and Hussain, Anwar and Nazir, Shah and Khan, Fazlullah and Oad, Ammar and Alshehri, Mohammad Dahman",Computer Communications,0140-3664,182,,31-40,2022,,https://www.sciencedirect.com/science/article/pii/S0140366421004217,"Future generation networks such as millimeter-wave LAN, broadband wireless access systems, and 5th or 6th generation (5G/6G) networks demand more security, low latency with more reliable standards and communication capacity. Efficient congestion control is considered one of the key elements of 5G/6G technology that allows the operators to run various network instances using a single infrastructure for a better quality of services. Artificial intelligence (AI) and machine learning (ML) are playing an essential role in reconfiguring and optimizing the performance of a 5G/6G wireless network due to a vast amount of data. A smart decision-making mechanism is required for the incoming network traffic to ensure load balancing, restrict network slice failure and provide alternate slices in case of slice failure or overloading. To circumvent these issues, a hybrid deep learning-enabled efficient congestion control mechanism is proposed in this paper. This hybrid deep learning model consists of long short term memory (LSTM) and support vector machine (SVM). The applicability of the proposed model is validated by simulating for one week using multiple unknown devices, slice failure conditions, and overloading conditions. An overall accuracy rate of 93.23% is calculated for the proposed hybrid model that reflects the applicability. Apart from this, other performance metrics such as specificity, recall, time consumption, varying training, test sets, true-false rates, and f-score were used for the performance evaluation purposes of the proposed model.",,https://doi.org/10.1016/j.comcom.2021.11.001,"Network slicing , 5G/6G network , Hybrid deep learning model , Machine learning-based reconfigurable wireless network , LSTM ,   ,   ,   ,  ",,,,,,,,,,
155,Deep learning-driven opportunistic spectrum access (OSA) framework for cognitive 5G and beyond 5G (B5G) networks,"Ahmed, Ramsha and Chen, Yueyun and Hassan, Bilal",Ad Hoc Networks,1570-8705,123,,102632,2021,,https://www.sciencedirect.com/science/article/pii/S1570870521001529,"The evolving 5G and beyond 5G (B5G) wireless technologies are envisioned to provide ubiquitous connectivity and great heterogeneity in communication infrastructure by connecting diverse devices and providing multifarious services. Recently, the Internet of Things (IoT) and unmanned aerial vehicles (UAVs) are realized as an essential component of the upcoming 5G/B5G networks, enabling enhanced communication capacity, high reliability, low latency, and massive connectivity. However, one limiting factor in the expansion of 5G/B5G technology is the finite radio spectrum, which necessitates managing the anticipated spectrum crunch for future wireless networks. One potential solution is to develop intelligent cognitive methods to dynamically optimize the use of spectrum in 5G/B5G networks to solve the imminent problem of spectrum congestion and improve radio efficiency. This paper addresses the opportunistic spectrum access (OSA) problem in the 5G/B5G cognitive radio (CR) network of IoTs and UAVs through the novel deep learning-based detector, dubbed as Deep-CRNet. The proposed detector employs residual connections with cascaded multi-kernel convolutions to identify the primary user (PU) spectrum usage by extracting the inherent multi-scale signal and noise features in the sensed transmission patterns. Thereby, Deep-CRNet intelligently learns and locates the spectrum holes so that secondary users (SUs) and PUs can dynamically share network spectrum resources. The efficacy of Deep-CRNet is validated through simulation results, where it achieved 99.74% accuracy with 99.65% precision and 99.83% recall in accurately classifying the PU status. In addition, the average correct detection probability of Deep-CRNet in the low signal-to-noise ratio (−20 dB to −15 dB) range is 38.21% higher than the second best-performing detector.",,https://doi.org/10.1016/j.adhoc.2021.102632,"Deep learning , Cognitive radio (CR) , Spectrum sensing , Opportunistic spectrum access (OSA) , 5G/B5G wireless networks , Cognition ,   ,   ,  ",,,,,,,,,,
156,Intrusion Detection System on IoT with 5G Network Using Deep Learning,"Yadav, Neha and Pande, Sagar and Khamparia, Aditya and Gupta, Deepak and Gomez, Carles",Wireless Communications and Mobile Computing,1530-8669,2022,,9304689,2022,Hindawi,https://doi.org/10.1155/2022/9304689,"The Internet of Things (IoT) cyberattacks of fully integrated servers, applications, and communications networks are increasing at exponential speed. As problems caused by the Internet of Things network remain undetected for longer periods, the efficiency of sensitive devices harms end users, increases cyber threats and identity misuses, increases costs, and affects revenue. For productive safety and security, Internet of Things interface assaults must be observed nearly in real time. In this paper, a smart intrusion detection system suited to detect Internet of Things-based attacks is implemented. In particular, to detect malicious Internet of Things network traffic, a deep learning algorithm has been used. The identity solution ensures the security of operation and supports the Internet of Things connectivity protocols to interoperate. An intrusion detection system (IDS) is one of the popular types of network security technology that is used to secure the network. According to our experimental results, the proposed architecture for intrusion detection will easily recognize real global intruders. The use of a neural network to detect attacks works exceptionally well. In addition, there is an increasing focus on providing user-centric cybersecurity solutions, which necessitate the collection, processing, and analysis of massive amounts of data traffic and network connections in 5G networks. After testing, the autoencoder model, which effectively reduces detection time as well as effectively improves detection precision, has outperformed. Using the proposed technique, 99.76&#x0025; of accuracy was achieved.",,10.1155/2022/9304689,"Intrusion detection , IoT , 5G , Deep learning , Learning ,   ,   ,   ,  ",,,,,,,,,,
157,Authentication and Resource Allocation Strategies during Handoff for 5G IoVs Using Deep Learning,"Hemavathi and Akhila, Sreenatha R. and Alotaibi, Youseef and Khalaf, Osamah I. and Alghamdi, Saleh",Energies,1996-1073,15,6,,2022,,https://www.mdpi.com/1996-1073/15/6/2006,"One of the most sought-after applications of cellular technology is transforming a vehicle into a device that can connect with the outside world, similar to smartphones. This connectivity is changing the automotive world. With the speedy growth and densification of vehicles in Internet of Vehicles (IoV) technology, the need for consistency in communication amongst vehicles becomes more significant. This technology needs to be scalable, secure, and flexible when connecting products and services. 5G technology, with its incredible speed, is expected to power the future of vehicular networks. Owing to high mobility and constant change in the topology, cooperative intelligent transport systems ensure real time connectivity between vehicles. For ensuring a seamless connectivity amongst the entities in vehicular networks, a significant alternative to design is support of handoff. This paper proposes a scheme for the best Road Side Unit (RSU) selection during handoff. Authentication and security of the vehicles are ensured using the Deep Sparse Stacked Autoencoder Network (DS2AN) algorithm, developed using a deep learning model. Once authenticated, resource allocation by RSU to the vehicle is accomplished through Deep-Q learning (DQL) techniques. Compared with the existing handoff schemes, Reinforcement Learning based on the MDP (RL-MDP) has been found to have a 13% lesser decision delay for selecting the best RSU. A higher level of security and minimum time requirement for authentication is achieved using DS2AN. The proposed system simulation results demonstrate that it ensures reliable packet delivery, significantly improving system throughput, upholding tolerable delay levels during a change of RSUs.",,10.3390/en15062006,"Deep-Q learning , RSU , URLLC , DSRC , E2E delay , IoV , Markov decision process , authentication , Resource Allocation",,,,,,,,,,
158,Machine Learning Assisted Security Analysis of 5G-Network-Connected Systems,"Tanujay Saha , Najwa Aaraj , Niraj K. Jha ",IEEE Transactions on Emerging Topics in Computing,2376-4562,10,4,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9701880,"The core network architecture of telecommunication systems has undergone a paradigm shift in the fifth-generation (5G) networks. 5G networks have transitioned to software-defined infrastructures, thereby reducing their dependence on hardware-based network functions. New technologies, like network function virtualization and software-defined networking, have been incorporated in the 5G core network (5GCN) architecture to enable this transition. This transition has significantly improved network efficiency, performance, and robustness. However, this has also made the core network more vulnerable, as software systems are generally easier to compromise than hardware systems. This article presents a comprehensive security analysis framework for the 5GCN. The novelty of this approach lies in the creation and analysis of attack graphs of the software-defined and virtualized 5GCN through machine learning. This analysis points to 119 novel possible exploits in the 5GCN. We demonstrate that these potential exploits of 5GCN vulnerabilities generate five novel attacks on the 5G Authentication and Key Agreement protocol. We combine the attacks at the network, protocol, and application layers to generate complex attack vectors. In a case study, we use these attack vectors to find four novel security loopholes in WhatsApp running on a 5G network.",,10.1109/TETC.2022.3147192,"5G mobile communication , Security , Protocols , Computer architecture , Internet telephony , Freeware , Control systems ",,,,,,,,,,
159,Deep Learning for Fast and Reliable Initial Access in AI-Driven 6G mm Wave Networks,"Tarun S. Cousik , Vijay K. Shah , Tugba Erpek , Yalin E. Sagduyu , Jeffrey H. Reed ",IEEE Transactions on Network Science and Engineering,2334-329X,PP,99,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9869317,"We present DeepIA, a deep neural network (DNN) framework for fast and reliable initial access (IA) for artificial intelligence (AI)-driven 6G millimeter wave (mmWave) networks. DeepIA reduces the beam sweep time compared to a conventional exhaustive search-based IA process by utilizing only a subset of the available beams. DeepIA maps received signal strengths (RSSs) obtained from a subset of beams to the beam that is best oriented to the receiver. In both line of sight (LoS) and non-line of sight (NLoS) conditions, DeepIA reduces the IA time and outperforms the conventional IA&#x0027;s beam prediction accuracy. We show that DeepIA&#x0027;s accuracy saturates with the number of beams used, and also depends on the particular choice of the beams used. The choice of beams that are selected is consequential and improves accuracy by upto upto <inline-formula><tex-math notation=""LaTeX"">$35\%$</tex-math></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">$70\%$</tex-math></inline-formula> in NLoS and LoS channels. We find that, averaging multiple RSS snapshots further reduces the number of beams needed and achieves more than <inline-formula><tex-math notation=""LaTeX"">$95\%$</tex-math></inline-formula> accuracy in both LoS and NLoS conditions. We introduce interference into our models to understand impact on performance. Finally, we evaluate the beam prediction time of DeepIA through embedded hardware implementation and show the improvement over the baseline approach.",,10.1109/TNSE.2022.3201748,"Receivers , 6G mobile communication , Millimeter wave communication , 5G mobile communication , Transmitters , Antenna measurements , Time measurement ",,,,,,,,,,
160,Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review,"Miglani, Arzoo and Kumar, Neeraj",Computer Communications,0140-3664,178,,37-63,2021,,https://www.sciencedirect.com/science/article/pii/S0140366421002632,"Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.",,https://doi.org/10.1016/j.comcom.2021.07.009,"Blockchain , Machine learning , Federated learning , Internet of things , Deep learning , 5G , 6G ,   ,  ",,,,,,,,,,
161,Enabling Machine Learning with Service Function Chaining for Security Enhancement at 5G Edges,"Bohao Feng , Huachun Zhou , Guanglei Li , Yuming Zhang , Keshav Sood , Shui Yu ",IEEE Network,1558-156X,35,5,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9537924,"With massive sorts of terminals, devices, and machines connecting to 5G, a tremendous surge of data makes cyber-security a pressing issue, and conventional countermeasures are facing unprecedented challenges. Recently, with the rise of ML (Machine Learning) and SDN/NFV-based (Software-Defined Networks/Network Functions Virtualization) SFC (Service Function Chaining) techniques, how to leverage them for security enhancement in MEC (Multi-access/Mobile Edge Computing) has received much attention. Hence, in this article, we first propose an elastic framework to integrate ML with virtualized SFC, aiming at smart and efficient provision of different services at MEC. Then, we propose an ML-based anomaly detection algorithm used as a kind of service policy for SFC classifiers, which guides the latter for quick traffic classification and subsequent redirections of attack flows. Finally, we build a corresponding prototype system and evaluate the performance of the proposed algorithm through extensive experiments. Related results have confirmed the feasibility and advantages of the proposed framework and algorithm.",,10.1109/MNET.100.2000338,"Feature extraction , Security , Machine learning , Classification algorithms , Quality of service , Anomaly detection , Training data , 5G mobile communication ",,,,,,,,,,
162,Machine Learning Protocol for Secure 5G Handovers,"Nyangaresi, Vincent Omollo and Rodrigues, Anthony Joachim and Abeka, Silvance Onyango",International Journal of Wireless Information Networks,1572-8129,29,1,14-35,2022,,https://doi.org/10.1007/s10776-021-00547-2,"The fifth generation (5G) networks are characterized with ultra-dense deployment of base stations with limited footprint. Consequently, user equipment’s handover frequently as they move within 5G networks. In addition, 5G requirements of ultra-low latencies imply that handovers should be executed swiftly to minimize service disruptions. To preserve security and privacy while at the same time maintaining optimal performance during handovers, numerous schemes have been developed. However, majority of these techniques are either limited to security and privacy or address only performance aspect of the handover mechanism. As such, there is need for a novel handover authentication protocol that addresses security, privacy and performance simultaneously. This paper presents a machine learning protocol that not only facilitates optimal selection of target cell but also upholds both security and privacy during handovers. Formal security analysis using the widely adopted Burrows–Abadi–Needham (BAN) logic shows that the proposed protocol achieves all the six formulated under this proof. As such, the proposed protocol facilitates strong and secure mutual authentication among the communicating entities before generating the shares session key. The derived session key protected the exchanged packets to avert attacks such as forgery. In addition, informal security evaluation of the proposed protocol shows that it offers perfect forward key secrecy, mutual authentication any user anonymity. It is also demonstrated to be robust against attacks such as denial of service (DoS), man-in-the-middle (MitM), masquerade, packet replays and forgery. In terms of performance, simulation results shows that it has lower packets drop rate and ping–pong rate, with higher ratio of packets received compared with improved 5G authentication and key agreement (5G AKA’) protocol. Specifically, using 5G AKA’ as the basis, the proposed protocol reduces the handover rate by 94.4%, hence the resulting handover signaling is greatly minimized.",,10.1007/s10776-021-00547-2,"5G , ANN , Authentication , Handovers , Privacy , Security , Learning ,   ,  ",,,,,,,,,,
163,A Survey of Collaborative Machine Learning Using 5G Vehicular Communications,"Salvador V. Balkus , Honggang Wang , Brian D. Cornet , Chinmay Mahabal , Hieu Ngo , Hua Fang ",IEEE Communications Surveys & Tutorials,2373-745X,24,2,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9706268,"By enabling autonomous vehicles (AVs) to share data while driving, 5G vehicular communications allow AVs to collaborate on solving common autonomous driving tasks. AVs often rely on machine learning models to perform such tasks; as such, collaboration requires leveraging vehicular communications to improve the performance of machine learning algorithms. This paper provides a comprehensive literature survey of the intersection between machine learning for autonomous driving and vehicular communications. Throughout the paper, we explain how vehicle-to-vehicle (V2V) and vehicle-to-everything (V2X) communications are used to improve machine learning in AVs, answering five major questions regarding such systems. These questions include: 1) How can AVs effectively transmit data wirelessly on the road? 2) How do AVs manage the shared data? 3) How do AVs use shared data to improve their perception of the environment? 4) How do AVs use shared data to drive more safely and efficiently? and 5) How can AVs protect the privacy of shared data and prevent cyberattacks? We also summarize data sources that may support research in this area and discuss the future research potential surrounding these five questions.",,10.1109/COMST.2022.3149714,"Autonomous vehicles , Deep learning , Task analysis , Machine learning , Roads , 5G mobile communication , Neural networks ",,,,,,,,,,
164,Cooperative attacks detection based on artificial intelligence system for 5G networks,"Sedjelmaci, Hichem",Computers & Electrical Engineering,0045-7906,91,,107045,2021,,https://www.sciencedirect.com/science/article/pii/S004579062100063X,"The Fifth Generation (5G) network is vulnerable to a couple of attacks targeting the main segments of 5G architecture, user equipment, radio communication, edge and core network. Therefore, a reliable and accurate security mechanism is mandatory to protect the end-to-end network against the internal and external attacks. In this research work, I propose a hierarchical detection scheme based on a reinforcement learning process to secure the main segments of end-to-end 5G network. Specifically, a distributed attacks detection systems collaborate with a goal to reinforce their learnings, update their optimal defense strategies, and determine the current and futures attacks ‘misbehaviors occurred at different segments. Experimental results demonstrate that, the proposed cyber defense scheme requires a low computation overhead to protect the network from internal and external attacks as compared to the current cyber detection schemes.",,https://doi.org/10.1016/j.compeleceng.2021.107045,"5G network , Reinforcement learning process , Cooperative detection system , Internal and external attacks , Accuracy protection rate and computation overhead , Intelligence ,   ,   ,  ",,,,,,,,,,
165,A Machine Learning Approach for SNR Prediction in 5G Systems,"Krunal Saija , Shekar Nethi , Saptarshi Chaudhuri , R M Karthik ",2019 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS),2153-1676,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9118097,"Channel State Information (CSI) feedback from the User Equipment (UE) on the uplink (UL) channel is an integral part of the 5G-NR standard. It allows next-generation Node-B (gNB) to obtain the information about the channel impairments, and schedule appropriate radio resources to the UE in order to maintain the required Quality of Service (QoS). Errors in the channel estimation by the UE increases erroneous downlink transmission and subsequently inefficient spectral use. There is an inherent tradeoff between CSI feedback periodicity from the UE and the accuracy and relevance of channel estimation within the periodicity by gNB. Since the CSI depends on the received Signal-to-Noise Ratio (SNR) at the UE. Hence, the goal of this work is to design and develop suitable methodologies to estimate a CSI parameter namely, Channel Quality Indicator (CQI) by predicting the SNR using the state-of-the-art Machine Learning (ML) techniques. In this paper, we present an experimental framework to predict CQI using SNR for different environmental scenarios and UE characteristics like delay profile and speed respectively. The experimental results show better performance in terms of CQI to SNR mapping, and error in prediction over current techniques.",,10.1109/ANTS47819.2019.9118097,,,,,,,,,,,
166,Leveraging Machine Learning for Millimeter Wave Beamforming in Beyond 5G Networks,"Basem M. ElHalawany , Sherief Hashima , Kohei Hatano , Kaishun Wu , Ehab Mahmoud Mohamed ",IEEE Systems Journal,2373-7816,16,2,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9471834,"Millimeter wave (mmWave) communication has attracted considerable attention as a key technology for the next-generation wireless communications thanks to its exceptional advantages. MmWave leads the way to achieve a high transmission quality with directed narrow beams from source to multiple destinations by adopting different antenna beamforming (BF) techniques, which have a pivotal role in establishing and maintaining robust links. However, realizing such BF gains in practice requires overcoming several challenges, such as severe signal deterioration, hardware constraints, and design complexity. The elevated complexity of configuring mmWave BF vectors encourages researchers to leverage relevant machine learning (ML) techniques for better BF configurations deployment in 5G and beyond. In this article, we summarize mmWave BF strategies employed for future wireless networks. Then, we provide a comprehensive overview of ML techniques plus its applications and promising contributions toward efficient mmWave BF deployment. Furthermore, we discuss mmWave BF’s future research directions and challenges. Finally, we discuss a single and concurrent mmWave BF case study by applying multiarmed bandit to confirm the superiority of ML-based methods over conventional ones.",,10.1109/JSYST.2021.3089536,"Training , Feature extraction , 5G mobile communication , Sensors , Recurrent neural networks , Location awareness , IEEE 802.11 Standard ",,,,,,,,,,
167,Deep Learning Based Prediction of Signal-to-Noise Ratio (SNR) for LTE and 5G Systems,"Thinh Ngo , Brian Kelley , Paul Rad ",2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9272470,"Deep learning (DL) is applied to predict signal-to-noise ratio (SNR) in de facto LTE and 5G systems in a non-data-aided (NDA) manner. Various channel conditions and impairments are considered, including modulation types, path delays, and Doppler shifts. Both time-domain and frequency-domain signal grids are evaluated as inputs for SNR prediction. A combination of convolutional neural network (CNN) and long short term memory (LSTM) - CNN-LSTM - is used as the SNR predictor. Learning both spatial and temporal features is known to improve DL prediction accuracy. Techniques employed to enhance performance are SNR range/resolution manipulation, binary prediction, and multiple input prediction. Computer simulation is conducted using MATLAB LTE, 5G, and DL toolboxes to generate OFDM signals, model fading channels with AWGN noise, and construct CNN-LSTM. Simulation results show, with off-line training, DL based prediction of SNR in LTE and 5G systems has better accuracy and latency than traditional estimation techniques. Specifically, SNR prediction for SNR range of [-4, 32] dB and resolution of 2 dB utilizing time-domain signals has an accuracy of 100%, hence normalized mean square error (NMSE) of zero, and a latency of 1 millisecond or less.",,10.1109/WINCOM50532.2020.9272470,"Signal to noise ratio , Modulation , 5G mobile communication , Estimation , Time-domain analysis , Long Term Evolution , Frequency-domain analysis ",,,,,,,,,,
168,Implementing 5G NR Features in FPGA,"James Bishop , Jean-Marc Chareau , Fausto Bonavitacola ",2018 European Conference on Networks and Communications (EuCNC),2575-4912,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8443214,"A set of physical layer features defined in the 3GPP 38 series standards (5G) were implemented in commercial off the shelf SDR products. The system created an over-the-air link operating at 3.5GHz, having 40MHz channel width and using 256QAM. The PHY layer was executed in the FPGA of the SDR transceiver; higher layers executed in the host PC controlling the radio frontend. With a 4 K video stream as payload, the link allowed us to experiment with: 5G NR numerologies; frequency shaping techniques for reducing OOB emissions and improving spectral utilization; higher order QAM modulation; and mixed numerology communications.",,10.1109/EuCNC.2018.8443214,"OFDM , Finite impulse response filters , Field programmable gate arrays , 5G mobile communication , Long Term Evolution , Downlink , Transceivers ",,,,,,,,,,
169,Digital Predistortion for 5G Small Cell: GPU Implementation and RF Measurements,"Pascual Campo, Pablo and Lampu, Vesa and Meirhaeghe, Alexandre and Boutellier, Jani and Anttila, Lauri and Valkama, Mikko",Journal of Signal Processing Systems,1939-8115,92,5,475-486,2020,,https://doi.org/10.1007/s11265-019-01502-4,"In this paper, we present a high data rate implementation of a digital predistortion (DPD) algorithm on a modern mobile multicore CPU containing an on-chip GPU. The proposed implementation is capable of running in real-time, thanks to the execution of the predistortion stage inside the GPU, and the execution of the learning stage on a separate CPU core. This configuration, combined with the low complexity DPD design, allows for more than 400 Msamples/s sample rates. This is sufficient for satisfying 5G new radio (NR) base station radio transmission specifications in the sub-6 GHz bands, where signal bandwidths up to 100 MHz are specified. The linearization performance is validated with RF measurements on two base station power amplifiers at 3.7 GHz, showing that the 5G NR downlink emission requirements are satisfied.",,10.1007/s11265-019-01502-4,"Digital predistortion (DPD) , 5G , GPU , Real-time , High data rate ,   ,   ,   ,  ",,,,,,,,,,
170,Fiber-Fed Distributed Antenna System in an FPGA Software Defined Radio for 5G Demonstration,"Saad Mahboob , Rodney G. Vaughan ",IEEE Transactions on Circuits and Systems II: Express Briefs,1558-3791,67,2,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8684264,"The implementation of high-speed wireless networks, such as currently used fourth generation (4G) systems and future 5G systems, feature challenging processing. Field programmable gate arrays (FPGAs) can straddle research and development for these current and future networks since they provide scaling through reconfigurable logic, high parallelism, and low power consumption. This brief demonstrates an FPGA circuit implementation, with measurements, of a minimal system (a 5G element or “unit cell”): a single-user mobile with antenna diversity and a distributed antenna system (DAS) at the base station. The demonstration system has a bandwidth of 20 MHz, runs at 2.4 GHz, and has two antennas at both the transmitting base station and at the receiving mobile. The modulation is orthogonal frequency division multiplexing (OFDM) with space-time block coding (STBC). The FPGA is a Virtex-6, used for software defined radio (SDR), and this can readily be scaled to handle larger-dimensioned, higher-capacity systems. The receiver has time-offset synchronization, frequency-offset, and channel estimation. The high-level algorithm design (Xilinx System Generator) for these functions and the OFDM-STBC, and the resources consumed on the FPGA during real-time implementation, are included. We also compare the use of coax and fiber for linking the distributed antennas, using off-the-shelf components. The approach used here of combining simulations with physical measurement of a minimal system is a practical way forward for assessing candidate systems for 5G.",,10.1109/TCSII.2019.2909762,"Field programmable gate arrays , OFDM , MIMO communication , 5G mobile communication , Receivers , Antennas , Bandwidth ",,,,,,,,,,
171,Machine Learning for Securing SDN based 5G Network,"Alamri, Hassan and Thayananthan, Vijey and Yazdani, Javad",International Journal of Computer Applications,,174,,975-8887,2021,,https://www.researchgate.net/publication/348535226_Machine_Learning_for_Securing_SDN_based_5G_Network,"The fifth-generation (5G) network supports many systems such as reliable communication in potential applications that require maximum security. Advancement in Software-Defined Networking (SDN) is growing with the emerging network architectures targeted from many servers with the various types of Distributed Denial of Service (DDoS) attackers. When malicious users send DDoS attacks, the SDN based 5G networks face security problems and challenges. Despite the security solutions for preventing DDoS attacks in SDN, securing the SDN controller is one of the challenging problems. The purpose of this research is to analyze the suitable machine learning (ML) for securing the SDN controller targeted by DDoS attacks. This paper proposes a security scheme that includes the ML algorithm, adaptive bandwidth mechanism, and dynamic threshold technique. Therefore, the main focus is on the mitigation scheme of DDoS attacks considered in SDN controller through the ML trained model. In this scheme, the proposed approach uses the best ML as a method for finding security solutions that enhance the security of the SDN controller and network performance. In this method, the Extreme Gradient Boosting (XGBoost) and other ML algorithms were used, which not only enhance the accuracy of the security solutions but also improve the overall network performance. General Terms In this paper, the security of the SDN based 5G network is considered as a general term. Throughout this research, ML and detection technique of DDoS is considered to improve the security solutions of SDN based 5G networks.",,10.5120/ijca2021921027,"Machine learning , Distributed Denial-of-Service , SDN based , 5G networks , Security solution , Extreme gradient boosting , Algorithm (XGBoost) , Learning ,  ",,,,,,,,,,
172,"Intelligent zero trust architecture for 5G/6G networks: Principles, challenges, and the role of machine learning in the context of O-RAN","Ramezanpour, Keyvan and Jagannath, Jithin",Computer Networks,1389-1286,217,,109358,2022,,https://www.sciencedirect.com/science/article/pii/S1389128622003929,"In this position paper, we discuss the critical need for integrating zero trust (ZT) principles into next-generation communication networks (5G/6G). We highlight the challenges and introduce the concept of an intelligent zero trust architecture (i-ZTA) as a security framework in 5G/6G networks with untrusted components. While network virtualization, software-defined networking (SDN), and service-based architectures (SBA) are key enablers of 5G networks, operating in an untrusted environment has also become a key feature of the networks. Further, seamless connectivity to a high volume of devices has broadened the attack surface on information infrastructure. Network assurance in a dynamic untrusted environment calls for revolutionary architectures beyond existing static security frameworks. To the best of our knowledge, this is the first position paper that presents the architectural concept design of an i-ZTA upon which modern artificial intelligence (AI) algorithms can be developed to provide information security in untrusted networks. We introduce key ZT principles as real-time Monitoring of the security state of network assets, Evaluating the risk of individual access requests, and Deciding on access authorization using a dynamic trust algorithm, called MED components. To ensure ease of integration, the envisioned architecture adopts an SBA-based design, similar to the 3GPP specification of 5G networks, by leveraging the open radio access network (O-RAN) architecture with appropriate real-time engines and network interfaces for collecting necessary machine learning data. Therefore, this work provides novel research directions to design machine learning based components that contribute towards i-ZTA for the future 5G/6G networks.",,https://doi.org/10.1016/j.comnet.2022.109358,"Deep learning , 6G , 5G , Federated learning , Reinforcement learning , O-RAN , Zero-trust architecture , Intelligence ,  ",,,,,,,,,,
173,Edge Intelligence in Softwarized 6G: Deep Learning-enabled Network Traffic Predictions,"Shah Zeb , Muhammad Ahmad Rathore , Aamir Mahmood , Syed Ali Hassan , JongWon Kim , Mikael Gidlund ",2021 IEEE Globecom Workshops (GC Wkshps),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9682131,"The 6G vision is envisaged to enable agile network expansion and rapid deployment of new on-demand microservices (e.g., visibility services for data traffic management, mobile edge computing services) closer to the network’s edge IoT devices. However, providing one of the critical features of network visibility services, i.e., data flow prediction in the network, is challenging at the edge devices within a dynamic cloud-native environment as the traffic flow characteristics are random and sporadic. To provide the AI-native services for the 6G vision, we propose a novel edge-native framework to provide an intelligent prognosis technique for data traffic management in this paper. The prognosis model uses long short-term memory (LSTM)-based encoder-decoder deep learning, which we train on real time-series multivariate data records collected from the edge µ-boxes of a selected testbed network. Our result accurately predicts the statistical characteristics of data traffic and verifies the trained model against the ground truth observations. Moreover, we validate our novel framework with two performance metrics for each feature of the multivariate data.",,10.1109/GCWkshps52748.2021.9682131,"6G mobile communication , Measurement , Intelligent networks , Multi-access edge computing , Computational modeling , Microservice architectures , Telecommunication traffic ",,,,,,,,,,
174,Evolution Toward Artificial Intelligence of Things Under 6G Ubiquitous-X,"Zhang, P. and Xu, X. and Qin, Xiaoqi and Liu, Y. and Ma, N. and Han, Shujun",Journal of Harbin Institute of Technology (New Series),,27,,116-135,2020,,http://hit.alljournals.cn/html/jhit_cn/2020/3/20200309.html,"With the evolution of cellular networks, 6G is a promising technology to provide ubiquity of communications, computing, control, and consciousness (UC⁴) for ""human-machine-thing-genie"" and build a ubiquitous intelligent mobile society. Genie, which can act as the artificial intelligence assistance for 6G users, is the key enabler to realize the unprecedented transformation from mobile Internet to network of intelligence. While Internet of Things (IoT) is the digital nervous system, genie acts like the brain of the overall system. Supported by 6G, IoT will step into the Artificial Intelligence of Things (AIoT) era and the AIoT networks have the abilities of intelligent perception, intelligent analysis, and intelligent control. In this paper, the concept of Ubiquitous-X is introduced, which is considered as the fundamental architecture of 6G network, and the definition and architecture of AIoT under Ubiquitous-X is also presented. Several major technical challenges posed by the service requirements of novel AIoT applications are pinpointed, including massive and intelligent connectivity, efficient computing, security, privacy, authentication, high scalability and efficiency. Potential enabling technologies to support seamless service experiences across terminals to realize AIoT are introduced as well. © 2020, The Editorial Department of Journal of Harbin Institute of Technology. All right reserved.",,10.11916/j.issn.1005-9113.20036,"AIoT , 6G , Ubiquitous-X , AI , MEC , Blockchain , Physical layer security , Intelligence ,  ",,,,,,,,,,
175,Parallel and Flexible 5G LDPC Decoder Architecture Targeting FPGA,"Jérémy Nadal , Amer Baghdadi ",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,1557-9999,29,6,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9420267,"The quasi-cyclic (QC) low-density parity-check (LDPC) code is a key error correction code for the fifth generation (5G) of cellular network technology. Designed to support several frame sizes and code rates, the 5G LDPC code structure allows high parallelism to deliver the high demanding data rate of 10 Gb/s. This impressive performance introduces challenging constraints on the hardware design. Particularly, allowing such high flexibility can introduce processing rate penalties on some configurations. In this context, a novel highly parallel and flexible hardware architecture for the 5G LDPC decoder is proposed, targeting field-programmable gate array (FPGA) devices. The architecture supports frame parallelism to maximize the utilization of the processing units, significantly improving the processing rate. The controller unit was carefully designed to support all 5G configurations and to avoid update conflicts. Furthermore, an efficient data scheduling is proposed to increase the processing rate. Compared to the recent related state of the art, the proposed FPGA prototype achieves a higher processing rate per hardware resource for most configurations.",,10.1109/TVLSI.2021.3072866,"5G mobile communication , Decoding , Parity check codes , Hardware , Quantization (signal) , Parallel processing , Field programmable gate arrays ",,,,,,,,,,
176,FPGA-SDR Integration and Experimental Validation of a Joint DA ML SNR and Doppler Spread Estimator for 5G Cognitive Transceivers,"Haithem Haggui , Sofiène Affes , Faouzi Bellili ",IEEE Access,2169-3536,7,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8726401,"In a multi-connected, multi-technology, and pervasive mobile infrastructure, such as what is being planned for 5G, artificial intelligence and cognition will play a major role. An important goal of future mobile infrastructures is to self-adapt their characteristics to their operating conditions, at the physical link, as well as at the network and application layers, which gives rise to a new paradigm known as context-aware cognitive radio (CR). CR transceivers (CTRs) mostly incorporate a cognitive engine that relies on various sensorial entities, which attempt to provide sufficient information about the quality of the link through the estimation of various key channel parameters. Two important parameters are required in a wide range of CTR architectures: the signal-to-noise ratio (SNR) and the Doppler spread. Within this context, we tackle the hardware design and integration of a joint data-aided (DA) maximum likelihood (ML) SNR and Doppler spread estimator recently shown to outperform main state-of-the-art solutions both in terms of accuracy and complexity. We propose a deep-pipelined and resource-efficient architecture for the outlined joint DA ML estimator, and we integrate our design on an FPGA-based software-defined radio (SDR) platform. We finally validate and test this prototype in real time under realistic over-the-air propagation conditions reproduced by a highly-scalabile channel emulator. Compared to its MATLAB floating-point version, our hardware prototype suggests negligible losses in performance despite the existence of several hardware impairments, thereby confirming its very strong potential and attractiveness for possible integration in future 5G CTRs.",,10.1109/ACCESS.2019.2919978,"Doppler effect , Signal to noise ratio , Maximum likelihood estimation , Hardware , Field programmable gate arrays , 5G mobile communication , Computer architecture ",,,,,,,,,,
177,Potential technologies and applications based on deep learning in the 6G networks,"Zheng, Zunxin and Wang, Linmei and Zhu, Fumin and Liu, Ling",Computers and Electrical Engineering,0045-7906,95,,107373,2021,,https://www.sciencedirect.com/science/article/pii/S0045790621003426,"The fifth generation (5G) networks provide high speed, low latency, and reliable communication services, and have been making great differences in our daily life. However, it might be difficult to meet the growing demands of Internet of Things (IoT) devices based on the existing technologies in the future, so the concept of sixth generation (6G) networks were raised to improve the existing 5G networks and promote the development of intelligent applications furthermore. Therefore, we introduce the characteristics, challenges, and potential technologies of 6G networks, and find that deep learning is potential to overcome these challenges with the help of big data and cloud computing. In addition, Deep learning approaches and potential applications in 6G networks are also discussed. We summarize the important viewpoints and studies, and provide some new ideas for developing 6G networks.",,https://doi.org/10.1016/j.compeleceng.2021.107373,"Deep learning , 6G networks , IoT , Big data , Cloud computing , Learning ,   ,   ,  ",,,,,,,,,,
178,Unsupervised Machine Learning in 6G Networks -State-of-the-art and Future Trends,"Vasileios P. Rekkas , Sotirios Sotiroudis , Panagiotis Sarigiannidis , George K. Karagiannidis , Sotirios K. Goudos ",2021 10th International Conference on Modern Circuits and Systems Technologies (MOCAST),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9493388,"Wireless communication systems play a very crucial role for business, commercial, health and safety applications. With the commercial deployment of fifth generation (5G), academic and industrial research focuses on the sixth generation (6G) of wireless communication systems. Artificial Intelligence (AI) and especially Machine Learning (ML), will be a key component of 6G systems. Here, we present an up-to-date review of future 6G wireless systems and the role of unsupervised ML techniques in them.",,10.1109/MOCAST52088.2021.9493388,"6G mobile communication , Wireless communication , Machine learning algorithms , Circuits and systems , Focusing , Machine learning , Market research ",,,,,,,,,,
179,Artificial Intelligence and Machine Learning in 5G and beyond: A Survey and Perspectives,Abdelfatteh Haidine and Fatima Zahra Salmam and Abdelhak Aqqal and Aziz Dahbi,,978-1-83962-344-8,,,Ch. 3,2021,IntechOpen,https://doi.org/10.5772/intechopen.98517,"The deployment of 4G/LTE (Long-Term Evolution) mobile networks has solved the major challenge of high capacities to build a real broadband mobile internet. This was possible mainly through a very strong physical layer and flexible network architecture. However, bandwidth-hungry services such as virtual reality (VR) and augmented reality (AR), have been developed in an unprecedented way. Furthermore, mobile networks are facing other new services with extreme demand for greater reliability and almost zero-latency performance, like vehicle communications and the Internet of Vehicles (IoV). Therefore, industries and researchers are investigating new physical layers and softwarization techniques and including more intelligence in 5G and beyond 5G (B5G/6G). This book discusses some of these softwarization techniques, such as fog computing, cloud computing, and artificial intelligence (AI) and machine learning (ML). It also presents use cases showing practical aspects from 5G deployment scenarios, where other communications technologies will co-habit to build the landscape of next-generation mobile networks (NGMNs).",,10.5772/intechopen.98517,"Next generation mobile networks , 5G , Artificial intelligence , Machine learning , Deep learning , Physical layer , Big data , Network control , Intelligence",,,,,,,,,,
180,Performance Analysis of Deep Learning-Based Routing Protocol for an Efficient Data Transmission in 5G WSN Communication,"Greeshma Arya , Ashish Bagwari , Durg Singh Chauhan ",IEEE Access,2169-3536,10,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9676565,"For the past few years, huge interest and dramatic development have been shown for the Internet of Things (IoT) based constrained Wireless sensor network (WSN) to achieve efficient resource utilization and better service delivery. IoT requires a better communication network for data transmission between heterogeneous devices and an optimally deployed energy-efficient WSN. The clustering technique applied for WSN node deployment needs to be efficient; therefore, the entire architecture can obtain a better network lifetime. While clustering, the entire network is partitioned into various clusters. Moreover, the cluster head (CH) selection process also needs proper attention for achieving efficient data communication towards the sink node via selected CH and also for increasing the node reachability within the cluster. In this proposed framework, an energy efficient deep belief network (DBN) based routing protocol is developed, which achieves better data transmission through the selected path. Due to this the packet delivery ratio (PDR) gets improved. In this framework, initially, the nodes in the whole network is grouped as clusters using a reinforcement learning (RL) algorithm, which assigns a reward for the nodes that belong to the particular cluster. Then, the CH required for efficient data communication is selected using a Mantaray Foraging Optimization (MRFO) algorithm. The data is transmitted to the sink node via the selected CH using an efficient deep learning approach. At last, the performance of proposed deep network based routing protocol is evaluated using different evaluation metrics they are network lifetime, energy consumption, number of alive nodes, and packet delivery rate. Finally, the evaluated results are compared with few existing algorithms. Among all these algorithms, the proposed DBN routing protocol has achieved better network lifetime.",,10.1109/ACCESS.2022.3142082,"Routing , Wireless sensor networks , Routing protocols , Clustering algorithms , Optimization , 5G mobile communication , Machine learning algorithms ",,,,,,,,,,
181,Deep Learning and Onion Routing-Based Collaborative Intelligence Framework for Smart Homes Underlying 6G Networks,"Nilesh Kumar Jadav , Rajesh Gupta , Mohammad Dahman Alshehri , Harsh Mankodiya , Sudeep Tanwar , Neeraj Kumar ",IEEE Transactions on Network and Service Management,2373-7379,19,3,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9749122,"Sensor communication in the smart home environment is still in its infancy as the information exchange between sensors is vulnerable to security threats. Many traditional solutions use single-layer or multi-layer (i.e., onion routing protocol) encryption/decryption algorithms. But, in the traditional onion routing protocol, if the directory server is compromised, it may not track the malicious onion nodes within the onion network. It questioned the path anonymity of the onion routing protocol. Motivated by this, we proposed a blockchain and onion routing (OR)-based secure and trusted framework in the paper. The anonymity of the proposed OR network is maintained by storing and tracking the onion nodes threshold values through the blockchain network. A long short-term memory (LSTM) model is also utilized to classify the sensors data requests as malicious and non-malicious. The performance of the proposed system is evaluated with different performance metrics such as F1 score and accuracy. The LSTM model significantly improves the initial detection rate of malicious data requests from smart home sensors. Over these benefits, we considered the entire communication via 6G channel, reducing the overall communication latency. Additionally, the OR network is simulated over the shadow simulator to analyze the OR network’s performance considering parameters such as packet delivery ratio and malicious onion node detection rate.",,10.1109/TNSM.2022.3164715,"Smart homes , Routing , Encryption , Blockchains , Cryptography , Artificial intelligence , Routing protocols ",,,,,,,,,,
182,Experimental Demonstration of 5G Fronthaul and Backhaul Convergence Based on FPGA-Based Active Optical Transport,"Arash Farhadi-Beldachi , Emilio Huques-Satas , Anna Tzanakaki , Yan Yan , Reza Nejabati , Dimitra Simeonidou ",2018 European Conference on Optical Communication (ECOC),,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8535380,We demonstrate a dynamic frame-based optical network in support of a 5G transport supporting both Backhaul and Fronthaul functionalities exploiting FPGA-based real-time optical active technologies. This solution is successfully evaluated over a city field trial with <;1dB power penalty.,,10.1109/ECOC.2018.8535380,"Ethernet , 5G mobile communication , Optical fibers , Clocks , Optical fiber networks , Urban areas ",,,,,,,,,,
183,A survey: Distributed Machine Learning for 5G and beyond,"Nassef, Omar and Sun, Wenting and Purmehdi, Hakimeh and Tatipamula, Mallik and Mahmoodi, Toktam",Computer Networks,1389-1286,207,,108820,2022,,https://www.sciencedirect.com/science/article/pii/S1389128622000421,"5G is the fifth generation of cellular networks. It enables billions of connected devices to gather and share information in real time; a key facilitator in Industrial Internet of Things (IoT) applications. It has more capabilities in terms of bandwidth, latency/delay, processing powers and flexibility to utilize either edge or cloud resources. Furthermore, 6G is expected to be equipped with the new capability to converge ubiquitous communication, computation, sensing and controlling for a variety of sectors, which heightens the complexity in a more heterogeneous environment This increased complexity, combined with energy efficiency and Service Level Agreement (SLA) requirements makes application of Machine Learning (ML) and distributed ML necessary. A decentralized approach stemming from distributed learning is a very attractive option compared with a centralized architecture for model learning and inference. Distributed ML exploits recent Artificial Intelligence (AI) technology advancements to allow collaborated ML, whilst safeguarding private data, minimizing both communication and computation overhead along with addressing ultra-low latency requirements. In this paper, we review a number of distributed ML architectures and designs, that focus on optimizing communication, computation and resource distribution. Privacy, information security and compute frameworks, are also analyzed and compared with respect to different distributed ML approaches. We summarize the major contributions and trends in this area and highlight the potential of distributed ML to help researchers and practitioners make informed decisions on selecting the right ML approach for 5G and Beyond related AI applications. To enable distributed ML for 5G and Beyond, communication, security, and computing platform often counter balance each other, thus, consideration and optimization of these aspects at an overall system level is crucial to realize the full potential of AI for 5G and Beyond. These different aspects do not only pertain to 5G, but will also enable careful design of distributed machine learning architectures to circumvent the same hurdles that will inevitably burden 5G and Beyond network generations. This is the first survey paper that brings together all these aspects for distributed ML.",,https://doi.org/10.1016/j.comnet.2022.108820,"Machine learning , Distributed machine learning , Distributed inference , Latency , 5G networks ,   ,   ,   ,  ",,,,,,,,,,
184,Deep Learning Techniques for Advancing 6G Communications in the Physical Layer,"Shangwei Zhang , Jiajia Liu , Tiago Koketsu Rodrigues , Nei Kato ",IEEE Wireless Communications,1558-0687,28,5,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9508930,"As current 5G communication systems cannot fulfill the stringent requirements brought by emerging applications, 6G will innovatively employ deep learning (DL) techniques to fundamentally rethink the communication systems design problem from the bottom to top layers. Although recent evidence has shown the power of DL techniques in the communication domain, the exploration and utilization of DL techniques in communication systems is still in its infancy and should come in a progressive manner. To effectively and efficiently implement DL techniques in future 6G communications in the physical layer, we give some potential deployment strategies and key enabling technologies that relate to 6G in terms of joint design of block-structured and end-to-end DL, integration of model-driven and data-driven DL, combination of online and offline training, ubiquitous learning and explainable DL techniques.",,10.1109/MWC.001.2000516,"6G mobile communication , Wireless communication , Communication systems , Training data , Complexity theory , Receivers , Mathematical model ",,,,,,,,,,
185,AI-Based Vehicular Network toward 6G and IoT: Deep Learning Approaches,"Chen, Mu-Yen and Fan, Min-Hsuan and Huang, Li-Xiang",ACM Trans. Manage. Inf. Syst.,2158-656X,13,1,6,2022,,https://doi.org/10.1145/3466691,"In recent years, vehicular networks have become increasingly large, heterogeneous, and dynamic, making it difficult to meet strict requirements of ultralow latency, high reliability, high security, and massive connections for next generation (6G) networks. Recently, deep learning (DL) has emerged as a powerful artificial intelligence (AI) technique to optimize the efficiency and adaptability of vehicle and wireless communication. However, rapidly increasing absolute numbers of vehicles on the roads are leading to increased automobile accidents, many of which are attributable to drivers interacting with their mobile phones. To address potentially dangerous driver behavior, this study applies deep learning approaches to image recognition to develop an AI-based detection system that can detect potentially dangerous driving behavior. Multiple convolutional neural network (CNN)-based techniques including VGG16, VGG19, Densenet, and Openpose were compared in terms of their ability to detect and identify problematic driving.",,10.1145/3466691,"Deep learning , 6G , Convolutional neural network , Vehicular network , Internet of things , Learning ,   ,   ,  ",,,,,,,,,,
186,"Toward Native Artificial Intelligence in 6G Networks: System Design, Architectures, and Paradigms","Wu, Jianjun and Li, Rongpeng and An, Xueli and Peng, Chenghui and Liu, Zhe and Crowcroft, Jon and Zhang, Honggang",CoRR,,,,,2021,,https://arxiv.org/abs/2103.02823,"The mobile communication system has transformed to be the fundamental infrastructure to support digital demands from all industry sectors, and 6G is envisioned to go far beyond the communication-only purpose. There is coming to a consensus that 6G will treat Artificial Intelligence (AI) as the cornerstone and has a potential capability to provide ""intelligence inclusion"", which implies to enable the access of AI services at anytime and anywhere by anyone. Apparently, the intelligent inclusion vision produces far-reaching influence on the corresponding network architecture design in 6G and deserves a clean-slate rethink. In this article, we propose an end-to-end system architecture design scope for 6G, and talk about the necessity to incorporate an independent data plane and a novel intelligent plane with particular emphasis on end-to-end AI workflow orchestration, management and operation. We also highlight the advantages to provision converged connectivity and computing services at the network function plane. Benefiting from these approaches, we believe that 6G will turn to an ""everything as a service"" (XaaS) platform with significantly enhanced business merits.",,,"Artificial intelligence , 6G , Mobile networks , Intelligence ,   ,   ,   ,   ,  ",,,,,,,,,,
187,Artificial intelligence image recognition based on 5G deep learning edge algorithm of Digestive endoscopy on medical construction,"Yang, Lili and Li, Zhichao and Ma, Shilan and Yang, Xinghua",Alexandria Engineering Journal,1110-0168,61,3,1852-1863,2022,,https://www.sciencedirect.com/science/article/pii/S1110016821004774,"In this paper, we use artificial intelligence image recognition to obtain Digestive endoscopy image, and process the image based on 5G Deep learning edge algorithm to judge the disease type of the patient, and then consider the treatment plan. The combination of body area network and edge computing technology can meet the demand of low delay in body area network. In this case, the resource constrained body area network gateway node can process the physiological data collected by it into an offloadable task, and then unload the task and data to the edge computing node according to a certain strategy. The edge computing node completes the corresponding task processing and data storage, and finally provides the results to the relevant medical institutions and body area network users for reading auxiliary diagnosis and treatment of diseases. Studies have shown that 25% of patients with colon polyps have CD4 cells in peripheral blood based on 5G deep learning edge algorithm under artificial intelligence image recognition of Digestive endoscopy. The number of lymphocyte in group of differentiation was less than 200/μL, and the blood RNA in 92.3% patients was lower than 100 IU/ml, while fam CTP (A-cyclic peptide) was lower than 100 IU/ml. Opportunistic infections of the intestine and viruses can directly cause enteropathy because the fluorescence intensity of the probe is essentially unchanged and cannot form a triple helix structure. In terms of feature recognition accuracy, the 5G deep learning edge algorithm in this paper improves accuracy by 68% compared to the simple Yolo algorithm, and is similar in speed. Compared with RCNN algorithm, the accuracy and speed are improved by 21% and 85% respectively. Therefore, the 5G deep learning edge algorithm based on artificial intelligence image recognition has the advantages of accuracy and speed in digestive endoscopy of intelligent medical.",,https://doi.org/10.1016/j.aej.2021.07.007,"Smart medical construction , Artificial intelligence image recognition , 5G deep learning edge algorithm , Digestive endoscopy , Gastrointestinal Agents , Algorithms , Intelligence ,   ,  ",,,,,,,,,,
188,Deep transfer learning-based network traffic classification for scarce dataset in 5G IoT systems,"Guan, Jianfeng and Cai, Junxian and Bai, Haozhe and You, Ilsun",International Journal of Machine Learning and Cybernetics,1868-808X,12,11,3351-3365,2021,,https://doi.org/10.1007/s13042-021-01415-4,"Internet of Things (IoT) can provide the interconnection and data sharing among devices, vehicles, buildings via various sensors with the development of 5G, and it has been widely used in different services such as e-commerce, heath-care, smart buildings. In the meantime, various cyber-attacks for IoT have increased and caused huge losses. Lots of security mechanisms are rapidly being proposed to prevent the potentially malicious attackers for IoT, in which machine learning especially deep learning (DL) as increasingly popular solution for security has been implemented in intrusion detection system (IDS) and others. However, the lack of enough datasets prevents the application of IDS in 5G IoT system. As one of fundamental components of IDS, network traffic classification shows a discretization, individualization and fine-grained trend which derives the different personalized classification methods for different requirements and scenarios. In this case, the data-driven DL faces the following challenges. First, there are only a few labeled datasets in the various personalized application scenarios, which undoubtedly limits the deployment of DL classification. Second, not all scenarios have rich computing capability for that training a neural network requires lots of computing resources. Therefore, this paper proposes a traffic classification method based on deep transfer learning for 5G IoT scenarios with scarce labeled data and limited computing capability, and trains the classification model by weight transferring and neural network fine-tuning. Different from the previous work that extract artificially designed features, the proposed method retains the end-to-end learning performance of DL and reduces the risk of suffering concept drift to reduce human intervention. Experimental results show that when only 10% of dataset are used to label the data samples, the classification accuracy is close to the results of full training dataset.",,10.1007/s13042-021-01415-4,"Traffic classification , Deep learning , Transfer learning , Scarce dataset ,   ,   ,   ,   ,  ",,,,,,,,,,
189,Integrated Sensing and Communication for 6G: Ten Key Machine Learning Roles,"Umut Demirhan , Ahmed Alkhateeb ",IEEE Communications Magazine,1558-1896,PP,99,,2023,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10049816,"Integrating sensing and communication is a defining theme for future wireless systems. This is motivated by the promising performance gains, especially as they assist each other, and by the better utilization of the wireless and hardware resources. Realizing these gains in practice, however, is subject to several challenges where leveraging machine learning can provide a potential solution. This article focuses on ten key machine learning roles for joint sensing and communication, sensingaided communication, and communication-aided sensing systems, explains why and how machine learning can be utilized, and highlights important directions for future research. The article also presents real-world results for some of these machine learning roles based on the large-scale real-world dataset DeepSense 6G, which could be adopted in investigating a wide range of integrated sensing and communication problems.",,10.1109/MCOM.006.2200480,"Sensors , Machine learning , Hardware , Wireless communication , Interference cancellation , Optimization , Wireless sensor networks ",,,,,,,,,,
190,"Deep Learning Era for Future 6G Wireless Communications — Theory, Applications, and Challenges","Sangeetha, S.K.B. and Dhaya, R.",Artificial Intelligent Techniques for Wireless Communication and Networking,9781119821809,,,105-119,2022,,https://doi.org/10.1002/9781119821809.ch8,"Summary Over hundreds of years have passed since wireless communication technology was first introduced. The developers have made remarkable strides since 1880, including setting up an LTE network. Industry experts, from Verizon to Qualcomm, together with various experts, have a range of ideas about how to accelerate the evolution of wireless communication into ? and even beyond ? the 21st century. The wireless network technology requires smooth connectivity, easy accessibility, and safe connections at worldwide supply. 5G wireless technology is designed to provide higher data peak speeds, very reduced latency, increased reliability, massive network bandwidth, improved availability, plus an enhanced UI6G. Mobile users with a higher speed, superb coverage and no latency can broaden their existing mobile ecosystems into new worlds. 6 G will impact all sectors to increase the safety of transport, distributed health, agricultural accuracy, digitised logistics and more. Therefore, efforts on 6 G networks from both industry and academia have already been put into the study. Recently deep learning has been used as a new paradigm for designing and optimizing high-intelligence 6 G networks. With the realistic account of all challenges and opportunities of wireless technology from engineering perspectives, requires a complete study of available methods. In this chapter, huge potential efforts are made to study the background of 6G wireless communication with the detail of how deep learning made a contribution to 6G wireless technology. In the end, this chapter also highlights future research directions for deep learning-driven wireless technology.",,https://doi.org/10.1002/9781119821809.ch8,"Deep learning , 6G , Wireless communication , Artificial intelligence , Challenges ,   ,   ,   ,  ",,,,,,,,,,
191,Artificial Intelligence in Wireless Communications - Evolution Towards 6G Mobile Networks,"Teodor B. Iliev , Elena P. Ivanova , Ivaylo S. Stoyanov , Grigor Y. Mihaylov , Ivan H. Beloev ","2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",2623-8764,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9597147,"With the deployment of the 5G in wireless communications, the researchers' interest is focused on the sixth generation networks. This forthcoming generation is expected to replace the 5G network by the end of 2030. Artificial intelligence is one of the leading technologies in 5G, beyond 5G, and future 6G networks. Intelligence is endowing the tendency to throw open the capabilities of the 5G networks and the future 6G mobile wireless networks by leveraging the universal infrastructure, open network architectures, software-defined networking, network function virtualization, multi-access edge computing, vehicular network, etc. This discussion is aimed at providing, in a comprehensive manner, how artificial intelligence can be integrated into different applications and finally, we analyse and discuss the opportunities and main technical challenges of the wireless communication standards, present novel approaches, and recent results that will encourage the development and implementation of the sixth generation networks.",,10.23919/MIPRO52101.2021.9597147,"6G mobile communication , Underwater communication , Three-dimensional displays , 5G mobile communication , Wireless networks , Terahertz materials , Ultra reliable low latency communication ",,,,,,,,,,
192,Deep Learning-Aided 6G Wireless Networks: {A} Comprehensive Survey of Revolutionary {PHY} Architectures,"Ozpoyraz, Burak and Dogukan, Ali Tugberk and Gevez, Yarkin and Altun, Ufuk and Basar, Ertugrul",CoRR,,,,,2022,,https://arxiv.org/abs/2201.03866,"Deep learning (DL) has proven its unprecedented success in diverse fields such as computer vision, natural language processing, and speech recognition by its strong representation ability and ease of computation. As we move forward to a thoroughly intelligent society with 6G wireless networks, new applications and use-cases have been emerging with stringent requirements for next-generation wireless communications. Therefore, recent studies have focused on the potential of DL approaches in satisfying these rigorous needs and overcoming the deficiencies of existing model-based techniques. The main objective of this article is to unveil the state-of-the-art advancements in the field of DL-based physical layer (PHY) methods to pave the way for fascinating applications of 6G. In particular, we have focused our attention on four promising PHY concepts foreseen to dominate next-generation communications, namely massive multiple-input multiple-output (MIMO) systems, sophisticated multi-carrier (MC) waveform designs, reconfigurable intelligent surface (RIS)-empowered communications, and PHY security. We examine up-to-date developments in DL-based techniques, provide comparisons with state-of-the-art methods, and introduce a comprehensive guide for future directions. We also present an overview of the underlying concepts of DL, along with the theoretical background of well-known DL techniques. Furthermore, this article provides programming examples for a number of DL techniques and the implementation of a DL-based MIMO by sharing user-friendly code snippets, which might be useful for interested readers.",,,"Deep learning , 6G , Massive multiple-input multiple-output (MIMO) , Multi-carrier (MC) , Waveform designs , Reconfigurable intelligent surfaces (RIS) , Physical layer (PHY) security , Acquired Immunodeficiency Syndrome ,  ",,,,,,,,,,
193,Machine Learning for Physical Layer in 5G and beyond Wireless Networks: A Survey,"Tanveer, Jawad and Haider, Amir and Ali, Rashid and Kim, Ajung",Electronics,2079-9292,11,1,,2022,,https://www.mdpi.com/article/10.3390/electronics11010121,"Fifth-generation (5G) technology will play a vital role in future wireless networks. The breakthrough 5G technology will unleash a massive Internet of Everything (IoE), where billions of connected devices, people, and processes will be simultaneously served. The services provided by 5G include several use cases enabled by the enhanced mobile broadband, massive machine-type communications, and ultra-reliable low-latency communication. Fifth-generation networks potentially merge multiple networks on a single platform, providing a landscape for seamless connectivity, particularly for high-mobility devices. With their enhanced speed, 5G networks are prone to various research challenges. In this context, we provide a comprehensive survey on 5G technologies that emphasize machine learning-based solutions to cope with existing and future challenges. First, we discuss 5G network architecture and outline the key performance indicators compared to the previous and upcoming network generations. Second, we discuss next-generation wireless networks and their characteristics, applications, and use cases for fast connectivity to billions of devices. Then, we confer physical layer services, functions, and issues that decrease the signal quality. We also present studies on 5G network technologies, 5G propelling trends, and architectures that help to achieve the goals of 5G. Moreover, we discuss signaling techniques for 5G massive multiple-input and multiple-output and beam-forming techniques to enhance data rates with efficient spectrum sharing. Further, we review security and privacy concerns in 5G and standard bodies&rsquo; actionable recommendations for policy makers. Finally, we also discuss emerging challenges and future directions.",,10.3390/electronics11010121,"5G , B5G , Wireless networks , Machine learning , MIMO , Physical layer , IoT ,   ,  ",,,,,,,,,,
194,Adaptive Artificial Intelligence for Resource-Constrained Connected Vehicles in Cybertwin-Driven 6G Network,"Shuaiqi Shen , Chong Yu , Kuan Zhang , Song Ci ",IEEE Internet of Things Journal,2372-2541,8,22,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9502164,"The emerging technology of cybertwin is expected to bring revolutionary benefits to the sixth-generation (6G) network in respect of communication, resources allocation, and digital asset management. Empowered by ubiquitous artificial intelligence (AI), cybertwin is capable of adjusting the requests for computing resources to support network services by analyzing user’s demands for quality of experience and resource scarcity in the market. For resource-constrained applications, such as connected vehicles in the 6G network, cybertwin can intelligently determine the time-varying requests of computing resources for various vehicles at different times. However, the current service architecture executes AI algorithms with universal configurations for all vehicles. This causes the difficulty of customizing the complexity of AI algorithms to maintain adaptive to cybertwin’s decisions on dynamic resources allocation. In this article, we propose an adaptive AI framework based on efficient feature selection to cooperate with cybertwin’s resource allocation. This proposed framework can adaptively customizing AI model complexity with available computing resources. Specifically, we systematically characterize the aggregated impacts of all feature combinations on the modeling outcomes of AI algorithms. By utilizing nonadditive measures, the interactions among features can be quantified to indicate their contributions to the modeling process. Then, we propose an efficient algorithm to obtain accurate interaction measures for adaptive feature selection to balance the tradeoff between modeling accuracy and computational overhead. Finally, extensive simulations are conducted to validate that our proposed framework substantially reduces the overhead of AI algorithms while guaranteeing desired modeling accuracy for cybertwin-driven connected vehicles in 6G.",,10.1109/JIOT.2021.3101231,"Artificial intelligence , Computational modeling , Connected vehicles , Adaptation models , Dimensionality reduction , 6G mobile communication , Complexity theory ",,,,,,,,,,
195,"Remote Control of a Robot Rover Combining 5G, AI, and GPU Image Processing at the Edge","Civerchia, F. and Giannone, F. and Kondepu, K. and Castoldi, P. and Valcarenghi, L. and Bragagnini, A. and Gatti, F. and Napolitano, A. and Borromeo, Justine Cris",Optical Fiber Communication Conference (OFC) 2020,,,,M3Z.10,2020,Optica Publishing Group,https://ieeexplore.ieee.org/document/9083398,The demo shows the effectiveness of a low latency remote control based on 5G and image processing at the edge exploiting artificial intelligence and GPUs to make a robot rover slalom between posts.,,10.1364/OFC.2020.M3Z.10,"Image processing , Image recognition , Image recognition algorithms , Image transmission , Neural networks , Object detection , Robotics ,   ,  ",,,,,,,,,,
196,High-performance AES-128 algorithm implementation by FPGA-based SoC for 5G communications,"Visconti, Paolo and Velázquez, Ramiro and Capoccia, Stefano and De fazio, Roberto",International Journal of Electrical and Computer Engineering,,11,,4221-4232,2021,,https://www.researchgate.net/publication/352836108_High-performance_AES-128_algorithm_implementation_by_FPGA-based_SoC_for_5G_communications,"In this research work, a fast and lightweight AES-128 cypher based on the Xilinx ZCU102 FPGA board is presented, suitable for 5G communications. In particular, both encryption and decryption algorithms have been developed using a pipelined approach, so enabling the simultaneous processing of the rounds on multiple data packets at each clock cycle. Both the encryption and decryption systems support an operative frequency up to 220 MHz, reaching 28.16 Gbit/s maximum data throughput; besides, the encryption and decryption phases last both only ten clock periods. To guarantee the interoperability of the developed encryption/decryption system with the other sections of the 5G communication apparatus, synchronization and control signals have been integrated. The encryption system uses only 1631 CLBs, whereas the decryption one only 3464 CLBs, ascribable, mainly, to the Inverse Mix Columns step. The developed cypher shows higher efficiency (8.63 Mbps/slice) than similar solutions present in literature.",,10.11591/ijece.v11i5.pp4221-4232,"5G communications , Advanced encryption standard , Field programmable gate array , VHDL , Xilinx ZCU102 platform , Algorithms ,   ,   ,  ",,,,,,,,,,
197,Artificial Intelligence for 6G Networks: Technology Advancement and Standardization,"Muhammad K. Shehzad , Luca Rose , M. Majid Butt , István Z. Kovács , Mohamad Assaad , Mohsen Guizani ",IEEE Vehicular Technology Magazine,1556-6080,17,3,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9768336,"With the deployment of 5G networks, standards organizations have started working on the design phase for 6G networks. 6G networks will be immensely complex, requiring more deployment time, cost, and management efforts. On the other hand, mobile network operators demand these networks to be intelligent, self-organizing, and cost-effective to reduce operating expenses (OPEX). Machine learning (ML), a branch of artificial intelligence (AI), is the answer to many of these challenges by providing pragmatic solutions, which can entirely change the future of wireless network technologies. By using some case study examples, we briefly examine the most compelling problems, particularly at the physical layer (PHY) and link layer in cellular networks, where ML can bring significant gains. We also review standardization activities in relation to the use of ML in wireless networks and a future timeline on the readiness of standardization bodies to adapt to these changes. Finally, we highlight major issues in ML use in wireless technology, and provide potential directions to mitigate some of them in 6G wireless networks.",,10.1109/MVT.2022.3164758,"6G mobile communication , Wireless networks , Artificial intelligence , Principal component analysis , Channel estimation , Cellular networks , Unsupervised learning , Standards ",,,,,,,,,,
198,A Comprehensive Review on Artificial Intelligence/Machine Learning Algorithms for Empowering the Future IoT Toward 6G Era,"M. Rezwanul Mahmood , Mohammad Abdul Matin , Panagiotis Sarigiannidis , Sotirios K. Goudos ",IEEE Access,2169-3536,10,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9861650,"The evolution of the wireless network systems over decades has been providing new services to the users with the help of innovative network and device technologies. In recent times, the 5G network systems are about to be deployed which creates the opportunity to realize massive connectivity with high throughput, low latency, high energy efficiency and security. It also focuses on providing massive Internet of Things (IoT) network connectivity as well as services for good health, large-scale agricultural and industrial production, intelligent traffic control and electricity generation, transmission and distribution systems. However, the ever-increasing number of user devices is directing the researchers towards beyond 5G systems to allocate these user devices with higher bandwidth. Researches on the 6G wireless network systems have already begun to provide higher bandwidth availability for densely connected larger network devices with QoS surety. Researchers are leveraging artificial intelligence (AI)/machine learning (ML) for enhancing future IoT network operations and services. This paper attempts to discuss AI/ML algorithms that can help in developing energy efficient, secured and effective IoT network operations and services. In particular, our article concentrates on the major issues and factors that influence the design of the communication systems for future IoT with the integration of AI/ML. It also highlights application domains, including smart healthcare, smart agriculture, smart transportation, smart grid and smart industry that can operate efficiently and securely. Finally, this paper ends with the discussion on future research scopes with these algorithms in addressing the open issues of the future IoT network systems.",,10.1109/ACCESS.2022.3199689,"6G mobile communication , Communication systems , 5G mobile communication , Internet of Things , Wireless networks , Network systems , Energy efficiency , Artificial intelligence , Machine learning ",,,,,,,,,,
199,Machine Learning for 5G MIMO Modulation Detection,"Chikha, Haithem B. and Almadhor, Ahmad and Khalid, Waqas",Sensors,1424-8220,21,5,,2021,,https://www.mdpi.com/1009436,"Modulation detection techniques have received much attention in recent years due to their importance in the military and commercial applications, such as software-defined radio and cognitive radios. Most of the existing modulation detection algorithms address the detection dedicated to the non-cooperative systems only. In this work, we propose the detection of modulations in the multi-relay cooperative multiple-input multiple-output (MIMO) systems for 5G communications in the presence of spatially correlated channels and imperfect channel state information (CSI). At the destination node, we extract the higher-order statistics of the received signals as the discriminating features. After applying the principal component analysis technique, we carry out a comparative study between the random committee and the AdaBoost machine learning techniques (MLTs) at low signal-to-noise ratio. The efficiency metrics, including the true positive rate, false positive rate, precision, recall, F-Measure, and the time taken to build the model, are used for the performance comparison. The simulation results show that the use of the random committee MLT, compared to the AdaBoost MLT, provides gain in terms of both the modulation detection and complexity.",,10.3390/s21051556,"5G , Multi-relay cooperative MIMO systems , Modulation detection , Random committee machine learning technique , Learning ,   ,   ,   ,  ",,,,,,,,,,
200,Vehicle Artificial Intelligence System Based on Intelligent Image Analysis and 5G Network,"Liu, Baojing and Han, Chenye and Liu, Xinxin and Li, Wei",International Journal of Wireless Information Networks,1572-8129,30,1,86-102,2023,,https://doi.org/10.1007/s10776-021-00535-6,"Artificial Intelligence is a medium for machine intelligence that offers tremendous opportunities for the intelligent industrial revolution. Smart transport, computer networks, and networked intelligent cities benefit from the rapid development of networking technologies. It has opened up new possibilities for traffic safety, comfort, and quality solutions. Data-driven approaches are refined using artificial intelligence, a widely used technique in various scientific fields. The new 5G network infrastructure challenges the existing networking situation by addressing the failings of 4G. These emerging technologies give intelligent cities and autonomous networks an additional means of being completely connected, including in high-mobility or densely populated areas, with massive simultaneous connecting and the ubiquity of the web. In this paper, an artificial intelligence-based Vehicle to everything (AI-V2X) system has been used. The proposed method can collect knowledge from various sources, increase driver awareness, and anticipate possible collisions, thus increasing driving comfort, security, and performance. Combining high-speed, robust, low latency networking and AI technology, the complementary between the real world and digital information in Industry 4.0 is transformed into an intelligent vehicle. AI-V2X aims to explore the possible contribution of the new AI approaches to an autonomous vehicle search. This discussion comprehensively illustrates the effects of 5G technologies on Smart Cities, Smart Transportation Networks – including Autonomous and Semi-Auto-Communications, Technological, Economic and Legal challenges. Finally, AI-V2X has open issues and concerns in research that must be resolved to realize AI’s responsibility to enhance V2X systems fully. The results are obtained various analysis of vehicles image recognition of 5G networking as follows: improvement of transportation with 5G ratio is 84.2%, vehicle image monitoring on traffic ratio is 88.2%, development of V2X communication is 85.36%, increasing driving comfort ratio is 82.15% and reduction of road congestion on traffic ratio is 91.84%.",,10.1007/s10776-021-00535-6,"Artificial Intelligence , Vehicle to everything , 5G technologies , Intelligence ,   ,   ,   ,   ,  ",,,,,,,,,,
201,Analysis of Network Slicing for Management of 5G Networks Using Machine Learning Techniques,"Singh, Randeep and Mehbodniya, Abolfazl and Webber, Julian L. and Dadheech, Pankaj and Pavithra, G. and Alzaidi, Mohammed S. and Akwafo, Reynah and Rajakani, Kalidoss",Wireless Communications and Mobile Computing,1530-8669,2022,,9169568,2022,Hindawi,https://doi.org/10.1155/2022/9169568,"Consumer expectations and demands for quality of service (QoS) from network service providers have risen as a result of the proliferation of devices, applications, and services. An exceptional study is being conducted by network design and optimization experts. But despite this, the constantly changing network environment continues to provide new issues that today&#x2019;s networks must be dealt with effectively. Increased capacity and coverage are achieved by joining existing networks. Mobility management, according to the researchers, is now being investigated in order to make the previous paradigm more flexible, user-centered, and service-centric. Additionally, 5G networks provide higher availability, extremely high capacity, increased stability, and improved connection, in addition to quicker speeds and less latency. In addition to being able to fulfil stringent application requirements, the network infrastructure must be more dynamic and adaptive than ever before. Network slicing may be able to meet the present stringent application requirements for network design, if done correctly. The current study makes use of sophisticated fuzzy logic to create algorithms for mobility and traffic management that are as flexible as possible while yet maintaining high performance. Ultimately, the purpose of this research is to improve the quality of service provided by current mobility management systems while also optimizing the use of available network resources. Building SDN (Software-Defined Networking) and NFV (Network Function Virtualization) technologies is essential. Network slicing is an architectural framework for 5G networks that is intended to accommodate a variety of different networks. In order to fully meet the needs of various use cases on the network, network slicing is becoming more important due to the increasing demand for data rates, bandwidth capacity, and low latency.",,10.1155/2022/9169568,"Network management , Network slicing , 5G networks , Machine learning , Learning ,   ,   ,   ,  ",,,,,,,,,,
202,An FPGA-based 1-bit Digital Transmitter with 800-MHz Bandwidth for 5G Millimeter-wave Active Antenna Systems,"Masaaki Tanio , Shinichi Hori , Noriaki Tawa , Toshihide Kuwabara , Kazuaki Kunihiro ",2018 IEEE/MTT-S International Microwave Symposium - IMS,2576-7216,,,,2018,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8439663,"An FPGA-based 1-bit digital transmitter with an 800-MHz bandwidth is presented for 5G millimeter wave (mmW) active antenna systems (AASs). To achieve over 20-GHz operation of the 2nd-order delta sigma modulator (DSM), a two-stage time-in-terleaved accumulator with double look-ahead blocks is proposed and implemented in a field-programmable gate array (FPGA). This transmitter achieves an 800-MHz bandwidth in the 27-GHz band under conditions satisfying the 3GPP specification, which verifies that this transmitter is a promising candidate for 5G mmW communication.",,10.1109/MWSYM.2018.8439663,"Bandwidth , 5G mobile communication , Field programmable gate arrays , Signal to noise ratio , Radio transmitters , 3GPP ",,,,,,,,,,
203,Improved dropping attacks detecting system in 5g networks using machine learning and deep learning approaches,"Mughaid, Ala and AlZu’bi, Shadi and Alnajjar, Asma and AbuElsoud, Esraa and Salhi, Subhieh El and Igried, Bashar and Abualigah, Laith",Multimedia Tools and Applications,1573-7721,82,9,13973-13995,2023,,https://doi.org/10.1007/s11042-022-13914-9,"Non Orthogonal Multiple Access (NOMA) successfully drew attention to the deployment of 5th Generation (5G) wireless communication systems, and it is now considered a significant technology in 5G communications. The primary enhancement in 5G is the speed, which may be 100 times faster than 4G. Due to the rising number of internal or external attacks on the Network, wireless intrusion detection systems are a vital aspect of any system connected to the Internet, and 5G will demand considerable improvements in data rate and security. In this paper, we have built a simulator for NOMA and applied a dropping attack to extract a dataset from the simulation model. The accuracy for detecting dropping attacks using the extracted data after applying ML algorithms was 95.7% for LR. Furthermore, this work suggests a methodology for wireless cyberattack detection in 5G networks based on applying several ML and DL techniques such as Decision Trees, KNN, Multi-class Decision Jungle, Multi-class Decision Forest, and Multi-class Neural Network. The proposed work is implemented and tested using a comprehensive Wi-Fi network benchmark dataset. The conducted experiments resulted in an outstanding performance with an accuracy of 99% for the KNN algorithm and 93% for DF and Neural Network.",,10.1007/s11042-022-13914-9,"Cyber security , 5G networks , Dropping attack , Simulation , Machine learning , Attack detection , Syncope , Learning ,  ",,,,,,,,,,
204,Reduction of satellite images size in 5G networks using machine learning algorithms,"Moorthy, Talari Venkata Krishna and Budati, Anil Kumar and Kautish, Sandeep and Goyal, S.B. and Prasad, Kolalapudi Lakshmi",IET Communications,1751-8628,16,5,584-591,2022,The Institution of Engineering and Technology,https://doi.org/10.1049/cmu2.12354,"Abstract The high data volume of multispectral satellite images is compressed for better visual perception without loss of image and statistical properties of the local or global image to provide superior information for obtaining effective results using 5G networks. This compression is a technique applied to remote sensing applications to analyse the data for prediction or forecasting the real-time applications by remote sensing applications like IoT and data transmission over 5G wireless networks. The extensive data images have multiple bands, which contain earth surface/object information with various frequencies. It is difficult to handle this extensive data for processing data. The compression is mandatory to avoid this complexity by removing redundancy data, unnecessary pixel information and non-visual redundancy data between bands. There are various standard compression techniques are available like JPEG 2000, Wavelet and DCT methods. The proposed method is implemented with a combination of intra coding and machine learning algorithm. The standard compression technique does not give better results due to degradation of pixels, lack of spatial and spectral information. This paper enriches progressive results by reduced satellite images for transmission of data in IoT and 5G wireless networks, in which qualitative results are compared by standard compression technique with suitable parameters.",,https://doi.org/10.1049/cmu2.12354,"Satellite images , Image compression , 5G networks , Machine learning , Algorithms , Learning ,   ,   ,  ",,,,,,,,,,
205,"Deep Learning-Based Solutions for 5G Network and 5G-Enabled Internet of Vehicles: Advances, Meta-Data Analysis, and Future Direction","Almutairi, Mubarak S. and Akgul, Akif",Mathematical Problems in Engineering,1024-123X,2022,,6855435,2022,Hindawi,https://doi.org/10.1155/2022/6855435,"The advent of the 5G mobile network has brought a lot of benefits. However, it prompted new challenges on the 5G network cybersecurity defense system, resource management, energy, cache, and mobile network, therefore making the existing approaches obsolete to tackle the new challenges. As a result of that, research studies were conducted to investigate deep learning approaches in solving problems in 5G network and 5G powered Internet of Vehicles (IoVs). In this article, we present a survey on the applications of deep learning algorithms for solving problems in 5G mobile network and 5G powered IoV. The survey pointed out the recent advances on the adoption of deep learning variants in solving the challenges of 5G mobile network and 5G powered IoV. The deep learning algorithm solutions for security, energy, resource management, 5G-enabled IoV, and mobile network in 5G communication systems were presented including several other applications. New comprehensive taxonomies were created, and new comprehensive taxonomies were suggested, analysed, and presented. The challenges of the approaches are already discussed in the literature, and new perspective for solving the challenges was outlined and discussed. We believed that this article can stimulate new interest in practical applications of deep learning in 5G network and provide clear direction for novel approaches to expert researchers.",,10.1155/2022/6855435,"Deep learning , 5G , Internet of vehicles , Internet , Statistics as Topic ,   ,   ,   ,  ",,,,,,,,,,
206,FPGA based technical solutions for high throughput data processing and encryption for 5G communication: A review,"Visconti, Paolo and Velázquez, Ramiro and Del-Valle-Soto, Carolina and de Fazio, Roberto",TELKOMNIKA (Telecommunication Computing Electronics and Control),,,,,2021,,https://www.semanticscholar.org/paper/FPGA-based-technical-solutions-for-high-throughput-Visconti-Vel%C3%A1zquez/07b27893426fa6d9483bb2d512cd1583bfffa898,"The Field Programmable Gate Array (FPGA) devices are ideal solutions for high-speed processing applications, given their flexibility, parallel processing capability, and power efficiency. In this review paper, at first, an overview of the key applications of FPGA-based platforms in 5G networks/systems is presented, exploiting the improved performances offered by such devices. FPGA-based implementations of Cloud Radio Access Network (C-RAN) accelerators, Network Function Virtualization (NFV)-based network slicers, cognitive radio systems, and Multiple Input Multiple Output (MIMO) channel characterizers are the main considered applications that can benefit from the high processing rate, power efficiency and flexibility of FPGAs. Furthermore, the implementations of encryption/decryption algorithms by employing the Xilinx Zynq Ultrascale+ MPSoC ZCU102 FPGA platform are discussed, and then we introduce our high-speed and light weight implementation of th ewell-known AES-128 algorithm, developed on the same FPGA platform, and comparing it with similar solutions already published in the literature. The comparison results indicate that our AES-128 implementation enables efficient hardware usagefor a given data-rate (up to28.16 Gbit/s), resulting in higher efficiency (8.64 Mbps/Slice) than other considered solutions. Finally,the applicationsof the ZCU102 platform for high-speed processing are explored,such as image and signal processing, visual recognition, hardware resource management, etc.",,,"5G communication , Advanced encryption standard , Field programmable gate array , High-speed processing , Xilinx ZCU102 ,   ,   ,   ,  ",,,,,,,,,,
207,Demonstration of FPGA-based A-IFoF/mmWave transceiver integration in mobile infrastructure for beyond 5G transport,"P. Toumasis , K. Kanta , K. Tokas , I. Stratakos , E. A. Papatheofanous , G. Giannoulis , I. Mesogiti , E. Theodoropoulou , G. Lyberopoulos , G. Lentaris , D. Apostolopoulos , D. Reisis , D. Soudris , H. Avramopoulos ",2021 European Conference on Optical Communication (ECOC),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9606061,"We demonstrate the successful operation of an FPGA-based A-IFoF/mmWave transceiver into an existing MNO infrastructure, delivering 4K video streaming and IP-calls over mobile core network. Physical layer connectivity was successfully established, with EVM measurements of &#x003C;10% for QPSK waveforms propagated through different optical-wireless network segments.",,10.1109/ECOC52684.2021.9606061,"Phase shift keying , OFDM , Optical propagation , Europe , Streaming media , Optical variables measurement , Optical fiber networks ",,,,,,,,,,
208,GPU-Based LDPC Decoding for vRAN Systems in 5G and Beyond,"Chance Tarver , Matthew Tonnemacher , Hao Chen , Jianzhong Charlie Zhang , Joseph R. Cavallaro ",2020 IEEE International Symposium on Circuits and Systems (ISCAS),2158-1525,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9181064,"Next-generation virtual radio access networks (vRAN) will benefit from the flexibility provided by virtualization in proposed Cloud-RAN configurations. These systems for 5G and beyond may consist of commodity hardware such as GPUs in data centers with multiple connected base stations (gNBs) flexibly receiving allocated resources depending on time-varying, real-time demands. In this paper, parallel reconfigurable algorithms and architectures for channel decoding are proposed. In particular, flexible rate and block length LDPC decoders for the new radio (NR) physical layer on GPU are characterized. We implement these GPU decoders using reduced word lengths of 8-bits to represent the log-likelihood ratios during decoding, and we utilize multiple GPU streams to process multiple blocks of codewords in parallel. These techniques allow our implementation to reduce the device transfer overhead and achieve the low-latency or high-throughput targets for 5G and beyond. Moreover, we integrate our decoder into the Open Air Interface (OAI) NR software stack to investigate virtualization capabilities when containerizing vRAN functionality such as the LDPC decoder.",,10.1109/ISCAS45731.2020.9181064,"Parity check codes , 5G mobile communication , Graphics processing units , Decoding , Throughput , Kernel ",,,,,,,,,,
209,5G NR LDPC Decoding Performance Comparison between GPU & FPGA Platforms,"Alex Aronov , Leonid Kazakevich , Jane Mack , Fred Schreider , Scott Newton ","2019 IEEE Long Island Systems, Applications and Technology Conference (LISAT)",2641-8053,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8816821,"Fifth Generation NR is the global standard for the wireless air interface that promises to deliver high reliability and low end-to-end latency while delivering ultra-high-speed data. The 3rd Generation Partnership Project (3GPP) has standardized Low Density Parity Check (LDPC) coding as the solution to satisfy the channel coding demands of 5G NR. The trend towards virtualization of traditionally hardware functions to reduce development and equipment costs motivates a GPU based SDR platform for 5G NR. To that end, we developed an optimized 3GPP compliant LDPC decoder [1]. Performance data was collected on this enhanced decoder algorithm hosted on both a field programmable gate array (FPGA) and a graphic processing unit (GPU) platform. The advantages and disadvantages of FPGA and GPU technology for LDPC decoding are discussed. Both implementations align with Standards, but we show that the GPU solution exhibits larger latency primarily due to memory accesses. Future work for improving the LDPC decoder latency on the GPU is described.",,10.1109/LISAT.2019.8816821,,,,,,,,,,,
210,FPGA implementation of LDPC decoder for 5G NR with parallel layered architecture and adaptive normalization,"Alexandr Katyushnyj , Aleksei Krylov , Andrey Rashich , Chao Zhang , Kewu Peng ",2020 IEEE International Conference on Electrical Engineering and Photonics (EExPolytech),,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9243997,This paper presents the FPGA ASIC-like implementation of LDPC decoder for 5G NR BG2 and single lifting factor. The proposed implementation has two main features: architectural and algorithmic. The first is the parallel layered architecture with special offsets and inter-layer network which provides great scalability for variable lifting factors support. The second one is an adaptive normalization approach to increase decoder BER performance. The proposed implementation requires relatively small number resources of Xilinx Kintex Ultrascale FPGA and provides 1 information bit/s/cycle throughput for 10 iterations.,,10.1109/EExPolytech50912.2020.9243997,"5G mobile communication , Scalability , Throughput , Parity check codes , Decoding , Table lookup , Field programmable gate arrays ",,,,,,,,,,
211,Parallel-Processing-Based Digital Predistortion Architecture and FPGA Implementation for Wide-band 5G Transmitters,"Hai Huang , Jingjing Xia , Slim Boumaiza ",2019 IEEE MTT-S International Microwave Conference on Hardware and Systems for 5G and Beyond (IMC-5G),,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9160360,"This paper presents a bandwidth-scalable and hardware-efficient parallel-processing-based D PD architecture for wide-band 5G transmitters. By computing multiple data samples at each clock cycle in parallel, the proposed DPD architecture extends the bandwidth of a conventional serial DPD architecture, as limited by the maximum FPGA clock rate, to a much higher rate that is proportional to the number of parallel data paths. With a cross-bar structure devised to reroute the intermediate computation results between the parallel data paths, it allows advanced DPD model with memory and cross-terms to be constructed efficiently. F or proof-of-concept, the pruned Complexity-Reduced-Volterra (CRV) DPD with four parallel data paths has been implemented using an Xilinx Ultrascale+ FPGA to achieve a total linearization bandwidth of 1.25 GHz. Subsequently, a 28 GHz power amplifier modulated with 400 MHz QAM64 signals has been successfully linearized in the proposed DPD system in real-time.",,10.1109/IMC-5G47857.2019.9160360,"Field programmable gate arrays , Bandwidth , Engines , Clocks , Hardware , Computer architecture , Predistortion ",,,,,,,,,,
212,OPEX-Limited 5G RAN Slicing: an Over-Dataset Constrained Deep Learning Approach,"Hatim Chergui , Christos Verikoukis ",ICC 2020 - 2020 IEEE International Conference on Communications (ICC),1550-3607,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9149051,"In this paper, we investigate the concept of OPEX-limited resource provisioning as a key component in fifth generation (5G) radio access networks (RAN) slicing. The different RAN slices' tenants (i.e. logical operators) are dynamically allocated isolated portions of physical resource blocks (PRBs), baseband processing resources and backhaul capacity. To achieve this dynamic resource allocation, we rely on key performance indicators (KPIs) datasets stemming from a live cellular network endowed with traffic probes. These datasets are used to train a new class of deep neural networks (DNNs) models where OPEX requirements, formulated as non-convex non-differentiable violation rate constraints, are also dataset-dependent. The designed constrained DNNs are then optimized via a non-zero sum two-player game strategy. In this respect, we highlight the effect of the different hyperparameters on the respect of the OPEX limitations, while ensuring a dynamic RAN resource orchestration that follows the slices' traffics trends.",,10.1109/ICC40277.2020.9149051,"Optimization , Machine learning , Neural networks , Pricing , Facebook , Resource management , Cloud computing ",,,,,,,,,,
213,A Federated Deep Learning Empowered Resource Management Method to Optimize 5G and 6G Quality of Services (QoS),"Alsulami, Hemaid and Serbaya, Suhail H. and Abualsauod, Emad H. and Othman, Asem Majed and Rizwan, Ali and Jalali, Asadullah and Rani, Shalli",Wireless Communications and Mobile Computing,1530-8669,2022,,1352985,2022,Hindawi,https://doi.org/10.1155/2022/1352985,"The quality of service (QoS) in 5G/6G communication enormously depends upon the mobility and agility of the network architecture. An increase in the possible uses of 5G vehicular network simultaneously expands the scope of the network&#x2019;s quality of service (QoS). To this end, a safety-critical real-time system has become one of the most demanding criteria for the vehicular network. Although different mathematical and computation methods have traditionally been used to optimize the allocation of resources, but the nonconvexity of optimization issues creates unique type of challenges. In recent years, machine learning (ML) has emerged as a valuable tool for dealing with computational complexity that involves large amounts of data in heterogeneous vehicular networks. By using optimization and cutting-edge machine learning techniques, this article gives readers an insight about how 5G vehicular network resources can be allocated to reinforce network communication. Furthermore, a new federated deep reinforcement learning- (FDRL-) based vehicle communication method is presented as a new insight. Finally, a UAV-aided vehicular communication system based on FDRL-based UAVs is proposed as a novel resource management technique to optimize 5G and 6G quality of services.",,10.1155/2022/1352985,"Federated deep learning , Resource management , 5G/6G , Quality of service ,   ,   ,   ,   ,  ",,,,,,,,,,
214,FPGA Design of Spatially Modulated Single-Input-Multiple-Output Signals in 5G Diversity Receivers,"Rafic Ayoubi , Jihad Daba ","2019 IEEE International Conference on Communication, Networks and Satellite (Comnetsat)",,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8844099,"In this work, FPGA implementation of a new optimal generalized receiver diversity combining scheme, termed Generalized Maximum Ratio Combining (GMRC), is implemented for transmission via 5G multiple-input-multiple-output (MIMO) channels. The MIMO channels comprise binary phase shift keying-spatially modulated (BPSK-SM) single-input-multiple-output (SIMO) channels that are conceived from robust selective combining of transmit diversity channels. The main disadvantage of GMRC is the fundamental nature of its analysis, which prompts us to investigate the feasibility of a FPGA implementation using a pipeline structure. Such implementation can serve as a practical test-bed for real-life wireless applications. Prior published FPGA implementation applied brute-force technique that led to the use of several square root blocks, which are slow and resource-hungry. In this work, all operations are transformed into addition and multiplication operations only, which are efficient in current FPGA technology due to the availability of such operations at the hardware level. Another important feature of the implementation is pipelining, which further leads to an improved clock cycle and subsequently higher throughput. Using the FPGA implementation on the SIMO channel, the design can be extended to the hardware of spatially modulated hyper-MIMO based 5th generation networks.",,10.1109/COMNETSAT.2019.8844099,"Diversity reception , Fading channels , Field programmable gate arrays , Pipeline processing , Hardware , Receiving antennas , Estimation error ",,,,,,,,,,
215,A vision on the artificial intelligence for 6G communication,"Ahammed, Tareq B. and Patgiri, Ripon and Nayak, Sabuzima",ICT Express,2405-9595,,,,2022,,https://www.sciencedirect.com/science/article/pii/S2405959522000741,"The 6G communication network will be a sixth-sense next-generation communication network, which will increase the worthiness of the intelligent Internet of Things. With the advent of various fields of artificial intelligence, 6G will create enormous possibilities, that is, Augmentation of Human Intelligence, Internet of Everything, Quality of Experiences, Quality of Life, etc. Artificial intelligence and 6G communication technology will completely change from connected things to connected intelligence. This article summarizes the scope of artificial intelligence in making a revolutionized 6G communication technology. We directly focus on implementing suitable applications that solve human needs and problems. Moreover, we emphasize such technology that can create value for new technologies.",,https://doi.org/10.1016/j.icte.2022.05.005,6G communication;Artificial intelligence;Edge AI;Quality of Life;Quality of Experience;Cognitive intelligence;Federated AI;Black-box;Data Science;Big Data;Unmanned Aerial Vehicles;Intelligence,,,,,,,,,,
216,Overview of Distributed Machine Learning Techniques for 6G Networks,"Muscinelli, Eugenio and Shinde, Swapnil Sadashiv and Tarchi, Daniele",Algorithms,1999-4893,15,6,,2022,,https://www.mdpi.com/1999-4893/15/6/210,"The main goal of this paper is to survey the influential research of distributed learning technologies playing a key role in the 6G world. Upcoming 6G technology is expected to create an intelligent, highly scalable, dynamic, and programable wireless communication network able to serve many heterogeneous wireless devices. Various machine learning (ML) techniques are expected to be deployed over the intelligent 6G wireless network that provide solutions to highly complex networking problems. In order to do this, various 6G nodes and devices are expected to generate tons of data through external sensors, and data analysis will be needed. With such massive and distributed data, and various innovations in computing hardware, distributed ML techniques are expected to play an important role in 6G. Though they have several advantages over the centralized ML techniques, implementing the distributed ML algorithms over resource-constrained wireless environments can be challenging. Therefore, it is important to select a proper ML algorithm based upon the characteristics of the wireless environment and the resource requirements of the learning process. In this work, we survey the recently introduced distributed ML techniques with their characteristics and possible benefits by focusing our attention on the most influential papers in the area. We finally give our perspective on the main challenges and advantages for telecommunication networks, along with the main scenarios that could eventuate.",,10.3390/a15060210,Learning,,,,,,,,,,
217,Collaborative Machine Learning for Energy-Efficient Edge Networks in 6G,"Xiaoyan Huang , Ke Zhang , Fan Wu , Supeng Leng ",IEEE Network,1558-156X,35,6,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9687500,"To fulfill the diversified requirements of the emerging Internet of Everything (IoE) applications, the future sixth generation (6G) mobile network is envisioned as a heterogeneous, ultra-dense, and highly dynamic intelligent network. Edge intelligence is a vital solution to enable various intelligent services to improve the quality of experience of resource-constrained end users. However, it is very challenging to coordinate the independent but interrelated edge nodes in a decentralized learning manner to improve their strategies. In this article, we propose a decentralized and collaborative machine learning architecture for intelligent edge networks to achieve ubiquitous intelligence in 6G. Considering energy efficiency to be an essential factor in building sustainable edge networks, we design a multi-agent deep reinforcement learning (DRL)-empowered computation offloading and resource allocation scheme to minimize the overall energy consumption while ensuring the latency requirement. Further, to decrease the computing complexity and signaling overhead of the training process, we design a federated DRL scheme. Numerical results demonstrate the effectiveness of the proposed schemes.",,10.1109/MNET.100.2100313,"6G mobile communication , Training data , Energy consumption , System performance , Collaboration , Computer architecture , Collaborative work ",,,,,,,,,,
218,Role of 5G and Artificial Intelligence for Research and Transformation of English Situational Teaching in Higher Studies,"Yu, Haojie and Nazir, Shah and Khattak, Hasan Ali",Mobile Information Systems,1574-017X,2021,,3773414,2021,Hindawi,https://doi.org/10.1155/2021/3773414,"We live in a modern and technological society run by intelligent and human-like machines and systems. This is due to the advancements in the field of artificial intelligence. The machines are directly or indirectly used in different sectors like healthcare, automatic vehicles, and complex decision-making and at the same used in educational institutes. The usage of AI-based systems and the Internet has brought numerous educational innovations for both teachers and students. With the online learning platforms grounded on AI techniques, 5G has revolutionized the teaching and learning methods by smooth and faster access to educational content. Students of foreign languages, especially English learners, can now use chatbots and intelligent tutoring systems to learn and practice their speaking and listening skills offline and online. With Computer-Assisted Language Learning (CALL), the English learning process can now be interactive and productive. The students can now improve their language skills by conversing with AI-based agents instead of native speakers to avoid any fear and anxiety. The intelligent platforms can understand the consuming power of the student and hence can create and give content according to their level to create an individualized learning environment. With the help of digital assistants, people can also find it very easy and productive to improve English proficiency. To accomplish the goal of English teaching very easily and ideally, the teachers should use AI-based techniques in the classrooms. With the help of intelligent assistants for the daily workload of a teacher, we will be able to concentrate fully on the language learning and skills of the students. The current study has presented a detailed overview of 5G and AI&#x2019;s role in research and transformation of English situational teaching in higher studies. The search results are compiled and presented with different details of the area.",,10.1155/2021/3773414,Intelligence,,,,,,,,,,
219,Future OFDM-based Communication Systems Towards 6G and Beyond: Machine Learning Approaches,"Juwono, Filbert H. and Reine, Regina",Green Intelligent Systems and Applications,,1,1,19–25,2021,,https://tecnoscientifica.com/journal/gisa/article/view/34,,,10.53623/gisa.v1i1.34,,,,,,,,,,,
220,Performance enhancement of FSO communication system using machine learning for 5G/6G and IoT applications,"Kumar, Lepuri Jathin Sravan and Krishnan, Prabu and Shreya, Biradher and M.S., Sudhakar",Optik,0030-4026,252,,168430,2022,,https://www.sciencedirect.com/science/article/pii/S0030402621019343,"6G networks will provide extremely high capacity and will support a wide range of new applications in the future, but the existing frequency bands may not be sufficient. Furthermore, because traditional wireless communications are incapable of providing high-speed data rates, 6G enables superior coverage by integrating space/air/underwater networks with terrestrial networks. 5G-and-beyond (5 GB) and 6G networks have been mandated as a paradigm shift to take the enhanced broadband, massive access, and ultra-reliable and low latency services of 5G wireless networks to an even more advanced and intelligent level, to meet the ever-growing quantities of demanding services. In 5G and 6G wireless communication systems, artificial intelligence (AI), particularly machine learning (ML), has emerged as an essential component of fully intelligent network orchestration and management. 5 GB and 6G communication systems will also rely heavily on a tactile Internet of Things (IoT). The diverse nature of heterogeneous traffic and the established service quality parameters in 5 GB networks will present numerous challenges. Many other wireless technologies, including free space optics (FSO), look promising for meeting the demands of 5 GB systems. FSO has been identified as a promising technology for achieving higher data rates while consuming less power. However, attenuation due to weather, pointing errors, and turbulences limits its performance. Traditional Maximum likelihood decoding techniques require prior channel information to decode the signals. in this paper, first time we proposed a novel decoding technique for decoding on–off keying (OOK) modulated FSO signals using support vector machines (SVM). The model is tested under various atmospheric weather conditions such as fog, rain, and snow, as well as turbulence and pointing errors. Simulated numerical results demonstrate that the proposed SVM-based decoding schemes are capable of mitigating attenuation, pointing error, and turbulent channel impairments.",,https://doi.org/10.1016/j.ijleo.2021.168430,Free space optical communication;Support vector machine;Maximum likelihood;Attenuation;Turbulence,,,,,,,,,,
221,5G Traffic Prediction Based on Deep Learning,"Gao, Zihang and Gu, Yang",Computational Intelligence and Neuroscience,1687-5265,2022,,3174530,2022,Hindawi,https://doi.org/10.1155/2022/3174530,"The demand of wireless access users is increasing explosively. The 5G network traffic is increasing exponentially and showing a trend of diversity and heterogeneity, which makes network traffic forecasting face many challenges. By studying the actual performance of the 5G network, this paper makes an accurate prediction of the 5G network and builds a smoothed long short-term memory (SLSTM) traffic prediction model. The model updates the number of layers and hidden units according to the prediction accuracy adaptive mechanism. At the same time, in order to reduce the randomness of the 5G traffic sequence, the output feature sequence of the original time series is stabilized by the seasonal time difference method. In the experiments, the prediction results of the proposed algorithm are compared with those of the traditional algorithms. The results show that the SLSTM algorithm can effectively improve the accuracy of 5G traffic prediction. The model can be used for 5G traffic prediction for decision-making.",,10.1155/2022/3174530,Learning,,,,,,,,,,
222,Research on clothing design based on 5G network and FPGA,"Ma, Tingting",Microprocessors and Microsystems,0141-9331,81,,103757,2021,,https://www.sciencedirect.com/science/article/pii/S0141933120309029,"Mobile phone technology is thought to have developed in huge demand in the next few years, it exists. 5 G and related techniques, better data transmission rates, improved productivity, low latency, and high-quality services such as a number of major goals, can be solved. To analyze the cultural characteristics of the performance, explaining the characteristics of thinking of positioning principles and modern clothing brand. From the perspective of expressionism and print advertising, literary support, case studies and image analysis, Self-Organizing Map (SOM) network training analysis, summarizing the relationship with the method of painting, the influence of expressionism Conceptual picture summaries, modes, compositions and colors are represented in recent international clothing brand print advertisements. Evaluated and suitable for communication methods such as discontinuous carrier aggregation, centralized processing, and 5 G waveform coexistence Field Programmable Gate Array (FPGA) baseband processing architecture. Cost-effectiveness, and support for5G waveform candidates. Design details, key technologies, and how to actually implement a smart clothing system. A typical application for smart clothing. It still highlights some design challenges and open issues that need to be addressed to make smart clothes for a wide range of applications.",,https://doi.org/10.1016/j.micpro.2020.103757,Clothing brand;Field programmable gate array (FPGA);5G-network;Smart clothing;Aggregation,,,,,,,,,,
223,NVIDIA Aerial GPU Hosted AI-on-5G,"Anupa Kelkar , Chris Dick ",2021 IEEE 4th 5G World Forum (5GWF),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9605055,"In this paper we present the NVIDIA hyper-converged platform supporting 5G connectivity and Mobile Edge Computing (MEC). 5G connectivity is realized with our Aerial [1] GPU-based cloud native 5G gNB. We introduce AI-on-5G on a converged accelerator to showcase our innovation in being able to host Aerial vRAN baseband processing, AI/ML training and inference, data analytics and other workloads. In other words, a data center at the edge that is provisioned with 5G connectivity as a service. We describe 3 uses-cases that highlight how existing NVIDIA AI/ML development frameworks, together with Aerial, can be leveraged to bring Industry 4.0 to reality.As an open platform Aerial is positioned to be industry transformational by providing researchers with a platform for next generation wireless and AI research.Aerial seeds the research ecosystem with a first-class out-of-the-box (OOB) experience with a standards compliant 5G NR PHY. Researchers can run the supplied 3GPP compliant test vectors, and perform over-the-air experiments, using standard servers equipped with a GPU-based PCIe card. The PHY code base can be tailored to support research that combines AI/ML with 5G wireless.",,10.1109/5GWF52925.2021.00019,"Wireless communication , Training , Base stations , Technological innovation , 5G mobile communication , Computational modeling , Graphics processing units ",,,,,,,,,,
224,WITHDRAWN: Medical intelligent infusion monitoring system based on 5G network and FPGA,"Bei, Cuilin and Liu, Yanli and Gu, Jungai",Microprocessors and Microsystems,0141-9331,,,103393,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120305500,"This article has been withdrawn: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/our-business/policies/article-withdrawal). This article has been withdrawn at the request of the Editor in Chief. Subsequent to acceptance of this special issue paper by the responsible Guest Editor Sundhararajan Mahalingam, the integrity and rigor of the peer-review process was investigated and confirmed to fall beneath the high standards expected by Microprocessors & Microsystems. There are also indications that much of the Special Issue includes unoriginal and heavily paraphrased content. Due to a configuration error in the editorial system, unfortunately the Editor in Chief did not receive these papers for approval as per the journal's standard workflow.",,https://doi.org/10.1016/j.micpro.2020.103393,Intelligence,,,,,,,,,,
225,Performance evaluation of offloading LDPC decoding to an FPGA in 5G baseband processing,"Florian Kaltenberger , Hongzhi Wang , Sakthivel Velumani ",WSA 2021; 25th International ITG Workshop on Smart Antennas,,,,,2021,VDE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9739186,"Offloading of computationally expensive physical layer processing such as the forward error correction, is one of the key enablers for a fully virtualized open radio access network (RAN). In this paper we show such an offloading architecture and will demonstrate it using the OpenAirInterface 5G New Radio open source software and the Xilinx T1 telco accelerator card. We will show the feasibility and the potential savings of such an architecture.",,,,,,,,,,,,,
226,A High Throughput and Flexible Rate 5G NR LDPC Encoder on a Single GPU,"Shixin Liao , Yueying Zhan , Ziyuan Shi , Lei Yang ",2021 23rd International Conference on Advanced Communication Technology (ICACT),1738-9445,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9370366,"In order to build a high performance low-density parity-check (LDPC) communication link simulation platform, high speed LDPC encoding for information sequence is required. In this paper, a high and flexible throughput LDPC encoding implementation based on a single GPU is proposed. We discuss the parallelism of the LDPC encoding algorithm employs the core parity check bits and single diagonal parity check bits for the fifth generation new ratio. We implement the parallel LDPC encoder on CUDA platform. The experimental results show that our LDPC encoding module achieves a 38-62Gbps throughput for the rate from 1/2 to 8/9 on a single GPU. The results also demonstrate that parallel simulation tasks based on GPUs can achieve a good trade-off between performance and cost.",,10.23919/ICACT51234.2021.9370366,"5G mobile communication , Graphics processing units , Parallel processing , Throughput , Parity check codes , Encoding , Task analysis ",,,,,,,,,,
227,Research and Implementation of eCPRI Processing Module for Fronthaul Network on FPGA in 5G – NR gNodeB Base Station,"Dang Tuan Kiet , Tran Minh Hieu , Nguyen Quang Hung , Nguyen Van Cuong , Vo Thanh Van , Pham Ngoc Cuong ","2020 4th International Conference on Recent Advances in Signal Processing, Telecommunications & Computing (SigTelCom)",,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9199019,"Fronthaul is associated with a new and different type of Radio Access Network architecture consisting of centralized Baseband Units (BBUs) and Remote Radio Units (RRUs) [2] . In order to meet the fifth-generation wireless (5G) challenges of increased traffic and data flows, a new connection protocol for the fronthaul network called Evolved Common Public Radio Interface (eCPRI) has been published. This paper aims to design a system that supports this interface to transmit data from the BBUs to RRUs in 5G Radio Access Network (RAN). By referencing the specification of eCPRI, we develop a hardware system on FPGA to encapsulate and extract required headers for this protocol. The system is compatible with a 10Gb Ethernet subsystem that provides 10Gb Ethernet MAC and a Physical Interface. On top of the system, we develop drivers and applications to configure and run the system.",,10.1109/SigTelCom49868.2020.9199019,"IP networks , Protocols , 5G mobile communication , Field programmable gate arrays , Signal processing , Radio access networks ",,,,,,,,,,
228,FPGA based design and prototyping of efficient 5G QC-LDPC channel decoding,"Jeremy Nadal , Amer Baghdadi ",2020 International Workshop on Rapid System Prototyping (RSP),2150-5500,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9244853,"The Quasi-Cyclic (QC) Low-Density ParityCode (LDPC) is the key error correction code for the 5th Generation (5G) of cellular network technology. Designed to support several frame sizes and code rates, the 5G LDPC code structure allows high parallelism to deliver the high demanding data rate of 10 Gb/s. This impressive performance introduces challenging constraints on the hardware design. Particularly, allowing such high flexibility can introduce processing rate penalties on some configurations. In this context, a novel efficient and flexible hardware architecture for the 5G LDPC decoder is proposed, targeting Field Programmable Gate Array (FPGA) devices and supporting all 5G configurations. The architecture supports frame parallelism to maximize the utilization of the processing units, significantly improving the processing rate. Compared to a recent commercial 5G LDPC decoder, the proposed FPGA prototype achieves a higher processing rate for most configurations while having similar complexity.",,10.1109/RSP51120.2020.9244853,"5G mobile communication , Parallel processing , Throughput , Parity check codes , Hardware , Decoding , Field programmable gate arrays ",,,,,,,,,,
229,FPGA-based adaptive space–time compression towards 5G MIMO fronthaul,"Zhu, Paikun and Yoshida, Yuki and Kitayama, Ken-ichi",Optics Communications,0030-4018,459,,124913,2020,,https://www.sciencedirect.com/science/article/pii/S0030401819310156,"Data compression-assisted radio-over-fiber (RoF) is a candidate solution to improve fronthaul (FH) bandwidth efficiency without losing cell-site simplicity for cloud/centralized radio access network (C-RAN) in fifth-generation (5G) mobile networks. One of the challenges in FH in 5G & beyond lies in MIMO fronthauling, where the FH bandwidth requirement scales (in spatial dimension) with number of antennas and thus cannot be resolved by conventional compression approaches tackling only time dimension. Also, it is desirable to implement FH compressor on real-time hardware to evaluate the implementation complexity, signal quality and processing latency. In this paper, we target 5G MIMO FH, and present field-programmable gate array (FPGA) based hardware design, characterization and evaluation of our proposed adaptive space–time FH compression technique. We experimentally demonstrate 8-antenna MIMO FH uplink enabled by the FPGA-based compressor. 48 Gb/s CPRI-equivalent rate encapsulating 1024QAM 5G new radio (NR)-like signals is transported using only 2.5 GBd optical PAM4, achieving <1.25% EVM with low processing latency. The FPGA demonstration indicates the feasibility of the space–time compression technique towards practice, which may be employed in not only FH network, but also other applications such as indoor MIMO distributed antenna system (DAS) or converged fiber-wireless transmission systems.",,https://doi.org/10.1016/j.optcom.2019.124913,5G;MIMO;Fronthaul;Analog-to-digital compression (ADX);Field programmable gate array (FPGA);Radio-over-fiber (RoF),,,,,,,,,,
230,FPGA Implementation of FBMC Baseband Modular for 5G Wireless Communication,"R. Keerthana , S. Rajaram ","2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)",,1,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8993290,"In recent years research, selection in regard to a potential waveform successor for the fifth generation (5G) telecommunication systems has been focused. On a contrary with OFDM, Filter-bank Multicarrier (FBMC) modulation has been put forward as a next valid waveform to be used in 5G systems because to its finer spectral efficiency and reduced out-of-band emissions. As an outcome, baseband processors to be developed in hardware, as the current state of the work was done, evaluating former design proposals for instance Polyphase Network and Frequency Spreading FBMC Transmitter. Based on the research, architecture to be realized was chosen. The system technologically advanced on comparison with a prevailing software model using MATLAB. The reconfiguration, concise size, high computational power makes FPGA efficient in DSP applications. The developed work involves modelling and execution of FBMC baseband modulator and evaluates key metrics concerning performance, resource utilization and power consumption on FPGA. The proposed architecture is simulated and synthesized using Verilog HDL in Xilinx ISE Design Suite 12.1 and to be implemented on Spartan -3 XC3S50 FPGA board. The recommended structure can be progressed further to surge the complexity of the hardware structure as a result of which a reduced amount of resource could be used to model prototype hardware for the presented FBMC scheme.",,10.1109/ICICICT46008.2019.8993290,,,,,,,,,,,
231,FPGA Demonstration of Adaptive Low-latency High-fidelity Analog-to-digital Compression for Beyond-5G Wireless-wired Conversion,"Paikun Zhu , Yuki Yoshida , Ken-ichi Kitayama ",2019 24th OptoElectronics and Communications Conference (OECC) and 2019 International Conference on Photonics in Switching and Computing (PSC),,,,,2019,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8818151,Time-domain analog-to-digital compression (ADX) is designed and implemented on FPGA for low-complexity high-fidelity wireless-wired signal conversion. We demonstrate 50ns processing latency and $EVM < 0.6\%\ over > 30dB$ input power range for 4096QAM-modulated OFDM and single-carrier radio signals.,,10.23919/PS.2019.8818151,"Field programmable gate arrays , Wireless communication , OFDM , Bandwidth , Real-time systems , Decoding , Time-domain analysis ",,,,,,,,,,
232,Deep Learning Enabled IRS for 6G Intelligent Transportation Systems: A Comprehensive Study,"Wei Song , Shaik Rajak , Shuping Dang , Ruijun Liu , Jun Li , Sunil Chinnadurai ",IEEE Transactions on Intelligent Transportation Systems,1558-0016,PP,99,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9806434,"Intelligent Transportation Systems (ITS) play an increasingly significant role in our life, where safe and effective vehicular networks supported by sixth-generation (6G) communication technologies are the essence of ITS. Vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications need to be studied to implement ITS in a secure, robust, and efficient manner, allowing massive connectivity in vehicular communications networks. Besides, with the rapid growth of different types of autonomous vehicles, it becomes challenging to facilitate the heterogeneous requirements of ITS. To meet the above needs, intelligent reflecting surfaces (IRS) are introduced to vehicular communications and ITS, containing the reflecting elements that can intelligently configure incident signals from and to vehicles. As a novel vehicular communication paradigm at its infancy, it is key to understand the latest research efforts on applying IRS to 6G ITS as well as the fundamental differences with other existing alternatives and the new challenges brought by implementing IRS in 6G ITS. In this paper, we provide a big picture of deep learning enabled IRS for 6G ITS and appraise most of the important literature in this field. By appraising and summarizing the existing literature, we also point out the challenges and worthwhile research directions related to IRS aided 6G ITS.",,10.1109/TITS.2022.3184314,"6G mobile communication , Channel estimation , MIMO communication , Optimization , Wireless networks , Deep learning , Array signal processing ",,,,,,,,,,
233,"Balancing QoS and Security in the Edge: Existing Practices, Challenges, and 6G Opportunities With Machine Learning","Zubair Md Fadlullah , Bomin Mao , Nei Kato ",IEEE Communications Surveys & Tutorials,2373-745X,24,4,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9831429,"While the emerging 6G networks are anticipated to meet the high-end service quality demands of the mobile edge users in terms of data rate and delay satisfaction, new attack surfaces and zero-day attacks continue to pose significant threats to their successful realization and rollouts. Traditionally, most service provisioning techniques considered security metrics separately from the Quality of Service (QoS) and Quality of Expectation (QoE) parameters. The QoS/QoE parameters include data throughput, experienced delay, tolerable latency, jitter, resource utilization rate, spectral efficiency, energy efficiency, fairness, and other emerging key performance indicators (KPIs). Also, there are various security attributes, such as encryption key strength, authentication strength, network anomaly score, privacy metric, and so on. Typically, the resource allocation optimization techniques to maximize the security aspects to protect the communication of mobile users or user equipment (UEs) have an adverse effect on the service quality. Therefore, a key research gap exists in balancing service quality and security levels in communication networks that has been either overlooked or identified in a rather scattered manner by researchers in the recent decade. Thus, a comprehensive survey of the state-of-the-art to clearly address this research gap and outline the possible solutions is yet to appear in the existing literature. In this paper, we address this by surveying the existing practices, challenges, and opportunities in the emerging 6G (i.e., beyond 5G) networks, where various AI (Artificial Intelligence)-based techniques such as deep learning meet the classical optimization techniques, to balance the service performance and security levels. Several networking topologies with relevant use-cases are included in the survey to discuss the existing and emerging trends of isolated as well as joint treatment of service and security levels. Lessons learned from each use-case are provided to demonstrate a clear road map for the interested readers and researchers in emerging networks to construct a natively combined service and security ecosystem, specifically in the network edge.",,10.1109/COMST.2022.3191697,"Quality of service , Security , 6G mobile communication , Optimization , 5G mobile communication , Measurement , Throughput ",,,,,,,,,,
234,A Deep Learning Assisted Software Defined Security Architecture for 6G Wireless Networks: IIoT Perspective,"Md. Abdur Rahman , M. Shamim Hossain ",IEEE Wireless Communications,1558-0687,29,2,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9801721,"The 6G wireless network is expected to drive cyber-physical systems (CPS) from merely connected things to securely connected intelligence. While 6G will offer real-time communication between cyber and physical entities, due to convergence of operational technology (OT) and information technology (IT) networks, security, and trustworthiness of the massive amount of data shared between cyber and physical entities will remain of great concern. Attackers having AI capability will be able to mount massive numbers of automated and novel attacks on the future 6G network. Human security specialists teaming with an AI-powered adaptive defense mechanism will be needed to counter emerging AI-based attacks on the massively connected CPS through 6G wireless networks. 6G networks are expected to add industrial immunity to IT, OT, and IIoT networks with the help of AI. 6G is expected to offer deep learning (DL) assisted security function virtualization (SFV) to support software defined security (SDS) architecture for dynamic defense mechanisms, intelligently monitor network traffic anomalies at different network endpoints and segments, and offer increased visibility across attack surfaces. In this article, we study the security challenges in 6G networks posed by the recent convergence of OT and IT networks and propose distributed DL-assisted SDS for 6G vertical that will autonomously detect, localize, and isolate security threats via SFV. Finally, we present future directions and the challenges ahead.",,10.1109/MWC.006.2100438,"6G mobile communication , Deep learning , Wireless networks , Computer architecture , Software , Real-time systems , Security , Cyber-physical systems ",,,,,,,,,,
235,GPF+: A Novel Ultrafast GPU-Based Proportional Fair Scheduler for 5G NR,"Yan Huang , Shaoran Li , Y. Thomas Hou , Wenjing Lou ",IEEE/ACM Transactions on Networking,1558-2566,30,2,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9582828,"5G NR is designed to operate over a broad range of frequency bands and support new applications with ultra-low latency requirements. To support its extremely diverse operating conditions, multiple OFDM numerologies have been defined in the 5G standards. Under these numerologies, it is necessary to perform scheduling with a time resolution of  $\sim 100 \mathrm {\mu s}$ . This requirement poses a new challenge beyond existing LTE and cannot be satisfied by any existing LTE schedulers. In this paper, we present the design of GPF+, which is a GPU-based proportional fair (PF) scheduler with timing performance under  $100 \mathrm {\mu s}$ . GPF+ is an improvement over our GPF in Huang et al. (2018). The key ideas include decomposing the original scheduling problem into a large number of small and independent sub-problems and selecting a subset of sub-problems from the most promising search space to fit into a GPU. By implementing GPF+ on an off-the-shelf NVIDIA Tesla V100 GPU, we show that GPF+ is able to achieve near-optimal PF performance with timing performance under  $100 \mathrm {\mu s}$ . GPF+ represents the fastest GPU-based PF scheduler that can meet the new real-time requirement in 5G NR.",,10.1109/TNET.2021.3118005,"Graphics processing units , 5G mobile communication , Scheduling , Long Term Evolution , Processor scheduling , Optimal scheduling , OFDM ",,,,,,,,,,
236,A GPU Hyperconverged Platform for 5G vRAN and Multi - Access Edge Computing,"Anupa Kelkar , Chris Dick ",2021 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),0840-7789,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9569133,"In this paper we present the NVIDIA hyper-converged platform supporting 5G connectivity and Mobile Edge Computing (MEC). 5G connectivity is realized with our Aerial [1] GPU-based cloud native 5G gNB. We introduce AI-on-5G on a converged accelerator to showcase our innovation in being able to host Aerial vRAN baseband processing, AI/ML training and inference, data analytics and other workloads. In other words, a data center at the edge that is provisioned with 5G connectivity as a service. We describe 3 uses-cases that highlight how existing NVIDIA AI/ML development frameworks, together with Aerial, can be leveraged to bring Industry 4.0 to reality. As an open platform Aerial is positioned to be industry transformational by providing researchers with a platform for next generation wireless and AI research. Aerial seeds the research ecosystem with a first-class out-of-the-box (OOB) experience with a standards compliant 5G NR PHY. Researchers can run the supplied 3GPP compliant test vectors, and perform over-the-air experiments, using standard servers equipped with a GPU-based PCIe card. The PHY code base can be tailored to support research that combines AI/ML with 5G wireless.",,10.1109/CCECE53047.2021.9569133,"Wireless communication , Training , Base stations , Technological innovation , 5G mobile communication , Computational modeling , Graphics processing units ",,,,,,,,,,
237,Channel Estimation Using Deep Learning on an FPGA for 5G Millimeter-Wave Communication Systems,"Pavan Kumar Chundi , Xiaodong Wang , Mingoo Seok ",IEEE Transactions on Circuits and Systems I: Regular Papers,1558-0806,69,2,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9576112,"5G millimeter-wave (mmWave) communication systems enable exciting new applications by significantly reducing the latency and increasing the data rate. However, this comes at a large computational cost, which results in long latency and large energy consumption. In this work, we aim to address this challenge in the problem of channel estimation of such systems through a set of algorithm-hardware co-optimizations. First of all, we employed a model-based neural network to improve the rate of convergence. We also optimized the neural network and achieved improved loss while using approximately the same number of operations. Furthermore, we were able to reduce the computational complexity through the use of sparsity inherent in mmWave channels. The proposed neural network for the channel estimation scales the computational complexity by more than two orders. Based on these innovations, we implemented a channel estimation subsystem on Zynq 7020 FPGA. The subsystem obtains an improvement in latency of up to ~10X and an improvement in energy consumption of up to ~300X over CPU and GPU based systems.",,10.1109/TCSI.2021.3117886,"Channel estimation , Neural networks , 5G mobile communication , Hardware , Field programmable gate arrays , Transmitting antennas , Millimeter wave communication ",,,,,,,,,,
238,WITHDRAWN: News Development in the 5G Network Era based on Machine Learning and FPGA,"Wang, Wei",Microprocessors and Microsystems,0141-9331,,,103391,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120305482,"This article has been withdrawn at the request of the author(s) and/or editor. The Publisher apologizes for any inconvenience this may cause. The full Elsevier Policy on Article Withdrawal can be found at http://www.elsevier.com/locate/withdrawalpolicy. Subsequent to acceptance of this special issue paper by the responsible Guest Editor Sundhararajan Mahalingam, the integrity and rigor of the peer-review process was investigated and confirmed to fall beneath the high standards expected by Microprocessors & Microsystems. There are also indications that much of the Special Issue includes unoriginal and heavily paraphrased content. Due to a configuration error in the editorial system, unfortunately the Editor in Chief did not receive these papers for approval as per the journal’s standard workflow.",,https://doi.org/10.1016/j.micpro.2020.103391,Learning,,,,,,,,,,
239,FPGA-based Design and Optimization of a 5G-NR DU Receiver,"Fábio D. L. Coutinho , Hugerles S. Silva , Arnaldo S. R. Oliveira ",2021 Telecoms Conference (ConfTELE),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9435579,"In this paper, a Fifth-Generation New Radio (5G-NR) Distributed Unit (DU) receiver case study is carried out to evaluate the trade-offs between different design parameters. The 5G-NR DU receiver is modelled using a fast implementation flow, from the behavioral model to the Field-Programmable Gate Array (FPGA) validation. The goal of this paper is to optimize the area, power, and DU receiver overall performance of the Register-Transfer Level (RTL) implementations from a high-level model by varying the model input data type, i.e., the number of quantized bits at the input of the processing chain. Matlab and Simulink are used to implement the 5G-NR DU behavioral model, and its synthesis is performed by the Hardware Description Language (HDL) Coder automated tool. The 5G-NR DU receiver is implemented in a ZCU102 evaluation kit, containing an XCZU9EG-FFVB1156-2-E device. The trade-offs are evaluated by analyzing the Error Vector Magnitude (EVM), the coarse symbol timing detection, the resource utilization, the power, throughput, the maximum operating frequency, and the latency for different modulation schemes. The results showed a direct dependence of the input data type on these design parameters, while the modulation scheme is almost agnostic, providing reliable information for an optimized DU implementation.",,10.1109/ConfTELE50222.2021.9435579,"Frequency modulation , Software packages , Receivers , Throughput , New Radio , Data models , Timing ",,,,,,,,,,
240,Hierarchical Scheduling with FPGA-based Accelerator for Flexible 5G Mobile Networks,"Yuki Arikawa , Takeshi Sakamoto , Satoshi Shigematsu ",2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring),1090-3038,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9128593,"Towards the deployment of flexible 5G mobile networks, this paper presents a hierarchical coordinated radio-resource scheduling method along with a scheme for its practical hardware implementation in a field-programmable gate array (FPGA). The scheduler in the 5G era will have to quickly perform coordinated scheduling for a huge number of transmission antennas and support flexibility for advanced scheduling algorithms. To meet these requirements, we designed an FPGA-based scheduler that hierarchically controls the radio transmissions of all transmission antennas beyond a central unit. Experimental measurements reveal that the FPGA-based hierarchical coordinated scheduler shows comparable processing speed on the radio-frame time scale. Moreover, in numerical simulations, the overall system throughput with the hierarchical coordinated scheduler is 1.25 times higher than that obtained by the conventional method when the number of transmission antennas and mobile terminals are 32 and 256, respectively. Our proposed scheduling method will enable the deployment of flexible 5G mobile networks.",,10.1109/VTC2020-Spring48590.2020.9128593,"Processor scheduling , Throughput , Field programmable gate arrays , Scheduling , Interference , 5G mobile communication , Copper ",,,,,,,,,,
241,"FPGA demonstration of adaptive spacetime compression towards high-fidelity, low-latency 5G fronthaul","Paikun Zhu , Yuki Yoshida , Ken-ichi Kitayama ",45th European Conference on Optical Communication (ECOC 2019),,,,,2019,IET,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9125473,"We experimentally demonstrate 8-antenna MIMO fronthaul uplink enabled by the first FPGA-based adaptive space-time compressor. 48Gb/s CPRI-equivalent rate encapsulating 1024QAM 5G NR-grade signals is transported using as low as 2.5GBd optical PAM4, achieving <1.25% EVM and 50ns latency for compression.",,10.1049/cp.2019.0745,,,,,,,,,,,
242,A Survey of Machine Learning Algorithms for 6G Wireless Networks,"Patil, Anita and Iyer, Sridhar and P and ya, Rahul Jashvantbhai",,,,,,2022,,https://arxiv.org/abs/2203.08429,"The primary focus of Artificial Intelligence/Machine Learning (AI/ML) integration within the wireless technology is to reduce capital expenditures, optimize network performance, and build new revenue streams. Replacing traditional algorithms with deep learning AI techniques have dramatically reduced the power consumption and improved the system performance. Further, implementation of ML algorithms also enables the wireless network service providers to (i) offer high automation levels from distributed AI/ML architectures applicable at the network edge, (ii) implement application-based traffic steering across the access networks, (iii) enable dynamic network slicing for addressing different scenarios with varying quality of service requirements, and (iv) enable ubiquitous connectivity across the various 6G communication platforms. In this chapter, we review/survey the ML techniques which are applicable to the 6G wireless networks. and also list the open problems of research which require timely solutions.",,https://doi.org/10.48550/arXiv.2203.08429,Algorithms,,,,,,,,,,
243,AI-based computer vision using deep learning in 6G wireless networks,"Kamruzzaman, MM and Alruwaili, Omar",Computers and Electrical Engineering,0045-7906,102,,108233,2022,,https://www.sciencedirect.com/science/article/pii/S0045790622004694,"Modern businesses benefit significantly from advances in computer vision technology, one of the important sectors of artificially intelligent and computer science research. Advanced computer vision issues like image processing, object recognition, and biometric authentication can benefit from using deep learning methods. As smart devices and facilities advance rapidly, current networks such as 4 G and the forthcoming 5 G networks may not adapt to the rapidly increasing demand. Classification of images, object classification, and facial recognition software are some of the most difficult computer vision problems that can be solved using deep learning methods. As a new paradigm for 6Core network design and analysis, artificial intelligence (AI) has recently been used. Therefore, in this paper, the 6 G wireless network is used along with Deep Learning to solve the above challenges by introducing a new methodology named Optimizing Computer Vision with AI-enabled technology (OCV-AI). This research uses deep learning – efficiency algorithms (DL-EA) for computer vision to address the issues mentioned and improve the system's outcome. Therefore, deep learning 6 G proposed frameworks (Dl-6 G) are suggested in this paper to recognize pattern recognition and intelligent management systems and provide driven methodology planned to be provisioned automatically. For Advanced analytics wise, 6 G networks can summarize the significant areas for future research and potential solutions, including image enhancement, machine vision, and access control.",,https://doi.org/10.1016/j.compeleceng.2022.108233,6G;Wireless communication;AI;Machine learning;Deep learning;Mobile communication;Learning,,,,,,,,,,
244,A sustainable deep learning framework for fault detection in 6G Industry 4.0 heterogeneous data environments,"Mezair, Tinhinane and Djenouri, Youcef and Belhadi, Asma and Srivastava, Gautam and Lin, Jerry Chun-Wei",Computer Communications,0140-3664,187,,164-171,2022,,https://www.sciencedirect.com/science/article/pii/S014036642200055X,"The integration of 5G and Beyond 5G (B5G)/6G in Machine-to-Machine (M2M) communications, is making Industry 4.0 smarter. However, the goal of having a sustainable self-monitored industry has not been reached yet. State-of-the-art deep learning-based Fault Detection algorithms cannot handle heterogeneous data, meaning that more than one fault detection computational device has to be used for each data format, in addition to the inability to take advantage of the combination of all the information available in different formats to derive more accurate conclusions. Moreover, these algorithms rely on inefficient hyper-parameters tuning strategies. In this paper, we propose an Advanced Deep Learning framework for Fault Diagnosis in Industry 4.0 (ADL-FDI4), which combines Long Short Term Memory (LSTM), Convolutional Neural Networks (CNN) and graph CNN (GNN), to handle heterogeneous data. Furthermore, our novel framework uses a Branch-and-Bound procedure to guide the learning process. Our experimental results show that ADL-FDI4 outperforms the state-of-the-art solutions in terms of detection rate and running time, and for that, it consumes less energy. In addition to handling heterogeneous data, which implies that one computational device is sufficient to handle all data formats.",,https://doi.org/10.1016/j.comcom.2022.02.010,Industry 4.0;Deep Learning;Fault Detection;Machine to machine communication;Sustainable 6G,,,,,,,,,,
245,FPGA-accelerated SmartNIC for supporting 5G virtualized Radio Access Network,"Borromeo, Justine Cris and Kondepu, Koteswararao and Andriolli, Nicola and Valcarenghi, Luca",Computer Networks,1389-1286,210,,108931,2022,,https://www.sciencedirect.com/science/article/pii/S1389128622001189,"Disaggregated, virtualized, and open next-generation eNodeB (gNB) could bring several benefits to the Next Generation Radio Access Network (NG-RAN) by enabling more market competition and customer choice, lower equipment costs, and improved network performance. This can be achieved through gNB-central unit (CU)-control plane (CP), gNB-CU-user plane (UP) and gNB-distributed unit (DU) separation, CU and DU function virtualization, and zero touch RAN management and control. However, to achieve the performance required by specific foreseen 5G usage scenarios (e.g., Ultra Reliable Low Latency Communications — URLLC), offloading selected disaggregated gNB functions into an accelerated hardware becomes a necessity. To this aim, this study proposes the implementation of 5G DU Low-PHY layer functions into an FPGA-based SmartNIC exploiting the Open Computing Language (OpenCL) framework to facilitate the integration of accelerated 5G functions within the mobile protocol stack. The proposed implementation is compared against (i) a CPU-based OpenAirInterface implementation, and (ii) a GPU-based implementation of IFFT exploiting clfft and cufft libraries. Experimental results show that the different optimization techniques implemented in the proposed solution reduce the Low-PHY processing time and the use of FPGA resources. Moreover, the GPU-based implementation of the cufft and the proposed FPGA-based implementation have a lower processing time and power consumption compared to a CPU-based implementation for up to two cores. Finally, the implementation in a SmartNIC reduces the delay added by the host-to-device communication through the Peripheral Component Interconnect Express (PCIe) interface, considering both functional split options 2 and 7-1.",,https://doi.org/10.1016/j.comnet.2022.108931,Hardware Acceleration;Network function virtualization;OpenCL,,,,,,,,,,
246,GPU-Accelerated Partially Linear Multiuser Detection for 5G and Beyond URLLC Systems,"Matthias Mehlhose , Guillermo Marcus , Daniel Schäufele , Daniyal Amir Awan , Nikolaus Binder , Martin Kasparick , Renato L. G. Cavalcante , Sławomir Stañczak , Alexander Keller ",IEEE Access,2169-3536,10,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9810242,"We have implemented a recently proposed partially linear multiuser detection algorithm in reproducing kernel Hilbert spaces (RKHSs) on a GPU-accelerated platform. Our proof of concept combines the robustness of linear detection and non-linear detection for the non-orthogonal multiple access (NOMA) based massive connectivity scenario. Mastering the computation of the vast number of inner products (which involve kernel evaluations) is a challenge in ultra-low latency (ULL) applications due to the sub-millisecond latency requirement. To address the issue, we propose a massively parallel implementation of the detection of user data in a received orthogonal frequency-division multiplexing (OFDM) radio frame. The result is a GPU-accelerated real-time OFDM receiver that enables detection latency of less than one millisecond that complies with the requirements of 5th generation (5G) and beyond ultra-reliable and low latency communications (URLLC) systems. Moreover, the parallelization and acceleration techniques explored and demonstrated in this study can be extended to many signal processing algorithms in Hilbert spaces, such as projection onto convex sets (POCS) and adaptive projected subgradient method (APSM) based algorithms. Results and comparisons with the state-of-the-art confirm the effectiveness of our approach.",,10.1109/ACCESS.2022.3187040,"Signal processing algorithms , Kernel , Nonlinear filters , Multiuser detection , Maximum likelihood detection , Hilbert space , OFDM ",,,,,,,,,,
247,FPGA Design of an Efficient EEG Signal Transmission Through 5G Wireless Network Using Optimized Pilot Based Channel Estimation: A Telemedicine Application,"Kumar, K. B. Santhosh and Sujatha, B. R.",Wireless Personal Communications,1572-834X,123,4,3597-3621,2022,,https://doi.org/10.1007/s11277-021-09305-2,"Electroencephalogram (EEG) signifies a neurophysiologic measurement, which perceives the electrical activity of brain via making a record of EEG signal from the electrodes positioned on the scalp. With the progression of wired and wireless technologies both m-healthcare and e-healthcare turn into an essential fragment of biomedical science. Mixing of EEG signal with some other biological signal is referred as artifacts. Removal of artifacts postures an abundant challenge in the medical field. In this paper, Hybrid multi resolution discrete wavelet transform based delayed error normalized least mean square is proposed to eradicate motion artifact from the recorded EEG signal. After filtering process Encryption and Encoding take place with chaos encryption and Turbo encoder. Encryption is the process of scrambling the plain EEG signal in to Chipper format. Telemedicine system can be used to transmit medical data transmission and it requires an optimal channel estimation method (ESSA) to reduce BERs. The main aim of ESSA algorithm is to optimally place the pilot symbols and in-order to enable the automatic estimation of state of the channel. Channel estimation is facilitated through GFDM-IM modulation approach and the estimation can be done through SVD-LMMSE module. The proposed optimal based channel estimation is simulated under Xilinx platform with Verilog coding. Then, the performance of the proposed method will be analysed in terms of BER, area and frequency.",,10.1007/s11277-021-09305-2,Electroencephalography,,,,,,,,,,
248,WITHDRAWN: Modern accounting data analysis platform based on 5G network and FPGA,"Lin, Jianxiong",Microprocessors and Microsystems,0141-9331,,,103388,2020,Elsevier,https://www.sciencedirect.com/science/article/pii/S0141933120305457,"This article has been withdrawn: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/our-business/policies/article-withdrawal). This article has been withdrawn at the request of the Editor in Chief. Subsequent to acceptance of this special issue paper by the responsible Guest Editor Sundhararajan Mahalingam, the integrity and rigor of the peer-review process was investigated and confirmed to fall beneath the high standards expected by Microprocessors & Microsystems. There are also indications that much of the Special Issue includes unoriginal and heavily paraphrased content. Due to a configuration error in the editorial system, unfortunately the Editor in Chief did not receive these papers for approval as per the journal's standard workflow.",,https://doi.org/10.1016/j.micpro.2020.103388,Statistics as Topic,,,,,,,,,,
249,Fifth Generation (5G) New Radio (NR) Channel Codes Contenders Based on Field-Programmable Gate Arrays (FPGA): A Review Paper,"AA Hamad, MK Ibrahim, AA Al-hayder, SA Tamkeen",Journal of University of Babylon for Engineering Sciences,,27,3,76-83,2019,iasj.net,https://www.iasj.net/iasj/download/9d35d729771fae3a,"The increased demands for quality, like high throughput, low-latency, wide coverage, energy consumption, cost and reliable connections in mobile services, multimedia and data transmission impose the use of advance technical requirements for the next fifth-generation (5G) new radio (NR). One of the most crucial parts in the physical layer of the new generation is the error correction coding technique. Three schemes, namely; Turbo, low density parity check (LDPC), and polar codes are potentially considered as the candidate codes for both data and control channels. The competition is evaluated in terms of error correction capability, computational complexity, and flexibility. The parallelism, flexibility and high processing speed of Field-Programmable Gate Array (FPGA) make it preferable in prototyping and implementation of different codes. This paper presents a survey on the current literatures that deals with FPGA-based decoder design associated with the previously mentioned channel codes",,,"Fifth generation (5G), New radio (NR), LDPC, Turbo Code, Polar code, FPGA",,,,,,,,,,
250,Unmasking Concealed 5G Privacy Identity with Machine Learning and GPU in 12 mins,"Tea, Vui Huang",,,,,,2020,,https://www.techrxiv.org/articles/preprint/Unmasking_Concealed_5G_Privacy_Identity_with_Machine_Learning_and_GPU_in_12_mins/13187636,"The 3rd Generation Partnership Project (3GPP) standard for 5G telecommunications specifies privacy protection schemes to cryptographically encrypt and conceal permanent identifiers of subscribers to prevent them from being exposed and tracked by over-the-air eavesdroppers. However, conventional privacy-preserving protocols and architectures alone are insufficient to protect subscriber privacy as they are vulnerable to new types of attacks due to the utilization of the emerging technologies such artificial intelligence (AI). A conventional brute force attack to unmask concealed 5G identity using a CPU would require ~877 million years. This paper presents an apparatus using machine learning (ML) and a graphics processing unit (GPU) that is able to unmask a concealed 5G identity in ~12 minutes with an untrained neural-network, or ~0.015 milliseconds with a pre-trained neural-network. The 5G concealed identities are effectively identified without requiring decryption, hence severely diminishing the level of privacy-preservation. Finally, several ML defence countermeasures are proposed to re-establish privacy protection in 5G identity.",,10.36227/techrxiv.13187636.v1,5g;gpu;security;privacy;crypto;mobile phones;neural network;classification;machine learning;Artifical Intelligence;Privacy;Learning,,,,,,,,,,
251,Research on automatic evaluation method of Mandarin Chinese pronunciation based on 5G network and FPGA,"Wang, Zhongbo and Wu, Qi",Microprocessors and Microsystems,0141-9331,80,,103534,2021,,https://www.sciencedirect.com/science/article/pii/S0141933120306840,"In the automatic evaluation system, need to learn the standard mandarin of scoring method for teaching in native Chinese pronunciation. The most pronounced goal protocols focus on the context in which native speakers are unnatural. The new Hidden Markov Model (HMM) algorithm based on the traditional algorithm likely algorithm for Chinese syllables, whose final initial period is found in the area where evidence for the measurement of weight control has been found. Experiments have also shown that this algorithm is more effective than the traditional posterior recording algorithm of the Mandarin learning method. Force Hidden Markov Model- HMM Align alignment identification for each syllable and associated recording probability for speech evaluation via race-based reliability system applications. These processes could then be formalized as a linear combination after the overall assessment functions: phonics, tone, intensity, and rhythm. Because both linear and non-linear parameters are involved in the overall evaluation functions. Incorporates variation in pronunciation to generate structure through a novel approach that incorporates tons of sub-tones that represent the missing automatic sound models. The word level assessment achieved through the pronunciation is similar to that which in the future showed the singing ability being realized by the evaluation system in full-length pronunciation as a method.",,https://doi.org/10.1016/j.micpro.2020.103534,Chinese pronunciation;Hidden Markov Model-HMM;Mandarin learning;Pronunciation,,,,,,,,,,
252,FPGA implementation of new LM-SPIHT colored image compression with reduced complexity and low memory requirement compatible for 5G,"YM Tabra, B Sabbar",International Journal of Reconfigurable and Embedded Systems,,8,,,2019,IAES,https://ijres.iaescore.com/index.php/IJRES/article/view/17889,"The revolution in 5G mobile systems require changes to how image is handled. These changes are represented by the required processing time, the amount of space for uploading and downloading. In this paper, a development on WT (Wavelet Transform) along with LM-SPIHT (Listless-Modified Set Partitioning in Hierarchical tree) coding and with additional level of Runlength encoding for image compression has been proposed. The new implementation reduces the amount of data needed to be stored in several stages, also the amount of time required for processing. The compression has been implemented using VHDL (Very High Descriptive Language) on netFPGA-1G-CLM Kintex-7 board. The new implementation results show a reduction in the complexity as processing time.",, http://doi.org/10.11591/ijres.v8.i1.pp1-13 ,5G; Runlength; SPIHT; VHDL; Wavelet; Transform.,,,,,,,,,,
253,Build and deploy GPU-accelerated 5G virtual Radio Access Networks (vRAN),N Aerial,,,,,,2022,Accessed: Jun,,,,,,,,,,,,,,,
254,GPF: A GPU-Based Design to Achieve Scheduling for 5G NR,"Y Huang, S Li, YT Hou, W Lou",Proc. MobiCom 2018,,,,,2019,,,,,,,,,,,,,,,,
255,A Cognitive Radio Spectrum Sensing Implementation Based on Deep Learning and Real Signals,"Saber, Mohamed and Chehri, Abdellah and Rharras, Abdessamad El and Saadane, Rachid and Wahbi, Mohammed",,978-3-030-66840-2,,,930-941,2021,Springer International Publishing,https://link.springer.com/chapter/10.1007/978-3-030-66840-2_70,"In a cognitive radio environment, spectrum sensing is an essential phase for improving spectrum resources management. Based on a deep learning method and real signals, a new spectrum sensing implementation is proposed in this work. The real signals are artificially generated, using an ARDUINO UNO card and a 433 MHz wireless transmitter, in ASK and FSK modulation types. The reception interface is constructed using an RTL-SDR receiver connected to MATLAB software. The signals classification is carried out by a convolutional neural network (CNN) classifier. Our proposed model’s main objective is to identify the spectrum state (free or occupied) by classifying the received signals into a licensed user (primary user) signals or noise signals. Our proposed model’s performance evaluation is evaluated by two metrics: the probability of detection (Pd) and the false alarm probability (PFA). Finally, the proposed sensing method is compared with other used techniques for signal classification, such as energy detection, artificial neural network, and support vector machine. The experimental results show that CNN could classify the real signals better than traditional methods and machine learning methods.",,,Learning;Cognition,,,,,,,,,,
256,6G and Artificial Intelligence Technologies for Dementia Care: Literature Review and Practical Analysis,"Su, Zhaohui and Bentley, Barry L and McDonnell, Dean and Ahmad, Junaid and He, Jiguang and Shi, Feng and Takeuchi, Kazuaki and Cheshmehzangi, Ali and da Veiga, Claudimar Pereira",J Med Internet Res,1438-8871,24,4,e30503,2022,,http://www.ncbi.nlm.nih.gov/pubmed/35475733,"Background: The dementia epidemic is progressing fast. As the world's older population keeps skyrocketing, the traditional incompetent, time-consuming, and laborious interventions are becoming increasingly insufficient to address dementia patients' health care needs. This is particularly true amid COVID-19. Instead, efficient, cost-effective, and technology-based strategies, such as sixth-generation communication solutions (6G) and artificial intelligence (AI)-empowered health solutions, might be the key to successfully managing the dementia epidemic until a cure becomes available. However, while 6G and AI technologies hold great promise, no research has examined how 6G and AI applications can effectively and efficiently address dementia patients' health care needs and improve their quality of life. Objective: This study aims to investigate ways in which 6G and AI technologies could elevate dementia care to address this study gap. Methods: A literature review was conducted in databases such as PubMed, Scopus, and PsycINFO. The search focused on three themes: dementia, 6G, and AI technologies. The initial search was conducted on April 25, 2021, complemented by relevant articles identified via a follow-up search on November 11, 2021, and Google Scholar alerts. Results: The findings of the study were analyzed in terms of the interplay between people with dementia's unique health challenges and the promising capabilities of health technologies, with in-depth and comprehensive analyses of advanced technology-based solutions that could address key dementia care needs, ranging from impairments in memory (eg, Egocentric Live 4D Perception), speech (eg, Project Relate), motor (eg, Avatar Robot Caf{\'e}), cognitive (eg, Affectiva), to social interactions (eg, social robots). Conclusions: To live is to grow old. Yet dementia is neither a proper way to live nor a natural aging process. By identifying advanced health solutions powered by 6G and AI opportunities, our study sheds light on the imperative of leveraging the potential of advanced technologies to elevate dementia patients' will to live, enrich their daily activities, and help them engage in societies across shapes and forms.",,10.2196/30503,COVID-19; 6G; digital health; artificial intelligence; dementia; first-perspective health solutions;Dementia;Benchmarking;Intelligence,,,,,,,,,,
257,Real-Time GPU-Accelerated Machine Learning Based Multiuser Detection for 5G and Beyond,"M Mehlhose, D Schäufele, DA Awan, G Marcus, ...",arXiv preprint arXiv …,,,,,2022,arxiv.org,https://arxiv.org/abs/2201.05024,"In this feasibility study, we have implemented a recently proposed partially linear multiuser detection algorithm in reproducing kernel Hilbert spaces (RKHSs) on a GPU-accelerated platform. Partially linear multiuser detection, which combines the robustness of linear detection with the power of nonlinear methods, has been proposed for a massive connectivity scenario with the non-orthogonal multiple access (NOMA). This is a promising approach, but detecting payloads within a received orthogonal frequency division multiplexing (OFDM) radio frame requires the execution of a large number of inner product operations, which are the main computational burden of the algorithm. Although inner-product operations consist of simple kernel evaluations, their vast number poses a challenge in ultra-low latency (ULL) applications, because the time needed for computing the inner products might exceed the sub-millisecond latency requirement. To address this problem, this study demonstrates the acceleration of the inner-product operations through massive parallelization. The result is a GPU-accelerated real-time OFDM receiver that enables sub-millisecond latency detection to meet the requirements of 5th generation (5G) and beyond ultra-reliable and low latency communications (URLLC) systems. Moreover, the parallelization and acceleration techniques explored and demonstrated in this study can be extended to many other signal processing algorithms in Hilbert spaces, such as those based on projection onto convex sets (POCS) and adaptive projected subgradient method (APSM) algorithms. Experimental results and comparisons with the state-of-art confirm the effectiveness of our techniques.",,,,,,,,,,,,,
258,Slow-Envelope Shaping Function FPGA Implementation for 5G NR Envelope Tracking PA,"Wantao Li , Nikolaos Bartzoudis , José Rubio Fernández , David López-Bueno , Gabriel Montoro , Pere Gilabert ",2022 International Workshop on Integrated Nonlinear Microwave and Millimetre-Wave Circuits (INMMiC),2689-548X,,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9762194,"This paper focuses on the FPGA implementation of a slew-rate reduction (SR) shaping function for envelope tracking (ET) power amplifiers (PAs). The SR envelope has been proved effective to trade-off power efficiency and linearity in ET PA systems where the envelope tracking modulator (ETM) is bandwidth limited. However, the implementation issues need to be addressed when targeting high clock rates to cope with current 5G new radio wide-band signals. This paper shows the FPGA implementation of the SR envelope generation. We explore the use of high-level synthesis (HLS) for the SR envelope generation to evaluate the performance and resource usage of the hardware architecture. The HLS design is also compared with a hand-written hardware description language (HDL) version. An in-depth analysis shows strengths and limitations of the HLS design to meet the timing constraints when considering a throughput of 614.4 MSa/s.",,10.1109/INMMiC54248.2022.9762194,"Target tracking , 5G mobile communication , Prototypes , Power amplifiers , Modulation , Throughput , Reliability engineering ",,,,,,,,,,
259,A GPU accelerated framework for monitoring LTE/5G interference to DVB-T systems,"Federica Mangiatordi , Emiliano Pallotti ",2021 AEIT International Annual Conference (AEIT),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9626937,"This work presents a new computation system to evaluate the interference maps between the radio mobile signal and the DVB-T signal based on a multiprocessor computing architecture, such as GPU systems (Graphics Processing Units) designed for high-performance parallel computing. The goal is to overcome the time complexity of current approaches based on sequential implementations on multicores CPUs that require several hours to generate and update the interference maps of the Italian territory. The new system’s design considers new parallel computing methodologies to consider rapid changes in the GPU hardware and uses a high-performance distributed SQL engine on RAPIDs to create an efficient and scalable framework for processing a massive volume of simulation data radio. Carrying out test on the radio-electric data of the Italian provinces with the most significant territorial extension shows compression of at least a factor of 30 in the execution times, which for the entire Italian territory fall below 20 minutes.",,10.23919/AEIT53387.2021.9626937,"Solid modeling , Parallel programming , Memory management , Graphics processing units , Interference , Parallel processing , Servers ",,,,,,,,,,
260,FPGA-Based Ordered Statistic Decoding Architecture for B5G/6G URLLC IIOT Networks,"Changhyeon Kim , Dongyoung Rim , Jeongwon Choe , Dongyun Kam , Giyoon Park , Seokki Kim , Youngjoo Lee ",2021 IEEE Asian Solid-State Circuits Conference (A-SSCC),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9634714,"The ordered statistic decoding (OSD) approach for short-length BCH codes has been continuously considered as one of the promising error-correction codes by achieving a block error rate (BLER) of less than $10^{-6}$, which is attractive to the ultra-reliable and low-latency communication (URLLC) for industrial IoT (IIOT) solutions [1], [2]. However, it is hard to directly realize the conventional OSD algorithm because of the compute-intensive Gaussian elimination and iterative reprocessing steps. Based on the recent segmentation discarding decoding (SDD) approach [3], in this work, we newly present an ultralow-latency OSD architecture reducing the decoding latency by 12 times, which is implemented at an FPGA-based verification platform.",,10.1109/A-SSCC53895.2021.9634714,"Error analysis , Conferences , Computer architecture , Ultra reliable low latency communication , Iterative algorithms , Solid state circuits , Error correction codes ",,,,,,,,,,
261,FPGA Implementation of a Wideband Multi-Gb/s 5G BF-OFDM Transceiver,"Jean-Baptiste Doré , Marc Laugeois , Nicolas Cassiau , Xavier Popon ",2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),2475-6490,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9482424,"This paper describes a Field Programmable Gate Array (FPGA) implementation of a multi-Gb/s Block Filtered (BF) OFDM transceiver, fully 5G NR compatible. The main obstacles for such a work are (i) the support of multiple configurations and parameters, (ii) the high bandwidth w.r.t the board clock frequency and (iii) the intrinsic complexity of BF-OFDM. We prove that despite these barriers an hardware implementation of this waveform is possible, even with a bandwidth up to 400 MHz. We based our developments on the following pillars: smart layout of the basic modules, parallelization of dedicated functions design and ad hoc architecture. Measurements and complexity analysis demonstrate the high flexibility of BF-OFDM.",,10.1109/EuCNC/6GSummit51104.2021.9482424,"5G mobile communication , Transmitters , OFDM , Filter banks , Transceivers , Hardware , Complexity theory ",,,,,,,,,,
262,Optimizing 5G VPN+ Transport Networks with Vector Packet Processing and FPGA Cryptographic Offloading,"Dzogovic, Bruno and Santos, Bernardo and Feng, Boning and Do, Van Thuan and Jacot, Niels and Van Do, Thanh",Mobile Web ,978-3-030-83164-6,,,85-98,2021,Springer International Publishing,https://link.springer.com/chapter/10.1007/978-3-030-83164-6_7,"Network slicing is the crucial prerogative that allows end users and industries to thrive from 5G infrastructures, however, such a logical network component can deteriorate from security vulnerabilities that prevail within cloud environments and datacenters. The Quality of Experience in 5G is a metric that takes into consideration sets of factors, which play role in the definition of the end-to-end performance, which is indeed latency, packet processing, utilization of legacy protocols, old hardware, encryption, non-optimized network topologies, routing problems and multitude of other aspects. This research sheds light on the inherent networking stack performance issues that translate into 5G environments, in a use-case where encrypted VPN tunneling is used to secure the backhaul transport network between the 4G/5G cores and the frontend networks.",,,,,,,,,,,,,
263,FPGA Implementation of a 4G/5G Multimode DU Downlink Transmission Chain,"José D. Domingues , Hugerles S. Silva , Arnaldo S. R. Oliveira ",2021 Telecoms Conference (ConfTELE),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9435553,"This paper presents a multimode Distributed Unit (DU) transmission chain implemented in Field-Programmable Gate Array (FPGA). A careful analysis is taken towards the physical layer differences between Fourth Generation Long-Term Evolution (4G-LTE) and the Fifth Generation New Radio (5G-NR) of mobile networks to determine the fundamental changes in each generation's DU architecture. The DU supports both 4G-LTE and 5G-NR to be modulated parallely in real-time, and is developed in this work by using high level tools and Register-Transfer Level (RTL) code, from Matlab and Simulink to VHDL, to obtain the final implementation of a single DU. On the 4G side, the DU supports the maximum LTE channel bandwidth of 20 MHz, and the 5G counterpart supports a channel bandwidth from 5 MHz to 100 MHz. The results are validated in Matlab, Simulink, RTL and in real-time, culminating in a final FPGA implementation, with an EVM of 0.24% for 4G and 1.60% for 5G.",,10.1109/ConfTELE50222.2021.9435553,"VHDL , Software packages , Bandwidth , New Radio , Physical layer , Downlink , Real-time systems ",,,,,,,,,,
264,Centralized Single FPGA Real Time Zero Forcing Massive MIMO 5G Basestation Hardware and Gateware,"Andreas Benzin , Dennis Osterland , Maksim Dill , Giuseppe Caire ",2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),1948-3244,,,,2020,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9154248,In the following a massive MIMO 5G <; 6 GHz base station implementation is presented which is capable of realtime zero forcing precoding on a single central signal processing (CSP) FPGA. The built prototype is capable of simultaneously driving M = 196 separate RF ports all delivering samples to the CSP FPGA. Each RF chain's ADCs and DACs are running at 40 MSPS at full roll-out. The power consumption of the remote radio head is 1.56 W per RF port when running at a sample rate of 15.36 MSPS. The system allows for hardware-in-the-loop operation and real-time baseband signal processing with a round trip delay of 278 μs when processing 64 antennas and 8 simultaneous user streams for an 5GNR-like OFDMA system with 1024 sub-carriers and 50 resource blocks (600 used subcarriers) with a sample frequency of 15.36 MHz and a central signal processing clock of 184.32 MHz. The reciprocity calibration system runs completely internal to the system and doesn't radiate signals for the calibration procedure. Furthermore the central single-FPGA signal processing architecture allows for simplified implementation of algorithms and maintenance of the system.,,10.1109/SPAWC48557.2020.9154248,"Field programmable gate arrays , Radio frequency , Calibration , Clocks , Signal processing , Real-time systems , Hardware ",,,,,,,,,,
265,Real-time multi-GPU-based 8KVR stitching and streaming on 5G MEC/Cloud environments,"Lee, HeeKyung and Um, Gi-Mun and Lim, Seong Yong and Seo, Jeongil and Gwak, Moonsung",ETRI Journal,1225-6463,44,1,62-72,2022,"John Wiley & Sons, Ltd",https://doi.org/10.4218/etrij.2021-0210,"Abstract In this study, we propose a multi-GPU-based 8KVR stitching system that operates in real time on both local and cloud machine environments. The proposed system first obtains multiple 4 K video inputs, decodes them, and generates a stitched 8KVR video stream in real time. The generated 8KVR video stream can be downloaded and rendered omnidirectionally in player apps on smartphones, tablets, and head-mounted displays. To speed up processing, we adopt group-of-pictures-based distributed decoding/encoding and buffering with the NV12 format, along with multi-GPU-based parallel processing. Furthermore, we develop several algorithms such as equirectangular projection-based color correction, real-time CG overlay, and object motion-based seam estimation and correction, to improve the stitching quality. From experiments in both local and cloud machine environments, we confirm the feasibility of the proposed 8KVR stitching system with stitching speed of up to 83.7 fps for six-channel and 62.7 fps for eight-channel inputs. In addition, in an 8KVR live streaming test on the 5G MEC/cloud, the proposed system achieves stable performances with 8 K@30?fps in both indoor and outdoor environments, even during motion.",https://doi.org/10.4218/etrij.2021-0210,https://doi.org/10.4218/etrij.2021-0210,5G;8KVR;cloud;live streaming;smartphone camera,,,,,,,,,,
266,FPGA-based radio-resource scheduler for 5G mobile in NFV environments,"Arikawa, Yuki and Sakamoto, Takeshi and Shigematsu, Satoshi",IEICE Communications Express,,8,7,263-268,2019,,,"A coordinated radio-resource scheduler with an FPGA-based hardware accelerator is a key component for 5G mobile systems in NFV environments. This paper analyses the scheduling process and addresses ways to reduce the overhead of memory copy operation between a central unit and the accelerator. The experimental results show that the overhead is reduced to approximately 14% when the accelerator is connected with a central unit via PCIe with high-bandwidth memory copy technique. Moreover, they indicate that the accelerator tightly coupled with central units via the Ethernet is also a possible approach for coordinated scheduling among multiple central units. This will be advantageous in implementing future NFV-based mobile communications systems.",,10.1587/comex.2019XBL0048,,,,,,,,,,,
267,Development of an FPGA-based High-Speed Wireless Communication System in the 60GHz Frequency Band For CERN facilities and 5G deployment,"Jaoua, Mohamed",,,,,,2018,,https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1203832&dswid=8496,,,,,,,,,,,,,,
268,FPGA Implementation of 5G NR Primary and Secondary Synchronization,"AR Kumar, KL Kishore","Computers, Materials & Continua",,,,,2022,TECH SCIENCE PRESS 871 …,https://www.techscience.com/cmc/v73n1/47754,"The 5G communication systems are widely established for high-speed data processing to meet users demands. The 5G New Radio (NR) communications comprise a network of ultra-low latency, high processing speeds, high throughput and rapid synchronization with a time frame of 10 ms. Synchronization between User Equipment (UE) and 5G base station known as gNB is a fundamental procedure in a cellular system and it is performed by a synchronization signal. In 5G NR system, Primary Synchronization Signal (PSS) and Secondary Synchronization Signal (SSS) are used to detect the best serving base station with the help of a cell search procedure. The paper aims to determine the Physical Cell Identity (PCI) by using primary synchronization and secondary synchronization blocks. The PSS and SSS detection for finding PCI is implemented on Zynq-7000 series Field Programmable Gate Arrays (FPGA) board. FPGA are reconfigurable devices and easy to design complex circuits at high frequencies. The proposed architecture employs Primary Synchronization Signal (PSS) and Secondary Synchronization Signal (SSS) detection aims with high speed and low power consumption. The synchronization blocks have been designed and the synthesized design block is implemented on the Zynq-7000 series Zed board with a maximum operating clock frequency of 1 GHz.",,https://doi.org/10.32604/cmc.2022.021573,5G new radio; FPGA; physical cell identity; primary and secondary synchronization,,,,,,,,,,
269,FPGA Acceleration of 3GPP Channel Model Emulator for 5G New Radio,"Nasir Ali Shah , Mihai T. Lazarescu , Roberto Quasso , Salvatore Scarpina , Luciano Lavagno ",IEEE Access,2169-3536,10,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9944632,"The channel model is by far the most computing intensive part of the link level simulations of multiple-input and multiple-output (MIMO) fifth-generation new radio (5GNR) communication systems. Simulation effort further increases when using more realistic geometry-based channel models, such as the three-dimensional spatial channel model (3DSCM). Channel emulation is used for functional and performance verification of such models in the network planning phase. These models use multiple finite impulse response (FIR) filters and have a very high degree of parallelism which can be exploited for accelerated execution on Field Programmable Gate Array (FPGA) and Graphics Processing Unit (GPU) platforms. This paper proposes an efficient re-configurable implementation of the 3rd generation partnership project (3GPP) 3DSCM on FPGAs using a design flow based on high-level synthesis (HLS). It studies the effect of various HLS optimization techniques on the total latency and hardware resource utilization on Xilinx Alveo U280 and Intel Arria 10GX 1150 high-performance FPGAs, using in both cases the commercial HLS tools of the producer. The channel model accuracy is preserved using double precision floating point arithmetic. This work analyzes in detail the effort to target the FPGA platforms using HLS tools, both in terms of common parallelization effort (shared by both FPGAs), and in terms of platform-specific effort, different for Xilinx and Intel FPGAs. Compared to the baseline general-purpose central processing unit (CPU) implementation, the achieved speedups are 65X and 95X using the Xilinx UltraScale+ and Intel Arria FPGA platform respectively, when using a Double Data Rate (DDR) memory interface. The FPGA-based designs also achieved ~3X better performance compared to a similar technology node NVIDIA GeForce GTX 1070 GPU, while consuming ~4X less energy. The FPGA implementation speedup improves up to 173X over the CPU baseline when using the Xilinx UltraRAM (URAM) and High-Bandwidth Memory (HBM) resources, also achieving 6X lower latency and 12X lower energy consumption than the GPU implementation.",,10.1109/ACCESS.2022.3221124,"Channel models , Field programmable gate arrays , Computational modeling , 3GPP , Graphics processing units , Three-dimensional displays , Optimization , 5G mobile communication , Hardware acceleration ",,,,,,,,,,
270,Architectural Implementation of AES based 5G Security Protocol on FPGA,"Usva Rahim , Muhammad Faisal Siddiqui , Muhammad Awais Javed , Nazmus Nafi ",2022 32nd International Telecommunication Networks and Applications Conference (ITNAC),2474-1531,,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9998367,"Confidentiality and integrity security are the key challenges in future 5G networks. To encounter these challenges, various signature and key agreement protocols are being implemented in 5G systems to secure high-speed mobile-to-mobile communication. Many security ciphers such as SNOW 3G, Advanced Encryption Standard (AES), and ZUC are used for 5G security. Among these protocols, the AES algorithm has been shown to achieve higher hardware efficiency and throughput in the literature. In this paper, we implement the AES algorithm on Field Programmable Gate Array (FPGA) and real-time performance factors of the AES algorithm were exploited to best fit the needs and requirements of 5G. In addition, several modifications such as partial pipelining and deep pipelining (partial pipelining with sub-module pipelining) are implemented on Virtex 6 FPGA ML60S board to improve the throughput of the proposed design.",,10.1109/ITNAC55475.2022.9998367,"Protocols , 5G mobile communication , Snow , Simulation , Throughput , Real-time systems , Security ",,,,,,,,,,
271,Spark Distributed Real-Time Data and GPU Parallel Computing Based on 5G Virtual Reality,"Ying Chang , Dajun Chang , Li Li , Zhangquan Qiao ",IEEE Consumer Electronics Magazine,2162-2256,PP,99,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9735325,"5G virtual reality has attracted many manufacturers and users with its unique immersion, interactivity and imagination characteristics, which has become the focus of new markets. The purpose of this article is to use Spark distributed real-time data system and GPU parallel computing to quickly process and analyze data. This article mainly designs a general-purpose real-time data analysis and processing system based on Spark, which mainly includes new ETL and real-time processing engine modules, and is committed to achieving higher real-time performance than traditional Hadoop. And realize fast calculation. At the same time there is universality and stability. Includes real-time flow calculations. Fast batch processing and machine learning The various types of data computers are included in this article by preparing the cutting device and adjusting the cutting output. The device is ready to effectively terminate the CUDA environment. The cudamalloc function is used to allocate a linear space of bytes to the device, and then transfer the data from the host to the device to determine the number of GPU blocks and threads. GPU parallel computing can increase the data processing speed by 27%, while the secondary programming algorithm can reduce the optimization time of the cup by 12%.",,10.1109/MCE.2022.3159349,"Peer-to-peer computing , Real-time systems , Graphics processing units , Quadratic programming , Optimization , Sparks , Information entropy ",,,,,,,,,,
272,FPGA Implementation of FBMC Transceiver for 5G Technologies,Vignesh Kumar Singh ,2022 International Conference on Computer Communication and Informatics (ICCCI),2329-7190,,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9741009,"Filter bank Multicarrier (FBMC) is a clever procedure developed from OFDM which settle the vast majority of these issues by adopting a separating strategy to multicarrier correspondence framework. FBMC signs can undoubtedly meet the Adjacent Channel Leakage Ratio (ACLR) and they don't utilize cyclic prefix in this manner works on unearthly effectiveness. Because of the incorporation of band-restricted heartbeat molding channels into the sign model in FBMC procedure, the plan of effective handset designs for multicarrier frameworks turns into a difficult errand. An efficient FBMC hardware architecture is designed and implementation is presented. It includes both transmitter and receiver. The design uses pipelined 8-point IFFT/FFT and polyphase filter (PP-F) for the processing at transmitter and receiver module respectively, which indicate that the processing block contain 8 inputs data. It also includes serial to parallel and parallel to serial converter module. Finally, comparative performance analysis of proposed FBMC system over OFDM systems is performed in terms of Signal-to-noise ratio (SNR), Bit error rate (BER), Latency and Throughput. The Results show that proposed FBMC transceiver is better in terms of performance, hardware complexity overhead and power consumption as compared to OFDM system.",,10.1109/ICCCI54379.2022.9741009,"Transmitters , OFDM , Bit error rate , Filter banks , Receivers , Throughput , Telephone sets ",,,,,,,,,,
273,A Hardware/Software Co-Design Approach to Prototype 6G Mobile Applications inside the GNU Radio SDR Ecosystem Using FPGA Hardware Accelerators - International Symposium on Highly-Efficient Accelerators and Reconfigurable Technologies,"Karle, Christian Maximilian and Kreutzer, Marius and Pfau, Johannes and Becker, J\""{u}rgen",International Symposium on ,,,,33–41,2022,Association for Computing Machinery,https://doi.org/10.1145/3535044.3535049,"The novel communication 6G requires raw data rates of up to 400 Gbit s− 1 in a single Field Programmable Gate Array (FPGA) front-end. For these high data rates, a Software Defined Radio (SDR) on a multi-core processor reaches a performance limit due to technical boundaries, clock speeds and overhead in multiprocessing. Hence, additional hardware accelerators based on Application Specific Integrated Circuits (ASICs) or FPGAs are required to process those large amounts of data. In this paper, our hardware/software co-design concept is presented, which allows for the integration of hardware accelerators realized in FPGAs, to improve overall system performance. We can access the accelerators from the widely used GNU Radio, which also allows non-FPGA user to benefit from the acceleration. For this purpose, a concept to share data between Processing System (PS) and Programmable Logic (PL) is developed and evaluated regarding its suitability for high bandwidth applications. Using a common application for evaluation, we are able to compare a hardware/software co-design with an entirely software-based implementation. Resulting in an overall performance increase by a factor of up to 4.5, while serving as a foundation for an easy-to-use method for hardware accelerators in GNU Radio.",,10.1145/3535044.3535049,Hardware/Software Co-Design;GNU Radio;Hardware Accelerator;FPGA;SDR;PYNQ,,,,,,,,,,
274,5G Security: FPGA Implementation of SNOW-V Stream Cipher,"Lampros Pyrgas , Paris Kitsos ",2021 24th Euromicro Conference on Digital System Design (DSD),,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9556387,"In this paper, a very compact architecture the newest member of the SNOW family of stream ciphers, called SNOW-V, is presented. The proposed architecture has a 128-bit datapath and is pipelined in key areas in order to achieve the maximum possible frequency while using only a small number of hardware resources. The design was coded using the VERILOG hardware description language and the BASYS3 board (Artix 7 XC7A35T) was the target of the hardware implementation. The proposed implementation utilizes only 2109 FPGA LUTs and 1352 FFs and reaches a data throughput of 2.6 Gbps at 224 MHz clock frequency.",,10.1109/DSD53832.2021.00064,"Ciphers , 5G mobile communication , Snow , Throughput , Hardware , Table lookup , Security ",,,,,,,,,,
275,Machine Learning Enhanced CPU-GPU Simulation Platform for 5G System,"Ouyang, Yuling and Yin, Caiyuan and Zhou, Ting and Jin, Yan","11th EAI International Conference, MONAMI 2021, Virtual Event",978-3-030-94763-7,,,25-38,2022,Springer International Publishing,https://link.springer.com/chapter/10.1007/978-3-030-94763-7_3,"The exponential growth of mobile terminals and the explosion of data volume are promoting the continuous evolution of mobile communication network and also increasing the complexity of the system. Meanwhile, 5G system-level simulation also requires more complex operations and more data processing. Conventional system simulation platform based on CPU can not satisfy the computing power requirement of system-level simulation of 5G. For tremendously shorten the execution time, we proposed to develop the CPU-GPU based parallelization platform, which adopts Logistic Regression algorithm to optimizing the use of computational resources. Numerical results demonstrate the effectiveness in terms of reducing execution time and guaranteeing reliability of system-level simulation result in 5G scenarios.",,,Learning,,,,,,,,,,
276,Aerial: A GPU Hyper-Converged Platform for 5G - Proceedings of the SIGCOMM '21 Poster and Demo Sessions,"Kelkar, Anupa and Dick, Chris",Proceedings of the SIGCOMM'21 Poster and Demo Sessions,,,,79–81,2021,Association for Computing Machinery,https://doi.org/10.1145/3472716.3472864,"In addition to high-throughput, low-latency and high-reliability, softwarization, virtualization and open interfaces are key tenets of 5G wireless. In this paper we present Aerial, a pure software implementation of a MIMO multi-cell gNB. All of the baseband signal processing is implemented as CUDA running on an NVIDA GPU, layer-2 is hosted on a CPU and an O-RAN 7.2 split fronthaul interface is supported with a Mellanox ConnectX-6 DX NIC. The accompanying video demonstrates the three use-cases that are described in this paper.",,10.1145/3472716.3472864,artificial intelligence and machine learning;AR/VR and computer vision;edge data center vRAN;GPU wireless signal processing;mobile edge computing;robotics;5G and 6G research,,,,,,,,,,
277,FPGA-based Implementation and Evaluation of Realtime OFDM Phase Compensation in 5G,"Hiep Nguyen , Sang Nguyen ",2021 International Conference on Advanced Technologies for Communications (ATC),2162-1020,,,,2021,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9598312,"In the New Radio (NR) standard which is standardized by 3GPP as a candidate for 5G mobile communication system, Orthogonal Frequency Division Multiplexing (OFDM) has been selected as the waveform for the air interface. Unlike the 4G Long Term Evolution (LTE), where the carrier frequency between the transmitter and receiver are always at the same locations, in NR, they can be at different frequencies. In such case, this leads to the phase ramped up at the receiver which cannot be recovered if the OFDM symbol is not equipped with reference signal for channel estimation and equalization. In this paper, we present our model for this issue and how to eliminate it by compensating the phase difference at both transmitter and receiver. We also implement this approach in Field Programmable Gate Array (FPGA) and validate it in real hardware testbed. The result show that with phase compensation, the received signal constellation is significantly improved.",,10.1109/ATC52653.2021.9598312,"Constellation diagram , OFDM , Radio transmitters , Receivers , Hardware , Generators , Frequency division multiplexing ",,,,,,,,,,
278,FPGA implementation of polar codes for 5G eMBB control channels,"Aparna, G. and Swathi, Raparla and Joseph, M. Kezia and Sujatha, C.N. and Naik, B. Rajendra",International Journal of Ultra Wideband Communications and Systems,,4,3,170-181,2021,,https://www.inderscienceonline.com/doi/abs/10.1504/IJUWBCS.2021.119139,"The rapid growth in wireless communications and information technology over a decade is demanding advanced wireless technologies development. Design and implementation of channel codes for different code lengths (N) and varying code rates (R) to handle the latency and the hardware requirements for enhanced mobile broad band (eMBB) control channel standards like 5G is a challenging issue at present. Polar codes are treated as a solution to the above defined design aspects to fulfil the requirements by the 3rd Generation Partnership Project (3GPP). Therefore, a VLSI architecture for polar codes implementation using SCLD method based on LLR approach is designed and simulated using VHDL programming, ISE Navigator 14.2, HDL synthesiser on the target FPGA device XC6vlx760-1-ffl760 for analysing various code lengths N [128, 256, 512] as inputs and code rates R [1/6, 1/3, 1/2] in this paper.",,10.1504/IJUWBCS.2021.119139,,,,,,,,,,,
279,Design of a Real-Time DSP Engine on RF-SoC FPGA for 5G Networks,"Kitsakis, Vasileios and Kanta, Konstantina and Stratakos, Ioannis and Giannoulis, Giannis and Apostolopoulos, Dimitrios and Lentaris, George and Avramopoulos, Hercules and Soudris, Dimitrios and Reisis, Dionysios I.",Optical network design and planning,978-3-030-38085-4,,,540-551,2020,Springer International Publishing,https://link.springer.com/chapter/10.1007/978-3-030-38085-4_46,"5G advances the wireless communications by providing a significant improvement to the data rate, capability of connected devices and data volumes compared to the previous generations. While these advantages combine along with a wider range of applications to merit the end-user, the technologies to be used are not specified. Considering this problem and in order to efficiently support the 5G deployment researchers and engineers turned their attention on FPGA base band architectures that keep the implementation cost relatively low and at the same time they are reprogramable to provide solutions to the emerging requirements and their consequent modifications. Aiming at the contribution to the 5G technologies the current paper introduces the design of a base band DSP architecture that targets the required real time performance. Moreover, the proposed architecture is scalable by efficiently parallelizing and/or pipelining the corresponding data paths. The paper presents the pilot FPGA designs of the IFFT/FFT and Sampling Frequency Offset (SFO) functions that achieve a 500 Msps performance on a RF-SoC Xilinx ZCU111 board.",,,,,,,,,,,,,
280,Implementation of Decimator Filter in 5G system for Area and Power Optimization Using FPGA ,"Christopher, Maurice Eric and D, Bhoomika and M L, Bhoomika and S, Darshan and C, Hema","Perspectives in Communication, Embedded-systems and Signal-processing - PiCES",,,,36-38,2022,,http://www.pices-journal.com/ojs/index.php/pices/article/view/360,,,,,,,,,,,,,,
281,A 5G Based Demodulator On FPGA,"B, Shashikala and Kumar G, Arvind","Perspectives in Communication, Embedded-systems and Signal-processing - PiCES",,5,9,86-89,2022,,http://pices-journal.com/ojs/index.php/pices/article/view/341,,,10.5281/zenodo.5867741,,,,,,,,,,,
282,{FPGA accelerated verification of multiple Application-Specific Instruction-set Processor based 5G fronthaul IP},"Haidak, Henri",,,,,50,2022,,http://urn.fi/URN:NBN:fi:aalto-202210236118,"Before any hardware design can be manufactured, its functionality must be verified. Generally, this verification is done through simulation software. However, verification through simulation software becomes slow in complex designs such as Application-Specific Instruction-set Processor (ASIP) based designs. This is because the achieved clock rates in simulators can be thousands of times slower compared to the end-product hardware. In addition to verifying the hardware, the software must be verified with the hardware, complicating the verification further. This thesis proposes a solution to the slow verification problem by creating a real-time Field-Programmable Gate-Array (FPGA) based verification testbench in VHDL language. We refer to our proposed design as Test-bench. This work also includes software necessary to communicate with the Test-bench. The Test-bench is targeted for verification of Nokia proprietary 5G fronthaul network Intellectual Property (IP) design, referred here as Fronthaul IP. Fronthaul IP is a complex design with multiple ASIP processors. The proposed Test-bench is designed, implemented and its performance is evaluated and compared to software simulation. Test-bench working principle is following. Input data is sent to Test-bench from software using Ethernet. Test-bench then forwards this data to Fronthaul IP, where it is processed. Output from Fronthaul IP is sent out using Ethernet, captured in software and compared to expected values. Measured peak clock rate acceleration of the Test-bench with Fronthaul IP, compared to software simulation, is up to 70 000 times higher. Therefore, using FPGA as complex hardware design verification platform should be considered in order to reduce the development time of a design. While initial effort of creating FPGA-based testbench design can be high, maintaining the design requires significantly less effort.",,,FPGA;verification platform;fronthaul;ASIP,,,,,,,,,,
283,Throughput Analysis with Effect of Dimensionality Reduction on 5G Dataset using Machine Learning and Deep Learning Models,"Mithillesh Kumar P , M. Supriya ",2022 International Conference on Industry 4.0 Technology (I4Tech),,,,,2022,IEEE,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9952579,"5G or ZTE the latest improvement to the existing 4G communication standard. These technologies could be evaluated by various metrics called performance indicators among which throughput plays a major role. Throughput is the measure of the rate of data transferred to the device. Higher the throughput, better is the performance of the network. This work models and analyses the throughput obtained with the variations observed on the identified parameters on which it depends on. Here the problem is analysed as a regression problem and hence regressor models are applied. Multiple models ranging from statistical to probabilistic and machine learning to deep recurrent networks are analysed with a 10 fold cross validation. Also, the effect of dimensionality reduction is applied to the dataset and the performance is observed. It is noticed from the work that the top performing models are consistent in performance measured using the regression metrics.",,10.1109/I4Tech55392.2022.9952579,"Dimensionality reduction , Performance evaluation , Analytical models , 5G mobile communication , Receivers , Predictive models , Throughput ",,,,,,,,,,
284,Lightweight testbed for machine learning evaluation in 5G networks,"Hernández, C. , Cervelló-Pastor, C.",JITEL 2019 - XIV Jornadas de Ingeniería Telemática,,,,,2019,upcommons.upc.edu,https://upcommons.upc.edu/handle/2117/185190,"The adoption of Software Define Networking, Network Function Virtualization and Machine Learning will play a key role in the control and management of fifth-generation (5G) networks in order to meet the specific requirements of vertical industries and the stringent requirements of 5G. Machine learning could be applied in 5G networks to deal with issues such as traffic prediction, routing optimization and resource management. To evaluate the adoption of machine learning in 5G networks, an adequate testing environment is required. In this paper, we introduce a lightweight testbed, which utilizes the benefits of container lightweight virtualization technology to create machine learning network functions over the well-known Mininet network emulator. As a use case of this testbed, we present an experimental real-time bandwidth prediction using the Long Short Term Memory recurrent neural network.",,,,,,,,,,,,,
285,Shared dataset framework toward applications of machine learning and mathematical optimization for 6G,"K Maruta, Y Ida, Y Hou, O Muta, H Okada, ...",IEICE Technical Report,,,,,2020,ieice.org,https://www.ieice.org/ken/paper/202010220CaC/eng/,"… (in English) In the research of applying machine learning and mathematical optimization to … , if a dataset suitable for research purposes is not available, efforts to create a dataset with …",,,,,,,,,,,,,
